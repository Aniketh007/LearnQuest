{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "e7HQxCnoT0tO",
        "outputId": "d469c80a-7124-450f-ea6b-9529902d36bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulSoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Collecting googlesearch-python\n",
            "  Downloading googlesearch_python-1.2.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulSoup4) (2.5)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.2)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.9.4)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (3.8.1)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.1.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.5.15)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.15.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading googlesearch_python-1.2.5-py3-none-any.whl (4.8 kB)\n",
            "Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=f833011827a220cbecd2df1ef29e7fca072079c88cc0a3c3c8698bbf673fcedc\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d6/6c/384f58df48c00b9a31d638005143b5b3ac62c3d25fb1447f23\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3342 sha256=3980cce9de210033185300b9c1951ebee74b867115610bf2768d015e6fa034ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/02/e7/a1ff1760e12bdbaab0ac824fae5c1bc933e41c4ccd6a8f8edb\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=94dbccb77efbdc9aee4fdd676ac6c19955807fb48645e207214c32e8717ebeb5\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/c4/0c/12a9a314ecac499456c4c3b2fcc2f635a3b45a39dfbd240299\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=9bc1af907fb5dd4f1ff97dd7f20dec67b562176fe0cebfd01021780a7e2103a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, googlesearch-python, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.11 googlesearch-python-1.2.5 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-2.1.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulSoup4 newspaper3k spacy googlesearch-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "01cQWYbYcDOU",
        "outputId": "8d32fc33-942b-487b-c780-78059b33657e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sumy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LxRVMyruYa8r",
        "outputId": "eb7ce1c1-6ca1-4cba-b04a-ce3361607aa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.32.3)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.8.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.66.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2024.7.4)\n",
            "Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: breadability, docopt\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21691 sha256=ed0ec1142d43435fd01f6e94266f3c33bc93ad9f94797ebcb00dfa31f5c0b541\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/22/90/b84fcc30e16598db20a0d41340616dbf9b1e82bbcc627b0b33\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=0493a0a394587974f3ffcec63475070889c44fa0628bbede3435554501168250\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built breadability docopt\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-24.6.1 sumy-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from googlesearch import search\n",
        "import spacy\n",
        "import newspaper\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.luhn import LuhnSummarizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "aBRwI24eT_fO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2799c9c-3947-4a6a-9be2-271fd142ef6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***GOOGLE***"
      ],
      "metadata": {
        "id": "20IxaaVCf5lt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_web(url):\n",
        "  try:\n",
        "    response=requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    if(response.status_code!=200):\n",
        "      return None\n",
        "    return response.text\n",
        "  except:\n",
        "    return None"
      ],
      "metadata": {
        "id": "iq-oRn2weouy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_url(urls):\n",
        "  filtered_urls=[]\n",
        "  for url in urls:\n",
        "    html=fetch_web(url)\n",
        "    if(html):\n",
        "      soup=BeautifulSoup(html,'html.parser')\n",
        "      if(soup.find_all('p')):\n",
        "        filtered_urls.append(url)\n",
        "  return filtered_urls"
      ],
      "metadata": {
        "id": "51bPUpOid9F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "  tokens = nltk.word_tokenize(text.lower())\n",
        "  stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "  tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
        "  return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "W0jK04d2SCg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_similarity(content, query):\n",
        "  query_preprocess=preprocess(query)\n",
        "  content_preprocess=preprocess(content)\n",
        "  doc=[query]+[content]\n",
        "  vectorize=TfidfVectorizer().fit_transform(doc)\n",
        "  vector=vectorize.toarray()\n",
        "  query_vector=vector[0]\n",
        "  content_vector=vector[1:]\n",
        "  similarity=cosine_similarity([query_vector],content_vector).flatten()\n",
        "  if(similarity>0.8):\n",
        "    return content\n",
        "  return ''"
      ],
      "metadata": {
        "id": "5OD6E_R2PYoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def extract_data(url):\n",
        "  soup = BeautifulSoup(url, 'html.parser')\n",
        "  for script in soup([\"script\", \"style\", \"iframe\", \"header\", \"footer\", \"nav\"]):\n",
        "      script.extract()  # Removes these elements completely\n",
        "  for element in soup.find_all(class_=[\"ad\", \"advertisement\"]):\n",
        "      element.extract()\n",
        "  for element in soup.find_all(id=[\"ad\", \"advertisement\"]):\n",
        "      element.extract()\n",
        "  # Remove references or citation patterns [number] or (number)\n",
        "  text = re.sub(r'\\[\\d+\\]|\\(\\d+\\)', '', soup.get_text(separator=' '))\n",
        "\n",
        "  # Clean up extra spaces and lines\n",
        "  text = re.sub(r'\\s+', ' ', text)\n",
        "  return text.strip()"
      ],
      "metadata": {
        "id": "pcgUesxOtOCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_k_data(query,k):\n",
        "  urls=list(search(query,num_results=int(k)))\n",
        "  urlss=filter_url(urls)\n",
        "  page_data=[]\n",
        "  doc=nlp(query)\n",
        "  keywords=[token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop]\n",
        "  for url in urls:\n",
        "    html=fetch_web(url)\n",
        "    if(html):\n",
        "      soup=BeautifulSoup(html,'html.parser')\n",
        "      text=extract_data(html)\n",
        "      page_data.append(text)\n",
        "  return page_data"
      ],
      "metadata": {
        "id": "IfpQpHtxUaO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summary(all_data):\n",
        "  parser = PlaintextParser.from_string(all_data, Tokenizer(\"english\"))\n",
        "  text=len(list(parser.document.sentences))\n",
        "  sent_count=int(text*0.5)\n",
        "  summarizer = LuhnSummarizer()\n",
        "  summarys = summarizer(parser.document,sent_count)\n",
        "  print(type(summarys))\n",
        "  return ' '.join([str(sentence) for sentence in summarys])"
      ],
      "metadata": {
        "id": "I_lzr2QbxOZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=input(\"Enter the query:\")\n",
        "k=input(\"Enter the number of websites to search:\")\n",
        "url=get_k_data(query,k)\n",
        "# print(url)\n",
        "all_data=' '.join(urls for urls in url)\n",
        "data=summary(all_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGjV8EYDUExd",
        "outputId": "279e661d-9350-4f9c-d866-fe51688ab388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the query:what is ai\n",
            "Enter the number of websites to search:10\n",
            "<class 'tuple'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "pYCf13njCgrE",
        "outputId": "e2510856-8392-47a5-fa69-e9a293e78b6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Artificial intelligence (AI) is a set of technologies that enable computers to perform a variety of advanced functions, including the ability to see , understand and translate spoken and written language , analyze data , make recommendations, and more. AI is the backbone of innovation in modern computing, unlocking value for individuals and businesses. For example, optical character recognition (OCR ) uses AI to extract text and data from images and documents, turns unstructured content into business-ready structured data, and unlocks valuable insights. New customers get $300 in free credits to spend on Google Cloud. Get started for free Stay informed 22:54 Introduction to generative AI Artificial intelligence defined Artificial intelligence is a field of science concerned with building computers and machines that can reason, learn, and act in such a way that would normally require human intelligence or that involves data whose scale exceeds what humans can analyze. AI is a broad field that encompasses many different disciplines, including computer science, data analytics and statistics, hardware and software engineering, linguistics, neuroscience, and even philosophy and psychology. On an operational level for business use, AI is a set of technologies that are based primarily on machine learning and deep learning, used for data analytics, predictions and forecasting, object categorization, natural language processing, recommendations, intelligent data retrieval, and more. While the specifics vary across different AI techniques, the core principle revolves around data. AI systems learn and improve through exposure to vast amounts of data, identifying patterns and relationships that humans may miss. This learning process often involves algorithms, which are sets of rules or instructions that guide the AI\\'s analysis and decision-making. In machine learning, a popular subset of AI, algorithms are trained on labeled or unlabeled data to make predictions or categorize information. Deep learning , a further specialization, utilizes artificial neural networks with multiple layers to process information, mimicking the structure and function of the human brain. Through continuous learning and adaptation, AI systems become increasingly adept at performing specific tasks, from recognizing images to translating languages and beyond. Types of artificial intelligence Artificial intelligence can be organized in several ways, depending on stages of development or actions being performed. For instance, four stages of AI development are commonly recognized. Reactive machines: Limited AI that only reacts to different kinds of stimuli based on preprogrammed rules. Does not use memory and thus cannot learn with new data. IBM’s Deep Blue that beat chess champion Garry Kasparov in 1997 was an example of a reactive machine. Limited memory: Most modern AI is considered to be limited memory. It can use memory to improve over time by being trained with new data, typically through an artificial neural network or other training model. Deep learning, a subset of machine learning, is considered limited memory artificial intelligence. Theory of mind: Theory of mind AI does not currently exist, but research is ongoing into its possibilities. It describes AI that can emulate the human mind and has decision-making capabilities equal to that of a human, including recognizing and remembering emotions and reacting in social situations as a human would. Self aware: A step above theory of mind AI, self-aware AI describes a mythical machine that is aware of its own existence and has the intellectual and emotional capabilities of a human. Like theory of mind AI, self-aware AI does not currently exist. A more useful way of broadly categorizing types of artificial intelligence is by what the machine can do. All of what we currently call artificial intelligence is considered artificial “narrow” intelligence, in that it can perform only narrow sets of actions based on its programming and training. For instance, an AI algorithm that is used for object classification won’t be able to perform natural language processing. Google Search is a form of narrow AI, as is predictive analytics, or virtual assistants. Artificial general intelligence (AGI) would be the ability for a machine to “sense, think, and act” just like a human. The next level would be artificial superintelligence (ASI), in which the machine would be able to function in all ways superior to a human. Artificial intelligence training models When businesses talk about AI, they often talk about “training data.” But what does that mean? Remember that limited-memory artificial intelligence is AI that improves over time by being trained with new data. Machine learning is a subset of artificial intelligence that uses algorithms to train data to obtain results. In broad strokes, three kinds of learnings models are often used in machine learning: Supervised learning is a machine learning model that maps a specific input to an output using labeled training data (structured data). In simple terms, to train the algorithm to recognize pictures of cats, feed it pictures labeled as cats. Unsupervised learning is a machine learning model that learns patterns based on unlabeled data (unstructured data). Unlike supervised learning, the end result is not known ahead of time. Rather, the algorithm learns from the data, categorizing it into groups based on attributes. For instance, unsupervised learning is good at pattern matching and descriptive modeling. In addition to supervised and unsupervised learning, a mixed approach called semi-supervised learning is often employed, where only some of the data is labeled. In semi-supervised learning, an end result is known, but the algorithm must figure out how to organize and structure the data to achieve the desired results. Reinforcement learning is a machine learning model that can be broadly described as “learn by doing.” An “agent” learns to perform a defined task by trial and error (a feedback loop) until its performance is within a desirable range. The agent receives positive reinforcement when it performs the task well and negative reinforcement when it performs poorly. An example of reinforcement learning would be teaching a robotic hand to pick up a ball. Common types of artificial neural networks A common type of training model in AI is an artificial neural network, a model loosely based on the human brain. A neural network is a system of artificial neurons—sometimes called perceptrons—that are computational nodes used to classify and analyze data. The data is fed into the first layer of a neural network, with each perceptron making a decision, then passing that information onto multiple nodes in the next layer. Training models with more than three layers are referred to as “deep neural networks” or “deep learning.” Some modern neural networks have hundreds or thousands of layers. The output of the final perceptrons accomplish the task set to the neural network, such as classify an object or find patterns in data. Some of the most common types of artificial neural networks you may encounter include: Feedforward neural networks (FF) are one of the oldest forms of neural networks, with data flowing one way through layers of artificial neurons until the output is achieved. In modern days, most feedforward neural networks are considered “deep feedforward” with several layers (and more than one “hidden” layer). Feedforward neural networks are typically paired with an error-correction algorithm called “backpropagation” that, in simple terms, starts with the result of the neural network and works back through to the beginning, finding errors to improve the accuracy of the neural network. Recurrent neural networks (RNN) differ from feedforward neural networks in that they typically use time series data or data that involves sequences. Unlike feedforward neural networks, which use weights in each node of the network, recurrent neural networks have “memory” of what happened in the previous layer as contingent to the output of the current layer. For instance, when performing natural language processing, RNNs can “keep in mind” other words used in a sentence. RNNs are often used for speech recognition, translation, and to caption images. Long/short term memory (LSTM) is an advanced form of RNN that can use memory to “remember” what happened in previous layers. The difference between RNNs and LSTM is that LSTM can remember what happened several layers ago, through the use of “memory cells.” LSTM is often used in speech recognition and making predictions. Convolutional neural networks (CNN) include some of the most common neural networks in modern artificial intelligence. Most often used in image recognition, CNNs use several distinct layers (a convolutional layer, then a pooling layer) that filter different parts of an image before putting it back together (in the fully connected layer). The earlier convolutional layers may look for simple features of an image, such as colors and edges, before looking for more complex features in additional layers. Generative adversarial networks (GAN) involve two neural networks competing against each other in a game that ultimately improves the accuracy of the output. One network (the generator) creates examples that the other network (the discriminator) attempts to prove true or false. GANs have been used to create realistic images and even make art. Benefits of AI Automation AI can automate workflows and processes or work independently and autonomously from a human team. For example, AI can help automate aspects of cybersecurity by continuously monitoring and analyzing network traffic. Similarly, a smart factory may have dozens of different kinds of AI in use, such as robots using computer vision to navigate the factory floor or to inspect products for defects, create digital twins, or use real-time analytics to measure efficiency and output. Reduce human error AI can eliminate manual errors in data processing, analytics, assembly in manufacturing, and other tasks through automation and algorithms that follow the same processes every single time. Eliminate repetitive tasks AI can be used to perform repetitive tasks, freeing human capital to work on higher impact problems. AI can be used to automate processes, like verifying documents, transcribing phone calls, or answering simple customer questions like “what time do you close?” Robots are often used to perform “dull, dirty, or dangerous” tasks in the place of a human. Fast and accurate AI can process more information more quickly than a human, finding patterns and discovering relationships in data that a human may miss. Infinite availability AI is not limited by time of day, the need for breaks, or other human encumbrances. When running in the cloud, AI and machine learning can be “always on,” continuously working on its assigned tasks. Accelerated research and development The ability to analyze vast amounts of data quickly can lead to accelerated breakthroughs in research and development. For instance, AI has been used in predictive modeling of potential new pharmaceutical treatments, or to quantify the human genome. Solve your business challenges with Google Cloud New customers get $300 in free credits to spend on Google Cloud. Get started Sign up for Google Cloud newsletters with product updates, event information, special offers, and more. Stay informed Applications and use cases for artificial intelligence Speech recognition Automatically convert spoken speech into written text. Image recognition Identify and categorize various aspects of an image. Translation Translate written or spoken words from one language into another. Predictive modeling Mine data to forecast specific outcomes with high degrees of granularity. Data analytics Find patterns and relationships in data for business intelligence. Related products and services Google offers a number of sophisticated artificial intelligence products, solutions, and applications on a trusted cloud platform that enables businesses to easily build and implement AI algorithms and models. By using products like Vertex AI , CCAI , DocAI , or AI APIs , organizations can make sense of all the data they’re producing, collecting, or otherwise analyzing, no matter what format it’s in, to make actionable business decisions. Explore all AI products and solutions Innovative AI and machine learning products, solutions, and services powered by Google’s research and technology. Vertex AI Build, deploy, and scale ML models faster, with pretrained and custom tooling within a unified artificial intelligence platform. Vertex AI Studio Tool for rapidly prototyping and testing generative AI models. Document AI Automate data capture at scale to reduce document processing costs. AlloyDB AI Build a wide range of generative AI applications using familiar PostgreSQL and run models in Vertex AI. Solution Contact Center AI Deliver exceptional customer service and increase operational efficiency using artificial intelligence. Enable your virtual agent to converse naturally with customers and expertly assist human agents on complex cases. Take the next step Start building on Google Cloud with $300 in free credits and 20+ always free products. Contact sales Work with a trusted partner Find a partner Want to hear from us? Artificial intelligence, or AI, is technology that enables computers and machines to simulate human intelligence and problem-solving capabilities. On its own or combined with other technologies (e.g., sensors, geolocation, robotics) AI can perform tasks that would otherwise require human intelligence or intervention. Digital assistants, GPS guidance, autonomous vehicles, and generative AI tools (like Open AI\\'s Chat GPT) are just a few examples of AI in the daily news and our daily lives. As a field of computer science, artificial intelligence encompasses (and is often mentioned together with) machine learning and deep learning . These disciplines involve the development of AI algorithms, modeled after the decision-making processes of the human brain, that can ‘learn’ from available data and make increasingly more accurate classifications or predictions over time. Artificial intelligence has gone through many cycles of hype, but even to skeptics, the release of ChatGPT seems to mark a turning point. The last time generative AI loomed this large, the breakthroughs were in computer vision, but now the leap forward is in natural language processing (NLP). Today, generative AI can learn and synthesize not just human language but other data types including images, video, software code, and even molecular structures. But as the hype around the use of AI tools in business takes off, conversations around ai ethics and responsible ai become critically important. For more on where IBM stands on these issues, please read Building trust in AI . White paper Why AI governance is a business imperative for scaling enterprise AI Learn about barriers to AI adoptions, particularly lack of AI governance and risk management solutions. Related content Register for the guide on foundation models Begin your journey to AI Learn how to scale AI Explore the AI Academy Types of artificial intelligence: weak AI vs. strong AI Weak AI—also known as narrow AI or artificial narrow intelligence (ANI)—is AI trained and focused to perform specific tasks. \"Narrow\" might be a more apt descriptor for this type of AI as it is anything but weak: it enables some very robust applications, such as Apple\\'s Siri, Amazon\\'s Alexa, IBM watsonx™, and self-driving vehicles. Strong AI is made up of artificial general intelligence (AGI) and artificial super intelligence (ASI). AGI, or general AI, is a theoretical form of AI where a machine would have an intelligence equal to humans; it would be self-aware with a consciousness that would have the ability to solve problems, learn, and plan for the future. While strong AI is still entirely theoretical with no practical examples in use today, that doesn\\'t mean AI researchers aren\\'t also exploring its development. In the meantime, the best examples of ASI might be from science fiction, such as HAL, the superhuman and rogue computer assistant in 2001: A Space Odyssey. Deep learning vs. machine learning Machine learning and deep learning are sub-disciplines of AI, and deep learning is a sub-discipline of machine learning. Both machine learning and deep learning algorithms use neural networks to ‘learn’ from huge amounts of data. These neural networks are programmatic structures modeled after the decision-making processes of the human brain. They consist of layers of interconnected nodes that extract features from the data and make predictions about what the data represents. Machine learning and deep learning differ in the types of neural networks they use, and the amount of human intervention involved. Classic machine learning algorithms use neural networks with an input layer, one or two ‘hidden’ layers, and an output layer. Typically, these algorithms are limited to supervised learning : the data needs to be structured or labeled by human experts to enable the algorithm to extract features from the data. Deep learning algorithms use deep neural networks—networks composed of an input layer, three or more (but usually hundreds) of hidden layers, and an output layout. These multiple layers enable unsupervised learning : they automate extraction of features from large, unlabeled and unstructured data sets. Because it doesn’t require human intervention, deep learning essentially enables machine learning at scale. The rise of generative models Generative AI refers to deep-learning models that can take raw data—say, all of Wikipedia or the collected works of Rembrandt—and “learn” to generate statistically probable outputs when prompted. At a high level, generative models encode a simplified representation of their training data and draw from it to create a new work that’s similar, but not identical, to the original data. Generative models have been used for years in statistics to analyze numerical data. The rise of deep learning, however, made it possible to extend them to images, speech, and other complex data types. Among the first class of AI models to achieve this cross-over feat were variational autoencoders, or VAEs, introduced in 2013. VAEs were the first deep-learning models to be widely used for generating realistic images and speech. “VAEs opened the floodgates to deep generative modeling by making models easier to scale,” said Akash Srivastava , an expert on generative AI at the MIT-IBM Watson AI Lab. “Much of what we think of today as generative AI started here.” Early examples of models, including GPT-3, BERT, or DALL-E 2, have shown what’s possible. In the future, models will be trained on a broad set of unlabeled data that can be used for different tasks, with minimal fine-tuning. Systems that execute specific tasks in a single domain are giving way to broad AI systems that learn more generally and work across domains and problems. Foundation models, trained on large, unlabeled datasets and fine-tuned for an array of applications, are driving this shift. As to the future of AI, when it comes to generative AI, it is predicted that foundation models will dramatically accelerate AI adoption in enterprise. Reducing labeling requirements will make it much easier for businesses to dive in, and the highly accurate, efficient AI-driven automation they enable will mean that far more companies will be able to deploy AI in a wider range of mission-critical situations. For IBM, the hope is that the computing power of foundation models can eventually be brought to every enterprise in a frictionless hybrid-cloud environment. Explore foundation models in watsonx.ai Artificial intelligence applications There are numerous, real-world applications for AI systems today. Below are some of the most common use cases: Speech recognition Also known as automatic speech recognition (ASR), computer speech recognition, or speech-to-text, speech recognition uses NLP to process human speech into a written format. Many mobile devices incorporate speech recognition into their systems to conduct voice search—Siri, for example—or provide more accessibility around texting in English or many widely-used languages. See how Don Johnston used IBM Watson Text to Speech to improve accessibility in the classroom with our case study . Customer service Online virtual agents and chatbots are replacing human agents along the customer journey. They answer frequently asked questions (FAQ) around topics, like shipping, or provide personalized advice, cross-selling products or suggesting sizes for users, changing the way we think about customer engagement across websites and social media platforms. Examples include messaging bots on e-commerce sites with virtual agents , messaging apps, such as Slack and Facebook Messenger, and tasks usually done by virtual assistants and voice assistants . See how Autodesk Inc. used IBM watsonx Assistant to speed up customer response times by 99% with our case study . Computer vision This AI technology enables computers and systems to derive meaningful information from digital images, videos and other visual inputs, and based on those inputs, it can take action. This ability to provide recommendations distinguishes it from image recognition tasks. Powered by convolutional neural networks, computer vision has applications within photo tagging in social media, radiology imaging in healthcare, and self-driving cars within the automotive industry. See how ProMare used IBM Maximo to set a new course for ocean research with our case study . Supply chain Adaptive robotics act on Internet of Things (IoT) device information, and structured and unstructured data to make autonomous decisions. NLP tools can understand human speech and react to what they are being told. Predictive analytics are applied to demand responsiveness, inventory and network optimization, preventative maintenance and digital manufacturing. Search and pattern recognition algorithms—which are no longer just predictive, but hierarchical—analyze real-time data, helping supply chains to react to machine-generated, augmented intelligence, while providing instant visibility and transparency. See how Hendrickson used IBM Sterling to fuel real-time transactions with our case study . Weather forecasting The weather models broadcasters rely on to make accurate forecasts consist of complex algorithms run on supercomputers. Machine-learning techniques enhance these models by making them more applicable and precise. See how Emnotion used IBM Cloud to empower weather-sensitive enterprises to make more proactive, data-driven decisions with our case study . Anomaly detection AI models can comb through large amounts of data and discover atypical data points within a dataset. These anomalies can raise awareness around faulty equipment, human error, or breaches in security. See how Netox used IBM QRadar to protect digital businesses from cyberthreats with our case study . History of artificial intelligence: Key dates and names The idea of \"a machine that thinks\" dates back to ancient Greece. But since the advent of electronic computing (and relative to some of the topics discussed in this article) important events and milestones in the evolution of artificial intelligence include the following: 1950: Alan Turing publishes Computing Machinery and Intelligence (link resides outside ibm.com) . In this paper, Turing—famous for breaking the German ENIGMA code during WWII and often referred to as the \"father of computer science\"— asks the following question: \"Can machines think?\" From there, he offers a test, now famously known as the \"Turing Test,\" where a human interrogator would try to distinguish between a computer and human text response. While this test has undergone much scrutiny since it was published, it remains an important part of the history of AI, as well as an ongoing concept within philosophy as it utilizes ideas around linguistics. 1956: John McCarthy coins the term \"artificial intelligence\" at the first-ever AI conference at Dartmouth College. Later that year, Allen Newell, J.C. Shaw, and Herbert Simon create the Logic Theorist, the first-ever running AI software program. 1967: Frank Rosenblatt builds the Mark 1 Perceptron, the first computer based on a neural network that \"learned\" though trial and error. Just a year later, Marvin Minsky and Seymour Papert publish a book titled Perceptrons , which becomes both the landmark work on neural networks and, at least for a while, an argument against future neural network research projects. 1980s: Neural networks which use a backpropagation algorithm to train itself become widely used in AI applications. 1995 : Stuart Russell and Peter Norvig publish Artificial Intelligence: A Modern Approach (link resides outside ibm.com), which becomes one of the leading textbooks in the study of AI. In it, they delve into four potential goals or definitions of AI, which differentiates computer systems on the basis of rationality and thinking vs. acting. 1997: IBM\\'s Deep Blue beats then world chess champion Garry Kasparov, in a chess match (and rematch). 2011: IBM Watson beats champions Ken Jennings and Brad Rutter at Jeopardy! 2015: Baidu\\'s Minwa supercomputer uses a special kind of deep neural network called a convolutional neural network to identify and categorize images with a higher rate of accuracy than the average human. 2016: DeepMind\\'s AlphaGo program, powered by a deep neural network, beats Lee Sodol, the world champion Go player, in a five-game match. The victory is significant given the huge number of possible moves as the game progresses (over 14.5 trillion after just four moves!). 2023 : A rise in large language models, or LLMs, such as ChatGPT, create an enormous change in performance of AI and its potential to drive enterprise value. With these new generative AI practices, deep-learning models can be pre-trained on vast amounts of raw, unlabeled data. Related solutions Artificial Intelligence (AI) solutions Put AI to work in your business with IBM’s industry-leading AI expertise and portfolio of solutions at your side. Explore AI solutions AI services Reinvent critical workflows and operations by adding AI to maximize experiences, real-time decision-making and business value. Explore AI services AI for cybersecurity AI is changing the game for cybersecurity, analyzing massive quantities of risk data to speed response times and augment under-resourced security operations. Explore AI for cybersecurity Resources Ebook How to choose the right AI foundation model Learn how to use the model selection framework to select the foundation model for your business needs. Training Save up to 70% with our Digital Learning Subscription Access our full catalog of over 100 online courses by purchasing an individual or multi-user digital learning subscription today, enabling you to expand your skills across a range of our products at one low price. Market research 2023 Gartner Peer Insights Customers\\' Choice IBM watsonx Assistant recognized as a Customers\\' Choice in the 2023 Gartner Peer Insights Voice of the Customer report for Enterprise Conversational AI platforms Article AI-enhanced procurement strategy Discover how machine learning can predict demand and cut costs. Take the next step Train, validate, tune and deploy generative AI, foundation models and machine learning capabilities with IBM watsonx.ai, a next-generation enterprise studio for AI builders. Build AI applications in a fraction of the time with a fraction of the data. | MIT Technology Review You need to enable JavaScript to view this site. Skip to Content Internet nastiness, name-calling, and other not-so-petty, world-altering disagreements AI is sexy, AI is cool. AI is our final invention, AI is a moral obligation. AI is the buzzword of the decade, AI is marketing jargon from 1955. The AI boom will boost the economy, the AI bubble is about to burst. It sounds like a stupid question, but it’s one that’s never been more urgent. Here’s the short answer: AI is a catchall term for a set of technologies that make computers do things that are thought to require intelligence when done by people. Think of recognizing faces, understanding speech, driving cars, writing sentences, answering questions, creating pictures. What does it mean for machines to understand speech or write a sentence? What kinds of tasks could we ask such machines to do? And how much should we trust the machines to do them? As this technology moves from prototype to product faster and faster, these have become questions for all of us. The people making it don’t know what AI is either. “These are the kinds of questions that are important enough that everyone feels like they can have an opinion,” says Chris Olah, chief scientist at the San Francisco–based AI lab Anthropic. “I also think you can argue about this as much as you want and there’s no evidence that’s going to contradict you right now.” But if you’re willing to buckle up and come for a ride, I can tell you why nobody really knows, why everybody seems to disagree, and why you’re right to care about it. Back in 2022, partway through the first episode of Mystery AI Hype Theater 3000 , a party-pooping podcast in which the irascible cohosts Alex Hanna and Emily Bender have a lot of fun sticking “the sharpest needles’’ into some of Silicon Valley’s most inflated sacred cows, they make a ridiculous suggestion. They’re hate-reading aloud from a 12,500-word Medium post by a Google VP of engineering, Blaise Agüera y Arcas, titled “ Can machines learn how to behave? ” Agüera y Arcas makes a case that AI can understand concepts in a way that’s somehow analogous to the way humans understand concepts—concepts such as moral values. COURTESY IMAGE Hanna and Bender are having none of it. They decide to replace the term “AI’’ with “mathy math”—you know, just lots and lots of math. The irreverent phrase is meant to collapse what they see as bombast and anthropomorphism in the sentences being quoted. Pretty soon Hanna, a sociologist and director of research at the Distributed AI Research Institute, and Bender, a computational linguist at the University of Washington (and internet-famous critic of tech industry hype), open a gulf between what Agüera y Arcas wants to say and how they choose to hear it. “How should AIs, their creators, and their users be held morally accountable?” asks Agüera y Arcas. Hanna and Bender don’t just reject what Agüera y Arcas says; they claim it makes no sense. “Can we please stop it with the ‘an AI’ or ‘the AIs’ as if they are, like, individuals in the world?” Bender says. Alex Hanna BRITTANY HOSEA-SMALL It might sound as if they’re talking about different things, but they’re not. Both sides are talking about large language models, the technology behind the current AI boom. It’s just that the way we talk about AI is more polarized than ever. In May, OpenAI CEO Sam Altman teased the latest update to GPT-4 , his company’s flagship model, by tweeting , “Feels like magic to me.” There’s a lot of road between math and magic. Emily Bender COURTESY PHOTO AI has acolytes, with a faith-like belief in the technology’s current power and inevitable future improvement. Artificial general intelligence is in sight, they say; superintelligence is coming behind it. The buzzy popular narrative is shaped by a pantheon of big-name players, from Big Tech marketers in chief like Sundar Pichai and Satya Nadella to edgelords of industry like Elon Musk and Altman to celebrity computer scientists like Geoffrey Hinton . Sometimes these boosters and doomers are one and the same, telling us that the technology is so good it’s bad . Pulling in this direction are a raft of researchers, including Hanna and Bender, and also outspoken industry critics like influential computer scientist and former Googler Timnit Gebru and NYU cognitive scientist Gary Marcus. In short, AI has come to mean all things to all people, splitting the field into fandoms. It can feel as if different camps are talking past one another, not always in good faith. But given the power and complexity of these technologies—which are already used to determine how much we pay for insurance, how we look up information, how we do our jobs, etc. etc.—it’s about time we at least agreed on what it is we’re even talking about. Yet in all the conversations I’ve had with people at the cutting edge of this technology, no one has given a straight answer about exactly what it is they’re building. (A quick side note: This piece focuses on the AI debate in the US and Europe, largely because many of the best-funded, most cutting-edge AI labs are there. But of course there’s important research happening elsewhere, too, in countries with their own varying perspectives on AI, particularly China.) The field just can’t find common ground on what’s really going on under the hood . They appear to be able to do a lot more—from solving high school math problems to writing computer code to passing law exams to composing poems. When a person does these things, we take it as a sign of intelligence. These questions go to the heart of what we mean by “artificial intelligence,” a term people have actually been arguing about for decades. But the discourse around AI has become more acrimonious with the rise of large language models that can mimic the way we talk and write with thrilling/chilling (delete as applicable) realism. We have built machines with humanlike behavior but haven’t shrugged off the habit of imagining a humanlike mind behind them. This leads to over-egged evaluations of what AI can do; it hardens gut reactions into dogmatic positions, and it plays into the wider culture wars between techno-optimists and techno-skeptics. Add to this stew of uncertainty a truckload of cultural baggage, from the science fiction that I’d bet many in the industry were raised on, to far more malign ideologies that influence the way we think about the future. Given this heady mix, arguments about AI are no longer simply academic (and perhaps never were). “It’s not in an intellectually healthy place right now,” Marcus says of the debate. For years Marcus has pointed out the flaws and limitations of deep learning, the tech that launched AI into the mainstream, powering everything from LLMs to image recognition to self-driving cars. His 2001 book The Algebraic Mind argued that neural networks, the foundation on which deep learning is built, are incapable of reasoning by themselves. (We’ll skip over it for now, but I’ll come back to it later and we’ll see just how much a word like “reasoning” matters in a sentence like this.) Marcus says that he has tried to engage Hinton—who last year went public with existential fears about the technology he helped invent—in a proper debate about how good large language models really are. “He calls me a twit.” (Having talked to Hinton about Marcus in the past, I can confirm that. “ChatGPT clearly understands neural networks better than he does,” Hinton told me last year.) Marcus also drew ire when he wrote an essay titled “Deep learning is hitting a wall.” Altman responded to it with a tweet : “Give me the confidence of a mediocre deep learning skeptic.” At the same time, banging his drum has made Marcus a one-man brand and earned him an invitation to sit next to Altman and give testimony last year before the US Senate’s AI oversight committee. And that’s why all these fights matter more than your average internet nastiness. But more than that, these disputes matter when industry leaders and opinionated scientists are summoned by heads of state and lawmakers to explain what this technology is and what it can do (and how scared we should be). They matter when this technology is being built into software we use every day, from search engines to word-processing apps to assistants on your phone. But if we don’t know what we’re being sold, who’s the dupe? “It is hard to think of another technology in history about which such a debate could be had—a debate about whether it is everywhere, or nowhere at all,” Stephen Cave and Kanta Dihal write in Imagining AI , a 2023 collection of essays about how different cultural beliefs shape people’s views of artificial intelligence. “That it can be held about AI is a testament to its mythic quality.” Above all else, AI is an idea—an ideal—shaped by worldviews and sci-fi tropes as much as by math and computer science. Figuring out what we are talking about when we talk about AI will clarify many things. We won’t agree on them, but common ground on what AI is would be a great place to start talking about what AI should be . In late 2022, soon after OpenAI released ChatGPT , a new meme started circulating online that captured the weirdness of this technology better than anything else. In most versions , a Lovecraftian monster called the Shoggoth, all tentacles and eyeballs, holds up a bland smiley-face emoji as if to disguise its true nature. @ANTHRUPAD VIA KNOWYOURMEME.COM For years one of the best-known touchstones for AI in pop culture was The Terminator , says Dihal. But by putting ChatGPT online for free, OpenAI gave millions of people firsthand experience of something different. “AI has always been a sort of really vague concept that can expand endlessly to encompass all kinds of ideas,” she says. But ChatGPT made those ideas tangible: “Suddenly, everybody has a concrete thing to refer to.” What is AI? Consider how The Daily Show recently skewered the hype, as expressed by industry leaders. Silicon Valley’s VC in chief, Marc Andreessen: “This has the potential to make life much better … I think it’s honestly a layup.” Altman: “I hate to sound like a utopic tech bro here, but the increase in quality of life that AI can deliver is extraordinary.” Pichai: “AI is the most profound technology that humanity is working on. More profound than fire.” Jon Stewart: “Yeah, suck a dick, fire!” But as the meme points out, ChatGPT is a friendly mask. Behind it is a monster called GPT-4, a large language model built from a vast neural network that has ingested more words than most of us could read in a thousand lifetimes. During training, which can last months and cost tens of millions of dollars, such models are given the task of filling in blanks in sentences taken from millions of books and a significant fraction of the internet. The result is a model that has turned much of the world’s written information into a statistical representation of which words are most likely to follow other words, captured across billions and billions of numerical values. But is it just that, or does this complex math encode algorithms capable of something akin to human reasoning or the formation of concepts? Many of the people who answer yes to that question believe we’re close to unlocking something called artificial general intelligence , or AGI, a hypothetical future technology that can do a wide range of tasks as well as humans can. A few of them have even set their sights on what they call superintelligence , sci-fi technology that can do things far better than humans. It could fix all the world’s problems—or bring about its doom. kinda mad how the so called godfathers of AI managed to convince seemingly smart people within AI field & many regulators to buy into the absurd idea that a sophisticated curve fitting (to a dataset) machine can have the urge to exterminate humans — Abeba Birhane (@Abebab) June 30, 2024 Today AGI appears in the mission statements of the world’s top AI labs. But the term was invented in 2007 as a niche attempt to inject some pizzazz into a field that was then best known for applications that read handwriting on bank deposit slips or recommended your next book to buy. The idea was to reclaim the original vision of an artificial intelligence that could do humanlike things (more on that soon). It was really an aspiration more than anything else, Google DeepMind cofounder Shane Legg, who coined the term, told me last year: “I didn’t have an especially clear definition.” AGI became the most controversial idea in AI . Some talked it up as the next big thing: AGI was AI but, you know, much better . Others claimed the term was so vague that it was meaningless. “AGI used to be a dirty word,” Ilya Sutskever told me, before he resigned as chief scientist at OpenAI. But large language models, and ChatGPT in particular, changed everything. Which brings us to what I think is one of the most illustrative disputes of the moment—one that sets up the sides of the argument and the stakes in play. Seeing magic in the machine A few months before the public launch of OpenAI’s large language model GPT-4 in March 2023, the company shared a prerelease version with Microsoft, which wanted to use the new model to revamp its search engine Bing. At the time, Sebastian Bubeck was studying the limitations of LLMs and was somewhat skeptical of their abilities. In particular, Bubeck—the vice president of generative AI research at Microsoft Research in Redmond, Washington—had been trying and failing to get the technology to solve middle school math problems. “My belief was that reasoning was a bottleneck, an obstacle,” he says. “I thought that you would have to do something really fundamentally different to get over that obstacle.” Then he got his hands on GPT-4. The first thing he did was try those math problems. GPT-3 cannot do that.” But Bubeck’s real road-to-Damascus moment came when he pushed it to do something new. The thing about middle school math problems is that they are all over the internet, and GPT-4 may simply have memorized them. “How do you study a model that may have seen everything that human beings have written?” asks Bubeck. His answer was to test GPT-4 on a range of problems that he and his colleagues believed to be novel. Playing around with Ronen Eldan, a mathematician at Microsoft Research, Bubeck asked GPT-4 to give, in verse, a mathematical proof that there are an infinite number of primes. Here’s a snippet of GPT-4’s response: “If we take the smallest number in S that is not in P / And call it p, we can add it to our set, don’t you see? / Thus, our set P must also be infinite, you’ll agree.” Cute, right? But Bubeck and Eldan thought it was much more than that. “We were in this office,” says Bubeck, waving at the room behind him via Zoom. It was just so creative and so, like, you know, different.” The Microsoft team also got GPT-4 to generate the code to add a horn to a cartoon picture of a unicorn drawn in Latex, a word processing program. Bubeck thinks this shows that the model could read the existing Latex code, understand what it depicted, and identify where the horn should go. “There are many examples, but a few of them are smoking guns of reasoning,” he says—reasoning being a crucial building block of human intelligence. BUBECK ET AL Bubeck, Eldan, and a team of other Microsoft researchers described their findings in a paper that they called “ Spark s of artificial general intelligence ”: “We believe that GPT-4’s intelligence signals a true paradigm shift in the field of computer science and beyond.” When Bubeck shared the paper online, he tweeted : “time to face it, the sparks of #AGI have been ignited.” The Sparks paper quickly became infamous—and a touchstone for AI boosters. Agüera y Arcas and Peter Norvig, a former director of research at Google and coauthor of Artificial Intelligence: A Modern Approach , perhaps the most popular AI textbook in the world, cowrote an article called “ Artificial General Intelligence Is Already Here .” Published in Noema , a magazine backed by an LA think tank called the Berggruen Institute, their argument uses the Sparks paper as a jumping-off point: “Artificial General Intelligence (AGI) means many different things to different people, but the most important parts of it have already been achieved by the current generation of advanced AI large language models,” they wrote. “Decades from now, they will be recognized as the first true examples of AGI.” Since then, the hype has continued to balloon. Leopold Aschenbrenner, who at the time was a researcher at OpenAI focusing on superintelligence, told me last year: “AI progress in the last few years has been just extraordinarily rapid. We’ve been crushing all the benchmarks, and that progress is continuing unabated. We’re going to have superhuman models, models that are much smarter than us.” (He was fired from OpenAI in April because, he claims, he raised security concerns about the tech he was building and “ ruffled some feathers .” He has since set up a Silicon Valley investment fund.) In June, Aschenbrenner put out a 165-page manifesto arguing that AI will outpace college graduates by “2025/2026” and that “we will have superintelligence, in the true sense of the word” by the end of the decade. When Aschenbrenner tweeted a chart to show how fast he thought AI would continue to improve given how fast it had improved in last few years, the tech investor Christian Keil replied that by the same logic, his baby son, who had doubled in size since he was born, would weigh 7.5 trillion tons by the time he was 10. It’s no surprise that “sparks of AGI” has also become a byword for over-the-top buzz. “I think they got carried away,” says Marcus, speaking about the Microsoft team. This is amazing!’ They didn’t vet it with the scientific community.” Bender refers to the Sparks paper as a “fan fiction novella.” Not only was it provocative to claim that GPT-4’s behavior showed signs of AGI, but Microsoft, which uses GPT-4 in its own products, has a clear interest in promoting the capabilities of the technology. Its evidence is hard to verify because it comes from interactions with a version of GPT-4 that was not made available outside OpenAI and Microsoft. The public version has guardrails that restrict the model’s capabilities, admits Bubeck. This made it impossible for other researchers to re-create his experiments. One group tried to re-create the unicorn example with a coding language called Processing, which GPT-4 can also use to generate images . They found that the public version of GPT-4 could produce a passable unicorn but not flip or rotate that image by 90 degrees. It may seem like a small difference, but such things really matter when you’re claiming that the ability to draw a unicorn is a sign of AGI. The key thing about the examples in the Sparks paper, including the unicorn, is that Bubeck and his colleagues believe they are genuine examples of creative reasoning. This means the team had to be certain that examples of these tasks, or ones very like them, were not included anywhere in the vast data sets that OpenAI amassed to train its model. Otherwise, the results could be interpreted instead as instances where GPT-4 reproduced patterns it had already seen. JUN IONEDA Bubeck insists that they set the model only tasks that would not be found on the internet. Other researchers soon pointed out that there are indeed online forums dedicated to drawing animals in Latex . “Every single query of the Sparks paper was thoroughly looked for on the internet.” (This didn’t stop the name-calling: “I’m asking you to stop being a charlatan,” Ben Recht, a computer scientist at the University of California, Berkeley, tweeted back before accusing Bubeck of “being caught flat-out lying.”) Bubeck insists the work was done in good faith, but he and his coauthors admit in the paper itself that their approach was not rigorous—notebook observations rather than foolproof experiments. Still, he has no regrets: “The paper has been out for more than a year and I have yet to see anyone give me a convincing argument that the unicorn, for example, is not a real example of reasoning.” That’s not to say he can give me a straight answer to the big question—though his response reveals what kind of answer he’d like to give. The question can be simple, but the answer can be complex.” “There are many simple questions out there to which we still don’t know the answer. And some of those simple questions are the most profound ones,” he says. “I’m putting this on the same footing as, you know, What is the origin of life? Big, big questions like this.” Seeing only math in the machine Before Bender became one of the chief antagonists of AI’s boosters, she made her mark on the AI world as a coauthor on two influential papers. (Both peer-reviewed, she likes to point out—unlike the Sparks paper and many of the others that get much of the attention.) The first, written with Alexander Koller, a fellow computational linguist at Saarland University in Germany, and published in 2020, was called “ Climbing towards NLU ” (NLU is natural-language understanding). “The start of all this for me was arguing with other people in computational linguistics whether or not language models understand anything,” she says. (Understanding, like reasoning, is typically taken to be a basic ingredient of human intelligence.) Bender and Koller argue that a model trained exclusively on text will only ever learn the form of a language, not its meaning. Meaning, they argue, consists of two parts: the words (which could be marks or sounds) plus the reason those words were uttered. People use language for many reasons, such as sharing information, telling jokes, flirting, warning somebody to back off, and so on. Stripped of that context, the text used to train LLMs like GPT-4 lets them mimic the patterns of language well enough for many sentences generated by the LLM to look exactly like sentences written by a human. There is an underwater cable that lets them send text messages to each other. Now imagine that an octopus, which knows nothing about English but is a whiz at statistical pattern matching, wraps its suckers around the cable and starts listening in to the messages. The octopus gets really good at guessing what words follow other words. So good that when it breaks the cable and starts replying to messages from one of the islanders, she believes that she is still chatting with her neighbor. (In case you missed it, the octopus in this story is a chatbot.) The person talking to the octopus would stay fooled for a reasonable amount of time, but could that last? JUN IONEDA Imagine that the islander now says she has built a coconut catapult and asks the octopus to build one too and tell her what it thinks. Without knowing what the words in the messages refer to in the world, it cannot follow the islander’s instructions. Perhaps it guesses a reply: “Okay, cool idea!” The islander will probably take this to mean that the person she is speaking to understands her message. But if so, she is seeing meaning where there is none. Finally, imagine that the islander gets attacked by a bear and sends calls for help down the line. Bender and Koller believe that this is how large language models learn and why they are limited. “The thought experiment shows why this path is not going to lead us to a machine that understands anything,” says Bender. “The deal with the octopus is that we have given it its training data, the conversations between those two people, and that’s it. But then here’s something that comes out of the blue and it won’t be able to deal with it because it hasn’t understood.” The other paper Bender is known for, “ On the Dangers of Stochastic Parrots ,” highlights a series of harms that she and her coauthors believe the companies making large language models are ignoring. These include the huge computational costs of making the models and their environmental impact; the racist, sexist, and other abusive language the models entrench; and the dangers of building a system that could fool people by “haphazardly stitching together sequences of linguistic forms … according to probabilistic information about how they combine, but without any reference to meaning: a stochastic parrot.” Google senior management wasn’t happy with the paper, and the resulting conflict led two of Bender’s coauthors, Timnit Gebru and Margaret Mitchell, to be forced out of the company, where they had led the AI Ethics team. It also made “stochastic parrot” a popular put-down for large language models—and landed Bender right in the middle of the name-calling merry-go-round. The bottom line for Bender and for many like-minded researchers is that the field has been taken in by smoke and mirrors: “I think that they are led to imagine autonomous thinking entities that can make decisions for themselves and ultimately be the kind of thing that could actually be accountable for those decisions.” Always the linguist, Bender is now at the point where she won’t even use the term AI “without scare quotes,” she tells me. Ultimately, for her, it’s a Big Tech buzzword that distracts from the many associated harms. “I’ve got skin in the game now,” she says. “I care about these issues, and the hype is getting in the way.” Extraordinary evidence? Agüera y Arcas calls people like Bender “AI denialists”—the implication being that they won’t ever accept what he takes for granted. Bender’s position is that extraordinary claims require extraordinary evidence, which we do not have. But there are people looking for it, and until they find something clear-cut—sparks or stochastic parrots or something in between—they’d prefer to sit out the fight. As Ellie Pavlick, who studies neural networks at Brown University, tells me: “It’s offensive to some people to suggest that human intelligence could be re-created through these kinds of mechanisms.” She adds, “People have strong-held beliefs about this issue—it almost feels religious. On the other hand, there’s people who have a little bit of a God complex. So it’s also offensive to them to suggest that they just can’t do it.” Pavlick is ultimately agnostic. She’s a scientist, she insists, and will follow wherever the science leads. She rolls her eyes at the wilder claims, but she believes there’s something exciting going on. “That’s where I would disagree with Bender and Koller,” she tells me. “I think there’s actually some sparks—maybe not of AGI, but like, there’s some things in there that we didn’t expect to find.” Ellie Pavlick COURTESY PHOTO The problem is finding agreement on what those exciting things are and why they’re exciting. Researchers like Bubeck seem a lot more cool-headed when you hear them out. “I don’t see any problem in holding simultaneous views,” he says. We don’t have all the answers.” “We need a completely new vocabulary to describe what’s going on,” he says. “One reason why people push back when I talk about reasoning in large language models is because it’s not the same reasoning as in human beings. But I think there is no way we can not call it reasoning. It is reasoning.” Anthropic’s Olah plays it safe when pushed on what we’re seeing in LLMs, though his company, one of the hottest AI labs in the world right now, built Claude 3, an LLM that has received just as much hyperbolic praise as GPT-4 (if not more) since its release earlier this year. “I feel like a lot of these conversations about the capabilities of these models are very tribal,” he says. “People have preexisting opinions, and it’s not very informed by evidence on any side. Then it just becomes kind of vibes-based, and I think vibes-based arguments on the internet tend to go in a bad direction.” Olah tells me he has hunches of his own. “My subjective impression is that these things are tracking pretty sophisticated ideas,” he says. “We don’t have a comprehensive story of how very large models work, but I think it’s hard to reconcile what we’re seeing with the extreme ‘stochastic parrots’ picture.” That’s as far as he’ll go: “I don’t want to go too much beyond what can be really strongly inferred from the evidence that we have.” Last month, Anthropic released results from a study in which researchers gave Claude 3 the neural network equivalent of an MRI. By monitoring which bits of the model turned on and off as they ran it, they identified specific patterns of neurons that activated when the model was shown specific inputs. Anthropic also reported patterns that it says correlate with inputs that attempt to describe or show abstract concepts. “We see features related to deception and honesty, to sycophancy, to security vulnerabilities, to bias,” says Olah. “We find features related to power seeking and manipulation and betrayal.” ASK IT FOR A RECIPE pic.twitter.com/0ZM3uGRJi9 — heron (@iamaheron_) May 23, 2024 These results give one of the clearest looks yet at what’s inside a large language model. As Olah admits, they do not know what the model does with these patterns. “It’s a relatively limited picture, and the analysis is pretty hard,” he says. Even if Olah won’t spell out exactly what he thinks goes on inside a large language model like Claude 3, it’s clear why the question matters to him. Anthropic is known for its work on AI safety—making sure that powerful future models will behave in ways we want them to and not in ways we don’t (known as “alignment” in industry jargon). Figuring out how today’s models work is not only a necessary first step if you want to control future ones; it also tells you how much you need to worry about doomer scenarios in the first place. “If you don’t think that models are going to be very capable,” says Olah, “then they’re probably not going to be very dangerous.” Why we all can’t get along In a 2014 interview with the BBC that looked back on her career, the influential cognitive scientist Margaret Boden, now 87, was asked if she thought there were any limits that would prevent computers (or “tin cans,” as she called them) from doing what humans can do. “I certainly don’t think there’s anything in principle,” she said. “Because to deny that is to say that [human thinking] happens by magic, and I don’t believe that it happens by magic.” Margaret Boden ALAMY But, she cautioned, powerful computers won’t be enough to get us there: the AI field will also need “powerful ideas”—new theories of how thinking happens, new algorithms that might reproduce it. “But these things are very, very difficult and I see no reason to assume that we will one of these days be able to answer all of those questions. Maybe we will; maybe we won’t.” Boden was reflecting on the early days of the current boom, but this will-we-or-won’t-we teetering speaks to decades in which she and her peers grappled with the same hard questions that researchers struggle with today. AI began as an ambitious aspiration 70-odd years ago and we are still disagreeing about what is and isn’t achievable, and how we’ll even know if we have achieved it. Most—if not all—of these disputes come down to this: We don’t have a good grasp on what intelligence is or how to recognize it. The field is full of hunches, but no one can say for sure. We’ve been stuck on this point ever since people started taking the idea of AI seriously. Or even before that, when the stories we consumed started planting the idea of humanlike machines deep in our collective imagination. The long history of these disputes means that today’s fights often reinforce rifts that have been around since the beginning, making it even more difficult for people to find common ground. To understand how we got here, we need to understand where we’ve been. So let’s dive into AI’s origin story—one that also played up the hype in a bid for cash. A brief history of AI spin The computer scientist John McCarthy is credited with coming up with the term “artificial intelligence” in 1955 when writing a funding application for a summer research program at Dartmouth College in New Hampshire. The plan was for McCarthy and a small group of fellow researchers, a who’s-who of postwar US mathematicians and computer scientists—or “John McCarthy and the boys,” as Harry Law, a researcher who studies the history of AI at the University of Cambridge and ethics and policy at Google DeepMind, puts it—to get together for two months (not a typo) and make some serious headway on this new research challenge they’d set themselves. From left to right, Oliver Selfridge, Nathaniel Rochester, Ray Solomonoff, Marvin Minsky, Peter Milner, John McCarthy, and Claude Shannon sitting on the lawn at the 1956 Dartmouth conference. COURTESY OF THE MINSKY FAMILY “The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it,” McCarthy and his coauthors wrote. “An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves.” That list of things they wanted to make machines do—what Bender calls “the starry-eyed dream”—hasn’t changed much. Using language, forming concepts, and solving problems are defining goals for AI today. The hubris hasn’t changed much either: “We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer,” they wrote. And the extent to which these problems are in fact now solved is something that people still shout about on the internet. But what’s often left out of this canonical history is that artificial intelligence almost wasn’t called “artificial intelligence” at all. John McCarthy COURTESY PHOTO More than one of McCarthy’s colleagues hated the term he had come up with. “The word ‘artificial’ makes you think there’s something kind of phony about this,” Arthur Samuel, a Dartmouth participant and creator of the first checkers-playing computer, is quoted as saying in historian Pamela McCorduck’s 2004 book Machines Who Think . The mathematician Claude Shannon, a coauthor of the Dartmouth proposal who is sometimes billed as “the father of the information age,” preferred the term “automata studies.” Herbert Simon and Allen Newell, two other AI pioneers, continued to call their own work “complex information processing” for years afterwards. In fact, “artificial intelligence” was just one of several labels that might have captured the hodgepodge of ideas that the Dartmouth group was drawing on. The historian Jonnie Penn has identified possible alternatives that were in play at the time, including “engineering psychology,” “applied epistemology,” “neural cybernetics,” “non-numerical computing,” “neuraldynamics,” “advanced automatic programming,” and “hypothetical automata.” This list of names reveals how diverse the inspiration for their new field was, pulling from biology, neuroscience, statistics, and more. Marvin Minsky, another Dartmouth participant, has described AI as a “suitcase word” because it can hold so many divergent interpretations. But McCarthy wanted a name that captured the ambitious scope of his vision. In addition to terminology, the Dartmouth proposal codified a split between rival approaches to artificial intelligence that has divided the field ever since—a divide Law calls the “core tension in AI.” McCarthy and his colleagues wanted to describe in computer code “every aspect of learning or any other feature of intelligence” so that machines could mimic them. In other words, if they could just figure out how thinking worked—the rules of reasoning—and write down the recipe, they could program computers to follow it. This laid the foundation of what came to be known as rule-based or symbolic AI (sometimes referred to now as GOFAI, “good old-fashioned AI”). But coming up with hard-coded rules that captured the processes of problem-solving for actual, nontrivial problems proved too hard. The other path favored neural networks, computer programs that would try to learn those rules by themselves in the form of statistical patterns. Though the idea seemed less promising at first, some researchers nevertheless continued to work on versions of neural networks alongside symbolic AI. But it would take decades—plus vast amounts of computing power and much of the data on the internet—before they really took off. Fast-forward to today and this approach underpins the entire AI boom. The big takeaway here is that, just like today’s researchers, AI’s innovators fought about foundational concepts and got caught up in their own promotional spin. Aaron Sloman, a philosopher and fellow AI pioneer now in his late 80s, recalls how “old friends” Minsky and McCarthy “disagreed strongly” when he got to know them in the ’70s: “Minsky thought McCarthy’s claims about logic could not work, and McCarthy thought Minsky’s mechanisms could not do what could be done using logic. I got on well with both of them, but I was saying, ‘Neither of you have got it right.’” (Sloman still thinks no one can account for the way human reasoning uses intuition as much as logic, but that’s yet another tangent!) Marvin Minsky MIT MUSEUM As the fortunes of the technology waxed and waned, the term “AI” went in and out of fashion. In the early ’70s, both research tracks were effectively put on ice after the UK government published a report arguing that the AI dream had gone nowhere and wasn’t worth funding. Research projects were shuttered, and computer scientists scrubbed the words “artificial intelligence” from their grant proposals. When I was finishing a computer science PhD in 2008, only one person in the department was working on neural networks. Bender has a similar recollection: “When I was in college, a running joke was that AI is anything that we haven’t figured out how to do with computers yet. Like, as soon as you figure out how to do it, it wasn’t magic anymore, so it wasn’t AI.” But that magic—the grand vision laid out in the Dartmouth proposal—remained alive and, as we can now see, laid the foundations for the AGI dream. Good and bad behavior In 1950, five years before McCarthy started talking about artificial intelligence, Alan Turing had published a paper that asked: Can machines think? To address that question, the famous mathematician proposed a hypothetical test, which he called the imitation game. The setup imagines a human and a computer behind a screen and a second human who types questions to each. If the questioner cannot tell which answers come from the human and which come from the computer, Turing claimed, the computer may as well be said to think. What Turing saw—unlike McCarthy’s crew—was that thinking is a really difficult thing to describe. “He basically said: Instead of focusing on the nature of intelligence itself, I’m going to look for its manifestation in the world. Turing was joined in the studio by two of his Manchester University colleagues—professor of mathematics Maxwell Newman and professor of neurosurgery Geoffrey Jefferson—and Richard Braithwaite, a philosopher of science, ethics, and religion at the University of Cambridge. Braithwaite kicked things off: “Thinking is ordinarily regarded as so much the specialty of man, and perhaps of other higher animals, the question may seem too absurd to be discussed. But of course, it all depends on what is to be included in ‘thinking.’” The panelists circled Turing’s question but never quite pinned it down. When they tried to define what thinking involved, what its mechanisms were, the goalposts moved. “As soon as one can see the cause and effect working themselves out in the brain, one regards it as not being thinking but a sort of unimaginative donkey work,” said Turing. Here was the problem: When one panelist proposed some behavior that might be taken as evidence of thought—reacting to a new idea with outrage, say—another would point out that a computer could be made to do it. As Newman said, it would be easy enough to program a computer to print “I don’t like this new program.” But he admitted that this would be a trick. Exactly, Jefferson said: He wanted a computer that would print “I don’t like this new program” because it didn’t like the new program. As he had noted, uncovering a specific process—the donkey work, to use his phrase—did not pinpoint what thinking was either. “From this point of view, one might be tempted to define thinking as consisting of those mental processes that we don’t understand,” said Turing. “If this is right, then to make a thinking machine is to make one which does interesting things without our really understanding quite how it is done.” It is strange to hear people grapple with these ideas for the first time. What they seem to be going round and round on is that the Turing test is first and foremost a behaviorist test.” For Turing, intelligence was hard to define but easy to recognize. He proposed that the appearance of intelligence was enough—and said nothing about how that behavior should come about. JUN IONEDA And yet most people, when pushed, will have a gut instinct about what is and isn’t intelligent. There are dumb ways and clever ways to come across as intelligent. In 1981, Ned Block, a philosopher at New York University, showed that Turing’s proposal fell short of those gut instincts. Because it said nothing of what caused the behavior, the Turing test can be beaten through trickery (as Newman had noted in the BBC broadcast). “Could the issue of whether a machine in fact thinks or is intelligent depend on how gullible human interrogators tend to be?” asked Block. (Or as computer scientist Mark Reidl has remarked : “The Turing test is not for AI to pass but for humans to fail.”) Imagine, Block said, a vast look-up table in which human programmers had entered all possible answers to all possible questions. Type a question into this machine, and it would look up a matching answer in its database and send it back. Block argued that anyone using this machine would judge its behavior to be intelligent: “But actually, the machine has the intelligence of a toaster,” he wrote. “All the intelligence it exhibits is that of its programmers.” Block concluded that whether behavior is intelligent behavior is a matter of how it is produced, not how it appears. Block’s toasters, which became known as Blockheads, are one of the strongest counterexamples to the assumptions behind Turing’s proposal. Looking under the hood The Turing test is not meant to be a practical metric, but its implications are deeply ingrained in the way we think about artificial intelligence today. This has become particularly relevant as LLMs have exploded in the past several years. These models get ranked by their outward behaviors, specifically how well they do on a range of tests. When OpenAI announced GPT-4, it published an impressive-looking scorecard that detailed the model’s performance on multiple high school and professional exams. Almost nobody talks about how these models get those results. Today’s large language models are too complex for anybody to say exactly how their behavior is produced. Researchers outside the small handful of companies making those models don’t know what’s in their training data; none of the model makers have shared details. That makes it hard to say what is and isn’t a kind of memorization—a stochastic parroting. But even researchers on the inside, like Olah, don’t know what’s really going on when faced with a bridge-obsessed bot. This leaves the question wide open: Yes, large language models are built on math—but are they doing something intelligent with it? “Most people are trying to armchair through it,” says Brown University’s Pavlick, meaning that they are arguing about theories without looking at what’s really happening. “Some people are like, ‘I think it’s this way,’ and some people are like, ‘Well, I don’t.’ We’re kind of stuck and everyone’s unsatisfied.” Bender thinks that this sense of mystery plays into the mythmaking. Without a proper appreciation of where the LLM’s words come from, we fall back on familiar assumptions about humans, since that is our only real point of reference. When we talk to another person, we try to make sense of what that person is trying to tell us. “That process necessarily entails imagining a life behind the words,” says Bender. JUN IONEDA “The parlor trick of ChatGPT is so impressive that when we see these words coming out of it, we do the same thing instinctively,” she says. “It’s very good at mimicking the form of language. The problem is that we are not at all good at encountering the form of language and not imagining the rest of it.” For some researchers, it doesn’t really matter if we can’t understand the how . Bubeck used to study large language models to try to figure out how they worked, but GPT-4 changed the way he thought about them. “It seems like these questions are not so relevant anymore,” he says. “The model is so big, so complex, that we can’t hope to open it up and understand what’s really happening.” But Pavlick, like Olah, is trying to do just that. Her team has found that models seem to encode abstract relationships between objects, such as that between a country and its capital. Studying one large language model, Pavlick and her colleagues found that it used the same encoding to map France to Paris and Poland to Warsaw. But what struck Pavlick was that, unlike a Blockhead, the model had learned this lookup table on its own. In other words, the LLM figured out itself that Paris is to France as Warsaw is to Poland. Is encoding its own lookup table instead of using a hard-coded one a sign of intelligence? “Basically, the problem is that behavior is the only thing we know how to measure reliably,” says Pavlick. “Anything else requires a theoretical commitment, and people don’t like having to make a theoretical commitment because it’s so loaded.” Geoffrey Hinton RAMSEY CARDY / COLLISION / SPORTSFILE Not all people. Hinton, for example, insists that neural networks are all you need to re-create humanlike intelligence. “Deep learning is going to be able to do everything,” he told MIT Technology Review in 2020 . It’s a commitment that Hinton seems to have held onto from the start. Sloman, who recalls the two of them arguing when Hinton was a graduate student in his lab, remembers being unable to persuade him that neural networks cannot learn certain crucial abstract concepts that humans and some other animals seem to have an intuitive grasp of, such as whether something is impossible. “Despite Hinton’s outstanding intelligence, he never seemed to understand that point. I don’t know why, but there are large numbers of researchers in neural networks who share that failing.” And then there’s Marcus, whose view of neural networks is the exact opposite of Hinton’s. His case draws on what he says scientists have discovered about brains. Brains, Marcus points out, are not blank slates that learn fully from scratch—they come ready-made with innate structures and processes that guide learning. It’s how babies can learn things that the best neural networks still can’t, he argues. Gary Marcus AP IMAGES “Neural network people have this hammer, and now everything is a nail,” says Marcus. “They want to do all of it with learning, which many cognitive scientists would find unrealistic and silly. You’re not going to learn everything from scratch.” Not that Marcus—a cognitive scientist—is any less sure of himself. “If one really looked at who’s predicted the current situation well, I think I would have to be at the top of anybody’s list,” he tells me from the back of an Uber on his way to catch a flight to a speaking gig in Europe. “I know that doesn’t sound very modest, but I do have this perspective that turns out to be very important if what you’re trying to study is artificial intelligence.” Given his well-publicized attacks on the field, it might surprise you that Marcus still believes AGI is on the horizon. It’s just that he thinks today’s fixation on neural networks is a mistake. “We probably need a breakthrough or two or four,” he says. “You and I might not live that long, I’m sorry to say. Maybe we’ve got a shot at it.” The power of a technicolor dream Over Dor Skuler’s shoulder on the Zoom call from his home in Ramat Gan, Israel, a little lamp-like robot is winking on and off while we talk about it. Skuler’s company, Intuition Robotics, develops these devices for older people, and the design—part Amazon Alexa, part R2-D2—must make it very clear that ElliQ is a computer. If any of his customers show signs of being confused about that, Intuition Robotics takes the device back, says Skuler. Ask it about sports, and it will crack a joke about having no hand-eye coordination because it has no hands and no eyes. “For the life of me, I don’t understand why the industry is trying to fulfill the Turing test,” Skuler says. “Why is it in the best interest of humanity for us to develop technology whose goal is to dupe us?” Instead, Skuler’s firm is betting that people can form relationships with machines that present as machines. “Just like we have the ability to build a real relationship with a dog,” he says. People love their dog—but they never confuse it to be a human.” ELLIQ ElliQ’s users, many in their 80s and 90s, refer to the robot as an entity or a presence—sometimes a roommate. “They’re able to create a space for this in-between relationship, something between a device or a computer and something that’s alive,” says Skuler. But no matter how hard ElliQ’s designers try to control the way people view the device, they are competing with decades of pop culture that have shaped our expectations. “Because it’s hard for us to imagine something else,” says Skuler (who indeed refers to ElliQ as “she” throughout our conversation). “And because so many people in the tech industry are fans of science fiction. They try to make their dream come true.” How many developers grew up today thinking that building a smart machine was seriously the coolest thing—if not the most important thing—that they could possibly do? It was not long ago that OpenAI launched its new voice-controlled version of ChatGPT with a voice that sounded like Scarlett Johansson, after which many people—including Altman—flagged the connection to Spike Jonze’s 2013 movie Her . As Cave and Dihal write in Imagining AI : “AI was a cultural phenomenon long before it was a technological one.” Stories and myths about remaking humans as machines have been around for centuries. People have been dreaming of artificial humans for probably as long as they have dreamed of flight, says Dihal. She notes that Daedalus, the figure in Greek mythology famous for building a pair of wings for himself and his son, Icarus, also built what was effectively a giant bronze robot called Talos that threw rocks at passing pirates. The word robot comes from robota , a term for “forced labor” coined by the Czech playwright Karel Čapek in his 1920 play Rossum’s Universal Robots . The “laws of robotics” outlined in Isaac Asimov’s science fiction, forbidding machines from harming humans, are inverted by movies like The Terminator , which is an iconic reference point for popular fears about real-world technology. Last year’s blockbuster The Creator imagines a future world in which AI has been outlawed because it set off a nuclear bomb, an event that some doomers consider at least an outside possibility. Cave and Dihal relate how another movie, 2014’s Transcendence , in which an AI expert played by Johnny Depp gets his mind uploaded to a computer, served a narrative pushed by ur-doomers Stephen Hawking, fellow physicist Max Tegmark, and AI researcher Stuart Russell. In an article published in the Huffington Post on the movie’s opening weekend, the trio wrote: “As the Hollywood blockbuster Transcendence debuts this weekend with … clashing visions for the future of humanity, it’s tempting to dismiss the notion of highly intelligent machines as mere science fiction. But this would be a mistake, and potentially our worst mistake ever.” ALCON ENTERTAINMENT VIA ALAMY Right around the same time, Tegmark founded the Future of Life Institute, with a remit to study and promote AI safety. Depp’s costar in the movie, Morgan Freeman, was on the institute’s board, and Elon Musk, who had a cameo in the film, donated $10 million in its first year. For Cave and Dihal, Transcendence is a perfect example of the multiple entanglements between popular culture, academic research, industrial production, and “the billionaire-funded fight to shape the future.” On the London leg of his world tour last year, Altman was asked what he’d meant when he tweeted : “AI is the tech the world has always wanted.” Standing at the back of the room that day, behind an audience of hundreds, I listened to him offer his own kind of origin story: “I was, like, a very nervous kid. But I was always really interested in AI and I thought it’d be very cool.” He went to college, got rich, and watched as neural networks became better and better. What are we going to do about that?” he recalled thinking in 2015. “I ended up starting OpenAI.” Why you should care that a bunch of nerds are fighting about AI Okay, you get it: No one can agree on what AI is. But what everyone does seem to agree on is that the current debate around AI has moved far beyond the academic and the scientific. There are political and moral components in play—which doesn’t help with everyone thinking everyone else is wrong. It can be difficult to see what’s going on when some of those moral views take in the entire future of humanity and anchor them in a technology that nobody can quite define. Because no matter what this technology is, it’s coming, and unless you live under a rock, you’ll use it in one form or another. And the form that technology takes—and the problems it both solves and creates—will be shaped by the thinking and the motivations of people like the ones you just read about. In particular, by the people with the most power, the most cash, and the biggest megaphones. I realize it’s unfair to introduce yet another new concept so late in the game. But to understand how the people in power may mold the technologies they build, and how they explain them to the world’s regulators and lawmakers, you need to really understand their mindset. Timnit Gebru WIKIMEDIA Gebru, who founded the Distributed AI Research Institute after leaving Google, and Émile Torres, a philosopher and historian at Case Western Reserve University, have traced the influence of several techno-utopian belief systems on Silicon Valley. The pair argue that to understand what’s going on with AI right now—both why companies such as Google DeepMind and OpenAI are in a race to build AGI and why doomers like Tegmark and Hinton warn of a coming catastrophe—the field must be seen through the lens of what Torres has dubbed the TESCREAL framework . A lot has been written (and will be written) about each of these worldviews, so I’ll spare you here. (There are rabbit holes within rabbit holes for anyone wanting to dive deeper. Émile Torres COURTESY PHOTO This constellation of overlapping ideologies is attractive to a certain kind of galaxy-brain mindset common in the Western tech world. The common tenet is that an all-powerful technology—AGI or superintelligence, choose your team—is not only within reach but inevitable. You can see this in the do-or-die attitude that’s ubiquitous inside cutting-edge labs like OpenAI: If we don’t make AGI, someone else will. What’s more, TESCREALists believe that AGI could not only fix the world’s problems but level up humanity. “The development and proliferation of AI—far from a risk that we should fear—is a moral obligation that we have to ourselves, to our children and to our future,” Andreessen wrote in a much-dissected manifesto last year. I have been told many times over that AGI is the way to make the world a better place—by Demis Hassabis , CEO and cofounder of Google DeepMind; by Mustafa Suleyman , CEO of the newly minted Microsoft AI and another cofounder of DeepMind; by Sutskever , Altman , and more. If you believe that you are building a technology so powerful that it will solve all the world’s problems, you probably also believe there’s a non-zero chance it will all go very wrong. When asked at the World Government Summit in February what keeps him up at night, Altman replied: “It’s all the sci-fi stuff.” It’s a tension that Hinton has been talking up for the last year. It’s what Sutskever is focusing on in his new lab , and what he wanted a special in-house team at OpenAI to focus on last year before disagreements over the way the company balanced risk and reward led most members of that team to leave. (“Claiming that you have created something that is super-intelligent is good for sales figures,” says Dihal. “It’s like, ‘Please, someone stop me from being so good and so powerful.’”) But boom or doom, exactly what (and whose) problems are these guys supposedly solving? Are we really expected to trust what they build and what they tell our leaders? They are highly critical of these ideologies and how they may influence the development of future technology, especially AI. Fundamentally, they link several of these worldviews—with their common focus on “improving” humanity—to the racist eugenics movements of the 20th century. One danger, they argue, is that a shift of resources toward the kind of technological innovations that these ideologies demand, from building AGI to extending life spans to colonizing other planets, will ultimately benefit people who are Western and white at the cost of billions of people who aren’t. If your sight is set on fantastical futures, it’s easy to overlook the present-day costs of innovation, such as labor exploitation, the entrenchment of racist and sexist bias, and environmental damage. “Are we trying to build a tool that’s useful to us in some way?” asks Bender, reflecting on the casualties of this race to AGI. If so, who’s it for, how do we test it, how well does it work? “But if what we’re building it for is just so that we can say that we’ve done it, that’s not a goal that I can get behind. That’s not a goal that’s worth billions of dollars.” Bender says that seeing the connections between the TESCREAL ideologies is what made her realize there was something more to these debates. There’s a moral code tied up in it as well.” Of course, laid out like this without nuance, it doesn’t sound as if we—as a society, as individuals—are getting the best deal. When Gebru described parts of the TESCREAL bundle in a talk last year, her audience laughed. It’s also true that few people would identify themselves as card-carrying students of these schools of thought, at least in their extremes. But if we don’t understand how those building this tech approach it, how can we decide what deals we want to make? What apps we decide to use, what chatbots we want to give personal information to, what data centers we support in our neighborhoods, what politicians we want to vote for? It used to be like this: There was a problem in the world, and we built something to fix it. Here, everything is backward: The goal seems to be to build a machine that can do everything, and to skip the slow, hard work that goes into figuring out what the problem is before building the solution. And as Gebru said in that same talk, “A machine that solves all problems: if that’s not magic, what is it?” Semantics, semantics … semantics? In April, the CEO of Microsoft AI stood on the TED stage and told the audience what he’d told his six-year-old nephew in response to that question. The best answer he could give, Suleyman explained, was that AI was “a new kind of digital species”—a technology so universal, so powerful, that calling it a tool no longer captured what it could do for us. “On our current trajectory, we are heading toward the emergence of something we are all struggling to describe, and yet we cannot control what we don’t understand,” he said. “And so the metaphors, the mental models, the names—these all matter if we are to get the most out of AI whilst limiting its potential downsides.” Language matters! I hope that’s clear from the twists and turns and tantrums we’ve been through to get to this point. Suleyman is an industry leader at a technology giant that stands to make billions from its AI products. Describing the technology behind those products as a new kind of species conjures something wholly unprecedented, something with agency and capabilities that we have never seen before. I can’t tell you if there’s magic here (ironically or not). And I can’t tell you how math can realize what Bubeck and many others see in this technology (no one can yet). But I can pull back the curtain on my own point of view. Writing about GPT-3 back in 2020, I said that the greatest trick AI ever pulled was convincing the world it exists. I still think that: We are hardwired to see intelligence in things that behave in certain ways, whether it’s there or not. In the last few years, the tech industry has found reasons of its own to convince us that AI exists, too. This makes me skeptical of many of the claims made for this technology. With large language models—via their smiley-face masks—we are confronted by something we’ve never had to think about before. “It’s taking this hypothetical thing and making it really concrete,” says Pavlick. “I’ve never had to think about whether a piece of language required intelligence to generate because I’ve just never dealt with language that didn’t.” AI is many things. I don’t think it’s the solution to all (or even most) of our problems. It’s an idea, a vision, a kind of wish fulfillment. And ideas get shaped by other ideas, by morals, by quasi-religious convictions, by worldviews, by politics, and by gut instinct. But AI is not one thing; it never has been, no matter how often the branding gets seared into the outside of the box. “The truth is these words”—intelligence, reasoning, understanding, and more—“were defined before there was a need to be really precise about it,” says Pavlick. “I don’t really like when the question becomes ‘Does the model understand—yes or no?’ because, well, I don’t know. Words get redefined and concepts evolve all the time.” I think that’s right. And the sooner we can all take a step back, agree on what we don’t know, and accept that none of this is yet a done deal, the sooner we can—I don’t know, I guess not all hold hands and sing kumbaya. hide Popular Supershoes are reshaping distance running Jonathan W. Rosen How generative AI could reinvent what it means to play Niall Firth Google DeepMind trained a robot to beat humans at table tennis Rhiannon Williams Here’s how people are actually using AI Melissa Heikkilä Deep Dive Artificial intelligence How generative AI could reinvent what it means to play AI-powered NPCs that don’t need a script could make games—and other worlds—deeply immersive. By Niall Firth archive page Google DeepMind trained a robot to beat humans at table tennis It was able to draw on vast amounts of data to refine its playing style and adjust its tactics as matches progressed. By Rhiannon Williams archive page Here’s how people are actually using AI Something peculiar and slightly unexpected has happened: people have started forming relationships with AI systems. By Melissa Heikkilä archive page Synthesia’s hyperrealistic deepfakes will soon have full bodies With bodies that move and hands that wave, deepfakes just got a whole lot more realistic. By Melissa Heikkilä archive page Stay connected Illustration by Rose Wong Get the latest updates from MIT Technology Review Discover special offers, top stories, upcoming events, and more. If you continue to get this message, reach out to us at customer-service@technologyreview.com with a list of newsletters you’d like to receive. | Online Master of Engineering | University of Illinois Chicago Your browser is unsupported We recommend using the latest version of IE11, Edge, Chrome, Firefox or Safari. Learn the definition of AI, how it works, and educational/career opportunities. (AI) Artificial Intelligence: What is the definition of AI and how does AI work? Heading link Copy link A recurring theme in science fiction, artificial intelligence (AI) has captured our collective imagination and enthralled audiences for over a century. From the early days of science fiction literature to the captivating narratives of iconic movies, the concept of intelligent machines has been a source of fascination and speculation. Artificial Intelligence (AI) enables machines to learn from experience, adapt to new inputs, and execute tasks resembling human capabilities. By leveraging AI technologies, computers can undergo training to perform particular tasks through the analysis of extensive data sets and the identification of patterns within the data. Artificial intelligence represents a branch of computer science that aims to create machines capable of performing tasks that typically require human intelligence. These tasks include learning from experience (machine learning), understanding natural language, recognizing patterns, solving problems, and making decisions. From self-driving cars to virtual personal assistants, AI is reshaping various aspects of our daily lives, and its significance continues to grow. “Computer science is about building recipes to achieve different goals and objectives,” said Dr. Ian Kash, Associate Professor for the University of Illinois Chicago’s online Master of Engineering with a focus area in AI and Machine Learning (MEng) program . “In many areas of computer science, we can build things that are guaranteed to do what we want. However, there are a lot of extremely difficult problems in the world. So, to me, the field of AI is a set of techniques and tools that have been developed to solve these hard problems even when we can’t get a fully satisfying ‘just follow this recipe’ solution.” One pivotal moment in the exploration of AI came in 1950 with the visionary work of British polymath, Alan Turing . In his paper, “Computing Machinery and Intelligence,” Turing introduced the Turing test and explored the mathematical possibilities of AI and questioned why machines couldn’t leverage available information, just as humans do, to solve problems and make decisions. This marked a crucial step in the journey from speculative fiction to tangible innovation. Unlike traditional computer programs that follow predetermined instructions, AI systems can learn and adapt from data, allowing them to improve their performance over time. This ability to learn and evolve is a key characteristic that sets AI apart from conventional computing. Artificial Intelligence (AI) works by simulating human intelligence through the use of algorithms, data, and computational power. The goal is to enable machines or software to perform tasks that typically require human intelligence, such as learning, reasoning, problem-solving, perception, and language understanding. AI Subsets Artificial Intelligence comprises various subsets or subfields, each focusing on specific aspects of replicating human intelligence or solving particular types of problems. Although AI subsets often overlap and interdisciplinary approaches are common, below are some of the major subsets of artificial intelligence: Machine Learning (ML): The ML subset focuses on the development of algorithms and statistical models that enable computer systems to perform tasks without explicit programming. The primary goal of machine learning is to allow machines to learn patterns and make decisions based on data. Neural Network(s): This subset focuses on AI models that are inspired by the structure of the human brain. These networks consist of layers of interconnected nodes, each layer contributing to the model’s ability to understand increasingly complex features in the data. Deep learning is also a class of neural networks with multiple layers. Deep learning has been particularly successful in tasks like image recognition, natural language processing, and playing strategic games. Natural Language Processing (NLP): This subset focuses on enabling machines to understand, interpret, and generate human language. This subset is crucial for applications like chatbots, language translation, sentiment analysis, and voice recognition. Game Playing: This subset focuses on AI systems that are designed for game playing involve creating algorithms that can play strategic games, such as chess at a high level. Weak AI vs. Strong AI There are two different types of artificial intelligence capabilities, particularly in terms of mimicking human intelligence. These concepts help distinguish the extent to which AI systems can replicate cognitive functions and exhibit intelligence. Weak AI (or Narrow AI): AI systems that are designed and trained for a specific task or a narrow set of tasks and are most of the AI that we see today. It enables some very robust applications, such as Amazon’s Alexa and Tesla’s self-driving vehicles. Strong AI (or General AI): AI systems with the capacity to comprehend, learn, and apply knowledge across a diverse spectrum of tasks at a level equivalent to human intelligence. It’s a theoretical form of AI where a machine would have an intelligence equal to humans. Jobs in AI The field of AI is expected to grow explosively as it becomes capable of accomplishing more tasks thus leading to a demand for professionals with expertise in various domains. The U.S. Bureau of Labor Statistics expects employment of computer and information technology occupations to grow 13% from 2020 to 2030 and predicts the field of data scientists to grow 35% between 2022 and 2032. Here are some potential roles in AI: Machine Learning Engineer ($160,000 average salary) Data Engineer ($125,000 average salary) Natural Language Processing (NLP) Engineer ($111,000 average salary) Computer Vision Engineer ($135,000 average salary) Robotics Engineer ($109,000 average salary) Data Scientist ($125,000 average salary) AI Software Developer ($129,000 average salary) AI Consultant ($105,000 average salary) AI Product Manager ($135,000 average salary) Education in AI Individuals looking to enter the field of AI should consider pursuing an advanced degree. A Master of Engineering (MEng) degree can open a wide range of career opportunities in various industries where AI and machine learning are playing an increasingly important role. UIC’s online Master of Engineering with a focus area in AI and Machine Learning program offers a unique opportunity to dive headfirst into the cutting-edge world of artificial intelligence. The online program’s core courses help students develop their understanding of the fundamental math of AI and ML, as well as AI and ML theories, techniques and tools. They will apply this knowledge more deeply in the courses of Image Analysis and Computer Vision, Deep Neural Networks, and Natural Language Processing. Teaching the online MEng program’s CS 411: Artificial Intelligence course, Dr. Kash believes AI is a powerful set of tools that can help individuals in all careers solve complex problems. “Whether you’re building the AI tools themselves or thinking about how you can integrate AI tools within your company, there’s a fundamental understanding you’ll get in the online MEng program that is extremely valuable.” What are the current trends in AI? Staying on top of current AI trends is imperative to understanding the transformative developments shaping our future. There are several notable trends that are influencing the trajectory of this field. Dr. Kash is intrigued by the possibility of witnessing AI techniques that will address substantial, real-world challenges. Although we have seen AI techniques work well in small scale settings, Dr. Kash says we have not seen many tackle important engineering challenges. “An AI trend that I’m observing is the integration of classic AI techniques with modern deep learning methods and figuring out the engineering solutions to make those two things work seamlessly together,” said Dr. Kash. Here are some other noteworthy AI trends to keep a close eye on: AI Governance and Regulation: While the field of AI advances rapidly, governments and organizations are working together to establish guidelines, regulations, and frameworks to ensure AI technologies are developed and deployed responsibly. Generative AI: As we have seen with ChatGPT, generative models are producing remarkably realistic content. These models have applications in content creation, art, and media. Ethical AI: With the emergence of AI technologies, the field has the potential to be disruptive and it’s essential that these new technologies are ethical. We will continue to see organizations address any legal or ethical issues associated with AI to mitigate any potential problems. AI in Healthcare: While we watch AI/ML transform many parts of the economy, AI will be creating significant opportunities in healthcare services, life sciences tools and diagnostics, and medical technology. Heading link Copy link Envelope icon Request Information Calendar icon RSVP for an Info Session Modified on May 07, 2024 Call 772-2268 772-2268 Request Info Apply Now What is Artificial Intelligence (AI)? | Definition from TechTarget Home AI technologies Tech Accelerator A guide to artificial intelligence in the enterprise Prev Next 15 AI risks businesses must confront and how to address them AI use cases in banking create opportunities, improve systems Download this guide 1 X Free Download A guide to artificial intelligence in the enterprise This wide-ranging guide to artificial intelligence in the enterprise provides the building blocks for becoming successful business consumers of AI technologies. It starts with introductory explanations of AI\\'s history, how AI works and the main types of AI. The importance and impact of AI is covered next, followed by information on AI\\'s key benefits and risks, current and potential AI use cases, building a successful AI strategy, steps for implementing AI tools in the enterprise and technological breakthroughs that are driving the field forward. Throughout the guide, we include hyperlinks to TechTarget articles that provide more detail and insights on the topics discussed. Share this item with your network: By Lev Craig, Site Editor Nicole Laskowski, Senior News Director Linda Tucci, Industry Editor -- CIO/IT Strategy What is AI? Artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. Examples of AI applications include expert systems , natural language processing ( NLP ), speech recognition and machine vision . As the hype around AI has accelerated, vendors have scrambled to promote how their products and services incorporate it. Often, what they refer to as \"AI\" is a well-established technology such as machine learning . AI requires specialized hardware and software for writing and training machine learning algorithms. No single programming language is used exclusively in AI, but Python, R, Java, C++ and Julia are all popular languages among AI developers. In general, AI systems work by ingesting large amounts of labeled training data, analyzing that data for correlations and patterns, and using these patterns to make predictions about future states. This article is part of A guide to artificial intelligence in the enterprise Which also includes: 10 top AI and machine learning trends for 2024 10 top artificial intelligence certifications and courses for 2024 The future of AI: What to expect in the next 5 years For example, an AI chatbot that is fed examples of text can learn to generate lifelike exchanges with people, and an image recognition tool can learn to identify and describe objects in images by reviewing millions of examples. Generative AI techniques, which have advanced rapidly over the past few years, can create realistic text, images, music and other media. Programming AI systems focuses on cognitive skills such as the following: Learning. This aspect of AI programming involves acquiring data and creating rules, known as algorithms , to transform it into actionable information. These algorithms provide computing devices with step-by-step instructions for completing specific tasks. This aspect involves choosing the right algorithm to reach a desired outcome. This aspect involves algorithms continuously learning and tuning themselves to provide the most accurate results possible. This aspect uses neural networks , rule-based systems , statistical methods and other AI techniques to generate new images, text, music, ideas and so on. Differences among AI, machine learning and deep learning The terms AI, machine learning and deep learning are often used interchangeably, especially in companies\\' marketing materials, but they have distinct meanings. In short, AI describes the broad concept of machines simulating human intelligence, while machine learning and deep learning are specific techniques within this field. The term AI, coined in the 1950s, encompasses an evolving and wide range of technologies that aim to simulate human intelligence, including machine learning and deep learning. Machine learning enables software to autonomously learn patterns and predict outcomes by using historical data as input. This approach became more effective with the availability of large training data sets. Deep learning, a subset of machine learning, aims to mimic the brain\\'s structure using layered neural networks. It underpins many major breakthroughs and recent advances in AI, including autonomous vehicles and ChatGPT. AI is important for its potential to change how we live, work and play. It has been effectively used in business to automate tasks traditionally done by humans, including customer service, lead generation, fraud detection and quality control. In a number of areas, AI can perform tasks more efficiently and accurately than humans. It is especially useful for repetitive, detail-oriented tasks such as analyzing large numbers of legal documents to ensure relevant fields are properly filled in. AI\\'s ability to process massive data sets gives enterprises insights into their operations they might not otherwise have noticed. The rapidly expanding array of generative AI tools is also becoming important in fields ranging from education to marketing to product design. Advances in AI techniques have not only helped fuel an explosion in efficiency, but also opened the door to entirely new business opportunities for some larger enterprises. Prior to the current wave of AI, for example, it would have been hard to imagine using computer software to connect riders to taxis on demand, yet Uber has become a Fortune 500 company by doing just that. AI has become central to many of today\\'s largest and most successful companies, including Alphabet, Apple, Microsoft and Meta, which use AI to improve their operations and outpace competitors. At Alphabet subsidiary Google, for example, AI is central to its eponymous search engine, and self-driving car company Waymo began as an Alphabet division. The Google Brain research lab also invented the transformer architecture that underpins recent NLP breakthroughs such as OpenAI\\'s ChatGPT . AI technologies, particularly deep learning models such as artificial neural networks, can process large amounts of data much faster and make predictions more accurately than humans can. While the huge volume of data created on a daily basis would bury a human researcher, AI applications using machine learning can take that data and quickly turn it into actionable information. A primary disadvantage of AI is that it is expensive to process the large amounts of data AI requires. As AI techniques are incorporated into more products and services, organizations must also be attuned to AI\\'s potential to create biased and discriminatory systems, intentionally or inadvertently. Advantages of AI The following are some advantages of AI: Excellence in detail-oriented jobs. AI is a good fit for tasks that involve identifying subtle patterns and relationships in data that might be overlooked by humans. For example, in oncology, AI systems have demonstrated high accuracy in detecting early-stage cancers, such as breast cancer and melanoma , by highlighting areas of concern for further evaluation by healthcare professionals. AI systems and automation tools dramatically reduce the time required for data processing. This is particularly useful in sectors like finance, insurance and healthcare that involve a great deal of routine data entry and analysis, as well as data-driven decision-making. For example, in banking and finance , predictive AI models can process vast volumes of data to forecast market trends and analyze investment risk. AI and robotics can not only automate operations but also improve safety and efficiency. In manufacturing, for example, AI-powered robots are increasingly used to perform hazardous or repetitive tasks as part of warehouse automation , thus reducing the risk to human workers and increasing overall productivity. Today\\'s analytics tools use AI and machine learning to process extensive amounts of data in a uniform way, while retaining the ability to adapt to new information through continuous learning. For example, AI applications have delivered consistent and reliable outcomes in legal document review and language translation. AI systems can enhance user experience by personalizing interactions and content delivery on digital platforms. On e-commerce platforms, for example, AI models analyze user behavior to recommend products suited to an individual\\'s preferences, increasing customer satisfaction and engagement. For example, AI-powered virtual assistants can provide uninterrupted, 24/7 customer service even under high interaction volumes, improving response times and reducing costs. AI systems can scale to handle growing amounts of work and data. This makes AI well suited for scenarios where data volumes and workloads can grow exponentially, such as internet search and business analytics . AI can speed up the pace of R&D in fields such as pharmaceuticals and materials science. By rapidly simulating and analyzing many possible scenarios, AI models can help researchers discover new drugs , materials or compounds more quickly than traditional methods. AI and machine learning are increasingly used to monitor environmental changes, predict future weather events and manage conservation efforts . Machine learning models can process satellite imagery and sensor data to track wildfire risk , pollution levels and endangered species populations, for example. AI is used to streamline and automate complex processes across various industries. For example, AI models can identify inefficiencies and predict bottlenecks in manufacturing workflows, while in the energy sector, they can forecast electricity demand and allocate supply in real time. Disadvantages of AI The following are some disadvantages of AI: High costs. Building an AI model requires a substantial upfront investment in infrastructure , computational resources and software to train the model and store its training data. After initial training, there are further ongoing costs associated with model inference and retraining. As a result, costs can rack up quickly, particularly for advanced, complex systems like generative AI applications; OpenAI CEO Sam Altman has stated that training the company\\'s GPT-4 model cost over $100 million. Developing, operating and troubleshooting AI systems -- especially in real-world production environments -- requires a great deal of technical know-how. In many cases, this knowledge differs from that needed to build non-AI software . For example, building and deploying a machine learning application involves a complex, multistage and highly technical process, from data preparation to algorithm selection to parameter tuning and model testing. Compounding the problem of technical complexity, there is a significant shortage of professionals trained in AI and machine learning compared with the growing need for such skills. This gap between AI talent supply and demand means that, even though interest in AI applications is growing, many organizations cannot find enough qualified workers to staff their AI initiatives. AI and machine learning algorithms reflect the biases present in their training data -- and when AI systems are deployed at scale, the biases scale, too. In some cases, AI systems may even amplify subtle biases in their training data by encoding them into reinforceable and pseudo-objective patterns. In one well-known example, Amazon developed an AI-driven recruitment tool to automate the hiring process that inadvertently favored male candidates, reflecting larger-scale gender imbalances in the tech industry. AI models often excel at the specific tasks for which they were trained but struggle when asked to address novel scenarios. This lack of flexibility can limit AI\\'s usefulness, as new tasks might require the development of an entirely new model. An NLP model trained on English-language text, for example, might perform poorly on text in other languages without extensive additional training. While work is underway to improve models\\' generalization ability -- known as domain adaptation or transfer learning -- this remains an open research problem. AI can lead to job loss if organizations replace human workers with machines -- a growing area of concern as the capabilities of AI models become more sophisticated and companies increasingly look to automate workflows using AI. For example, some copywriters have reported being replaced by large language models ( LLMs ) such as ChatGPT. While widespread AI adoption may also create new job categories, these may not overlap with the jobs eliminated, raising concerns about economic inequality and reskilling. AI systems are susceptible to a wide range of cyberthreats, including data poisoning and adversarial machine learning . Hackers can extract sensitive training data from an AI model, for example, or trick AI systems into producing incorrect and harmful output. This is particularly concerning in security-sensitive sectors such as financial services and government. The data centers and network infrastructures that underpin the operations of AI models consume large amounts of energy and water. Consequently, training and running AI models has a significant impact on the climate . AI\\'s carbon footprint is especially concerning for large generative models, which require a great deal of computing resources for training and ongoing use. AI raises complex questions around privacy and legal liability, particularly amid an evolving AI regulation landscape that differs across regions. Using AI to analyze and make decisions based on personal data has serious privacy implications, for example, and it remains unclear how courts will view the authorship of material generated by LLMs trained on copyrighted works. Strong AI vs. weak AI AI can generally be categorized into two types: narrow (or weak ) AI and general (or strong) AI. This form of AI refers to models trained to perform specific tasks. Narrow AI operates within the context of the tasks it is programmed to perform, without the ability to generalize broadly or learn beyond its initial programming. Examples of narrow AI include virtual assistants, such as Apple Siri and Amazon Alexa, and recommendation engines , such as those found on streaming platforms like Spotify and Netflix. This type of AI, which does not currently exist, is more often referred to as artificial general intelligence ( AGI ). If created, AGI would be capable of performing any intellectual task that a human being can. To do so, AGI would need the ability to apply reasoning across a wide range of domains to understand complex problems it was not specifically programmed to solve. This, in turn, would require something known in AI as fuzzy logic : an approach that allows for gray areas and gradations of uncertainty, rather than binary, black-and-white outcomes. Importantly, the question of whether AGI can be created -- and the consequences of doing so -- remains hotly debated among AI experts. Even today\\'s most advanced AI technologies, such as ChatGPT and other highly capable LLMs, do not demonstrate cognitive abilities on par with humans and cannot generalize across diverse situations. ChatGPT, for example, is designed for natural language generation, and it is not capable of going beyond its original programming to perform tasks such as complex mathematical reasoning. 4 types of AI AI can be categorized into four types , beginning with the task-specific intelligent systems in wide use today and progressing to sentient systems, which do not yet exist. These AI systems have no memory and are task specific. An example is Deep Blue, the IBM chess program that beat Russian chess grandmaster Garry Kasparov in the 1990s. Deep Blue was able to identify pieces on a chessboard and make predictions, but because it had no memory, it could not use past experiences to inform future ones. These AI systems have memory, so they can use past experiences to inform future decisions. Some of the decision-making functions in self-driving cars are designed this way. When applied to AI, it refers to a system capable of understanding emotions. This type of AI can infer human intentions and predict behavior, a necessary skill for AI systems to become integral members of historically human teams. In this category, AI systems have a sense of self, which gives them consciousness. Understanding the key differences between artificial and human intelligence is crucial to effective and responsible AI use. What are examples of AI technology, and how is it used today? AI technologies can enhance existing tools\\' functionalities and automate various tasks and processes, affecting numerous aspects of everyday life. Automation AI enhances automation technologies by expanding the range, complexity and number of tasks that can be automated. An example is robotic process automation ( RPA ), which automates repetitive, rules-based data processing tasks traditionally performed by humans. Because AI helps RPA bots adapt to new data and dynamically respond to process changes, integrating AI and machine learning capabilities enables RPA to manage more complex workflows. Machine learning Machine learning is the science of teaching computers to learn from data and make decisions without being explicitly programmed to do so. Deep learning, a subset of machine learning, uses sophisticated neural networks to perform what is essentially an advanced form of predictive analytics. Machine learning algorithms can be broadly classified into three categories: supervised learning , unsupervised learning and reinforcement learning . Supervised learning trains models on labeled data sets, enabling them to accurately recognize patterns, predict outcomes or classify new data. Unsupervised learning trains models to sort through unlabeled data sets to find underlying relationships or clusters. Reinforcement learning takes a different approach, in which models learn to make decisions by acting as agents and receiving feedback on their actions. There is also semi-supervised learning , which combines aspects of supervised and unsupervised approaches. This technique uses a small amount of labeled data and a larger amount of unlabeled data, thereby improving learning accuracy while reducing the need for labeled data, which can be time and labor intensive to procure. Computer vision Computer vision is a field of AI that focuses on teaching machines how to interpret the visual world. By analyzing visual information such as camera images and videos using deep learning models, computer vision systems can learn to identify and classify objects and make decisions based on those analyses. The primary aim of computer vision is to replicate or improve on the human visual system using AI algorithms. Computer vision is used in a wide range of applications, from signature identification to medical image analysis to autonomous vehicles. Machine vision, a term often conflated with computer vision, refers specifically to the use of computer vision to analyze camera and video data in industrial automation contexts, such as production processes in manufacturing. Natural language processing NLP refers to the processing of human language by computer programs. NLP algorithms can interpret and interact with human language, performing tasks such as translation, speech recognition and sentiment analysis . One of the oldest and best-known examples of NLP is spam detection, which looks at the subject line and text of an email and decides whether it is junk. More advanced applications of NLP include LLMs such as ChatGPT and Anthropic\\'s Claude . Robotics Robotics is a field of engineering that focuses on the design, manufacturing and operation of robots: automated machines that replicate and replace human actions, particularly those that are difficult, dangerous or tedious for humans to perform. Examples of robotics applications include manufacturing, where robots perform repetitive or hazardous assembly-line tasks, and exploratory missions in distant, difficult-to-access areas such as outer space and the deep sea. The integration of AI and machine learning significantly expands robots\\' capabilities by enabling them to make better-informed autonomous decisions and adapt to new situations and data. For example, robots with machine vision capabilities can learn to sort objects on a factory line by shape and color. Autonomous vehicles Autonomous vehicles, more colloquially known as self-driving cars, can sense and navigate their surrounding environment with minimal or no human input. These vehicles rely on a combination of technologies, including radar, GPS, and a range of AI and machine learning algorithms, such as image recognition . These algorithms learn from real-world driving, traffic and map data to make informed decisions about when to brake, turn and accelerate; how to stay in a given lane; and how to avoid unexpected obstructions, including pedestrians. Although the technology has advanced considerably in recent years, the ultimate goal of an autonomous vehicle that can fully replace a human driver has yet to be achieved. Generative AI The term generative AI refers to machine learning systems that can generate new data from text prompts -- most commonly text and images, but also audio, video, software code, and even genetic sequences and protein structures. Through training on massive data sets, these algorithms gradually learn the patterns of the types of media they will be asked to generate, enabling them later to create new content that resembles that training data. Generative AI saw a rapid growth in popularity following the introduction of widely available text and image generators in 2022, such as ChatGPT, Dall-E and Midjourney, and is increasingly applied in business settings. While many generative AI tools\\' capabilities are impressive, they also raise concerns around issues such as copyright, fair use and security that remain a matter of open debate in the tech sector. AI has entered a wide variety of industry sectors and research areas. AI in healthcare AI is applied to a range of tasks in the healthcare domain, with the overarching goals of improving patient outcomes and reducing systemic costs. One major application is the use of machine learning models trained on large medical data sets to assist healthcare professionals in making better and faster diagnoses. On the patient side, online virtual health assistants and chatbots can provide general medical information, schedule appointments, explain billing processes and complete other administrative tasks. Predictive modeling AI algorithms can also be used to combat the spread of pandemics such as COVID-19 . AI in business AI is increasingly integrated into various business functions and industries, aiming to improve efficiency, customer experience, strategic planning and decision-making. For example, machine learning models power many of today\\'s data analytics and customer relationship management ( CRM ) platforms, helping companies understand how to best serve customers through personalizing offerings and delivering better-tailored marketing. Virtual assistants and chatbots are also deployed on corporate websites and in mobile applications to provide round-the-clock customer service and answer common questions. In addition, more and more companies are exploring the capabilities of generative AI tools such as ChatGPT for automating tasks such as document drafting and summarization, product design and ideation, and computer programming. AI in education AI has a number of potential applications in education technology. It can automate aspects of grading processes, giving educators more time for other tasks. AI tools can also assess students\\' performance and adapt to their individual needs, facilitating more personalized learning experiences that enable students to work at their own pace. AI tutors could also provide additional support to students, ensuring they stay on track. The technology could also change where and how students learn, perhaps altering the traditional role of educators. As the capabilities of LLMs such as ChatGPT and Google Gemini grow, such tools could help educators craft teaching materials and engage students in new ways. However, the advent of these tools also forces educators to reconsider homework and testing practices and revise plagiarism policies, especially given that AI detection and AI watermarking tools are currently unreliable. AI in finance and banking Banks and other financial organizations use AI to improve their decision-making for tasks such as granting loans, setting credit limits and identifying investment opportunities. In addition, algorithmic trading powered by advanced AI and machine learning has transformed financial markets, executing trades at speeds and efficiencies far surpassing what human traders could do manually. AI and machine learning have also entered the realm of consumer finance. For example, banks use AI chatbots to inform customers about services and offerings and to handle transactions and questions that don\\'t require human intervention. Similarly, Intuit offers generative AI features within its TurboTax e-filing product that provide users with personalized advice based on data such as the user\\'s tax profile and the tax code for their location. AI in law AI is changing the legal sector by automating labor-intensive tasks such as document review and discovery response, which can be tedious and time consuming for attorneys and paralegals. Law firms today use AI and machine learning for a variety of tasks, including analytics and predictive AI to analyze data and case law, computer vision to classify and extract information from documents, and NLP to interpret and respond to discovery requests. In addition to improving efficiency and productivity, this integration of AI frees up human legal professionals to spend more time with clients and focus on more creative, strategic work that AI is less well suited to handle. With the rise of generative AI in law , firms are also exploring using LLMs to draft common documents, such as boilerplate contracts. AI in entertainment and media The entertainment and media business uses AI techniques in targeted advertising, content recommendations, distribution and fraud detection. The technology enables companies to personalize audience members\\' experiences and optimize delivery of content. Generative AI is also a hot topic in the area of content creation. Advertising professionals are already using these tools to create marketing collateral and edit advertising images. However, their use is more controversial in areas such as film and TV scriptwriting and visual effects, where they offer increased efficiency but also threaten the livelihoods and intellectual property of humans in creative roles . AI in journalism In journalism, AI can streamline workflows by automating routine tasks, such as data entry and proofreading. Investigative journalists and data journalists also use AI to find and research stories by sifting through large data sets using machine learning models, thereby uncovering trends and hidden connections that would be time consuming to identify manually. For example, five finalists for the 2024 Pulitzer Prizes for journalism disclosed using AI in their reporting to perform tasks such as analyzing massive volumes of police records. While the use of traditional AI tools is increasingly common, the use of generative AI to write journalistic content is open to question, as it raises concerns around reliability, accuracy and ethics. AI in software development and IT AI is used to automate many processes in software development, DevOps and IT . For example, AIOps tools enable predictive maintenance of IT environments by analyzing system data to forecast potential issues before they occur, and AI-powered monitoring tools can help flag potential anomalies in real time based on historical system data. Generative AI tools such as GitHub Copilot and Tabnine are also increasingly used to produce application code based on natural-language prompts. While these tools have shown early promise and interest among developers , they are unlikely to fully replace software engineers. Instead, they serve as useful productivity aids, automating repetitive tasks and boilerplate code writing. AI in security AI and machine learning are prominent buzzwords in security vendor marketing, so buyers should take a cautious approach. Still, AI is indeed a useful technology in multiple aspects of cybersecurity , including anomaly detection, reducing false positives and conducting behavioral threat analytics. For example, organizations use machine learning in security information and event management ( SIEM ) software to detect suspicious activity and potential threats. By analyzing vast amounts of data and recognizing patterns that resemble known malicious code, AI tools can alert security teams to new and emerging attacks, often much sooner than human employees and previous technologies could. AI in manufacturing Manufacturing has been at the forefront of incorporating robots into workflows, with recent advancements focusing on collaborative robots, or cobots . Unlike traditional industrial robots, which were programmed to perform single tasks and operated separately from human workers, cobots are smaller, more versatile and designed to work alongside humans. These multitasking robots can take on responsibility for more tasks in warehouses, on factory floors and in other workspaces, including assembly, packaging and quality control. In particular, using robots to perform or assist with repetitive and physically demanding tasks can improve safety and efficiency for human workers. AI in transportation In addition to AI\\'s fundamental role in operating autonomous vehicles, AI technologies are used in automotive transportation to manage traffic, reduce congestion and enhance road safety. In air travel, AI can predict flight delays by analyzing data points such as weather and air traffic conditions. In overseas shipping, AI can enhance safety and efficiency by optimizing routes and automatically monitoring vessel conditions. In supply chains, AI is replacing traditional methods of demand forecasting and improving the accuracy of predictions about potential disruptions and bottlenecks. The COVID-19 pandemic highlighted the importance of these capabilities, as many companies were caught off guard by the effects of a global pandemic on the supply and demand of goods. Augmented intelligence vs. artificial intelligence The term artificial intelligence is closely linked to popular culture, which could create unrealistic expectations among the general public about AI\\'s impact on work and daily life. A proposed alternative term, augmented intelligence , distinguishes machine systems that support humans from the fully autonomous systems found in science fiction -- think HAL 9000 from 2001: A Space Odyssey or Skynet from the Terminator movies. The two terms can be defined as follows: Augmented intelligence. With its more neutral connotation, the term augmented intelligence suggests that most AI implementations are designed to enhance human capabilities, rather than replace them. These narrow AI systems primarily improve products and services by performing specific tasks. Examples include automatically surfacing important data in business intelligence reports or highlighting key information in legal filings. The rapid adoption of tools like ChatGPT and Gemini across various industries indicates a growing willingness to use AI to support human decision-making. In this framework, the term AI would be reserved for advanced general AI in order to better manage the public\\'s expectations and clarify the distinction between current use cases and the aspiration of achieving AGI. The concept of AGI is closely associated with the concept of the technological singularity -- a future wherein an artificial superintelligence far surpasses human cognitive abilities, potentially reshaping our reality in ways beyond our comprehension. The singularity has long been a staple of science fiction, but some AI developers today are actively pursuing the creation of AGI. Ethical use of artificial intelligence While AI tools present a range of new functionalities for businesses , their use raises significant ethical questions. For better or worse, AI systems reinforce what they have already learned, meaning that these algorithms are highly dependent on the data they are trained on. Because a human being selects that training data, the potential for bias is inherent and must be monitored closely. These tools can produce highly realistic and convincing text, images and audio -- a useful capability for many legitimate applications, but also a potential vector of misinformation and harmful content such as deepfakes . Consequently, anyone looking to use machine learning in real-world production systems needs to factor ethics into their AI training processes and strive to avoid unwanted bias. This is especially important for AI algorithms that lack transparency, such as complex neural networks used in deep learning. Explainability , or the ability to understand how an AI system makes decisions, is a growing area of interest in AI research. Lack of explainability presents a potential stumbling block to using AI in industries with strict regulatory compliance requirements. For example, fair lending laws require U.S. financial institutions to explain their credit-issuing decisions to loan and credit card applicants. When AI programs make such decisions, however, the subtle correlations among thousands of variables can create a black-box problem, where the system\\'s decision-making process is opaque. In summary, AI\\'s ethical challenges include the following: Bias due to improperly trained algorithms and human prejudices or oversights. Job displacement due to increasing use of AI to automate workplace tasks. Data privacy concerns , particularly in fields such as banking, healthcare and legal that deal with sensitive personal data. AI governance and regulations Despite potential risks, there are currently few regulations governing the use of AI tools, and where laws do exist, they typically pertain to AI indirectly. For example, as previously mentioned, U.S. fair lending regulations such as the Equal Credit Opportunity Act require financial institutions to explain credit decisions to potential customers. This limits the extent to which lenders can use deep learning algorithms, which by their nature are opaque and lack explainability. The European Union has been proactive in addressing AI governance. The EU\\'s General Data Protection Regulation ( GDPR ) already imposes strict limits on how enterprises can use consumer data, affecting the training and functionality of many consumer-facing AI applications. In addition, the Council of the EU has approved the AI Act , which aims to establish a comprehensive regulatory framework for AI development and deployment. The Act imposes varying levels of regulation on AI systems based on their riskiness, with areas such as biometrics and critical infrastructure receiving greater scrutiny. While the U.S. is making progress, the country still lacks comprehensive federal legislation akin to the EU\\'s AI Act. Policymakers have yet to issue comprehensive AI legislation , and existing federal-level regulations focus on specific use cases and risk management, complemented by state initiatives. That said, the EU\\'s more stringent regulations could end up setting de facto standards for multinational companies based in the U.S., similar to how GDPR shaped the global data privacy landscape. With regard to specific U.S. AI policy developments, the White House Office of Science and Technology Policy published a \"Blueprint for an AI Bill of Rights\" in October 2022, providing guidance for businesses on how to implement ethical AI systems. The U.S. Chamber of Commerce also called for AI regulations in a report released in March 2023, emphasizing the need for a balanced approach that fosters competition while addressing risks. More recently, in October 2023, President Biden issued an executive order on the topic of secure and responsible AI development. Among other things, the order directed federal agencies to take certain actions to assess and manage AI risk and developers of powerful AI systems to report safety test results. Crafting laws to regulate AI will not be easy, partly because AI comprises a variety of technologies used for different purposes, and partly because regulations can stifle AI progress and development, sparking industry backlash. The rapid evolution of AI technologies is another obstacle to forming meaningful regulations, as is AI\\'s lack of transparency, which makes it difficult to understand how algorithms arrive at their results. Moreover, technology breakthroughs and novel applications such as ChatGPT and Dall-E can quickly render existing laws obsolete. And, of course, laws and other regulations are unlikely to deter malicious actors from using AI for harmful purposes . These are commonly described as the four main types of AI. The concept of inanimate objects endowed with intelligence has been around since ancient times. The Greek god Hephaestus was depicted in myths as forging robot-like servants out of gold, while engineers in ancient Egypt built statues of gods that could move, animated by hidden mechanisms operated by priests. Throughout the centuries, thinkers from the Greek philosopher Aristotle to the 13th-century Spanish theologian Ramon Llull to mathematician René Descartes and statistician Thomas Bayes used the tools and logic of their times to describe human thought processes as symbols. Their work laid the foundation for AI concepts such as general knowledge representation and logical reasoning. The late 19th and early 20th centuries brought forth foundational work that would give rise to the modern computer. In 1836, Cambridge University mathematician Charles Babbage and Augusta Ada King, Countess of Lovelace, invented the first design for a programmable machine, known as the Analytical Engine. Babbage outlined the design for the first mechanical computer, while Lovelace -- often considered the first computer programmer -- foresaw the machine\\'s capability to go beyond simple calculations to perform any operation that could be described algorithmically. As the 20th century progressed, key developments in computing shaped the field that would become AI . In the 1930s, British mathematician and World War II codebreaker Alan Turing introduced the concept of a universal machine that could simulate any other machine. His theories were crucial to the development of digital computers and, eventually, AI. 1940s Princeton mathematician John Von Neumann conceived the architecture for the stored-program computer -- the idea that a computer\\'s program and the data it processes can be kept in the computer\\'s memory. Warren McCulloch and Walter Pitts proposed a mathematical model of artificial neurons, laying the foundation for neural networks and other future AI developments. 1950s With the advent of modern computers, scientists began to test their ideas about machine intelligence. In 1950, Turing devised a method for determining whether a computer has intelligence, which he called the imitation game but has become more commonly known as the Turing test . This test evaluates a computer\\'s ability to convince interrogators that its responses to their questions were made by a human being. The modern field of AI is widely cited as beginning in 1956 during a summer conference at Dartmouth College. Sponsored by the Defense Advanced Research Projects Agency, the conference was attended by 10 luminaries in the field, including AI pioneers Marvin Minsky, Oliver Selfridge and John McCarthy , who is credited with coining the term \"artificial intelligence.\" Also in attendance were Allen Newell, a computer scientist, and Herbert A. Simon, an economist, political scientist and cognitive psychologist. The two presented their groundbreaking Logic Theorist, a computer program capable of proving certain mathematical theorems and often referred to as the first AI program. A year later, in 1957, Newell and Simon created the General Problem Solver algorithm that, despite failing to solve more complex problems, laid the foundations for developing more sophisticated cognitive architectures. 1960s In the wake of the Dartmouth College conference, leaders in the fledgling field of AI predicted that human-created intelligence equivalent to the human brain was around the corner, attracting major government and industry support. Indeed, nearly 20 years of well-funded basic research generated significant advances in AI. McCarthy developed Lisp , a language originally designed for AI programming that is still used today. In the mid-1960s, MIT professor Joseph Weizenbaum developed Eliza, an early NLP program that laid the foundation for today\\'s chatbots. 1970s In the 1970s, achieving AGI proved elusive, not imminent, due to limitations in computer processing and memory as well as the complexity of the problem. As a result, government and corporate support for AI research waned, leading to a fallow period lasting from 1974 to 1980 known as the first AI winter . During this time, the nascent field of AI saw a significant decline in funding and interest. 1980s In the 1980s, research on deep learning techniques and industry adoption of Edward Feigenbaum\\'s expert systems sparked a new wave of AI enthusiasm. Expert systems, which use rule-based programs to mimic human experts\\' decision-making, were applied to tasks such as financial analysis and clinical diagnosis. However, because these systems remained costly and limited in their capabilities, AI\\'s resurgence was short-lived, followed by another collapse of government funding and industry support. This period of reduced interest and investment, known as the second AI winter, lasted until the mid-1990s. 1990s Increases in computational power and an explosion of data sparked an AI renaissance in the mid- to late 1990s, setting the stage for the remarkable advances in AI we see today. The combination of big data and increased computational power propelled breakthroughs in NLP, computer vision, robotics, machine learning and deep learning. A notable milestone occurred in 1997, when Deep Blue defeated Kasparov, becoming the first computer program to beat a world chess champion. 2000s Further advances in machine learning, deep learning, NLP, speech recognition and computer vision gave rise to products and services that have shaped the way we live today. Major developments include the 2000 launch of Google\\'s search engine and the 2001 launch of Amazon\\'s recommendation engine. Also in the 2000s, Netflix developed its movie recommendation system, Facebook introduced its facial recognition system and Microsoft launched its speech recognition system for transcribing audio. IBM launched its Watson question-answering system, and Google started its self-driving car initiative, Waymo. These include the launch of Apple\\'s Siri and Amazon\\'s Alexa voice assistants; IBM Watson\\'s victories on Jeopardy ; the development of self-driving features for cars; and the implementation of AI-based systems that detect cancers with a high degree of accuracy. The first generative adversarial network was developed, and Google launched TensorFlow, an open source machine learning framework that is widely used in AI development. A key milestone occurred in 2012 with the groundbreaking AlexNet, a convolutional neural network that significantly advanced the field of image recognition and popularized the use of GPUs for AI model training. In 2016, Google DeepMind\\'s AlphaGo model defeated world Go champion Lee Sedol, showcasing AI\\'s ability to master complex strategic games. The previous year saw the founding of research lab OpenAI , which would make important strides in the second half of that decade in reinforcement learning and NLP. 2020s The current decade has so far been dominated by the advent of generative AI, which can produce new content based on a user\\'s prompt. These prompts often take the form of text, but they can also be images, videos, design blueprints, music or any other input that the AI system can process. Output content can range from essays to problem-solving explanations to realistic images based on pictures of a person. In 2020, OpenAI released the third iteration of its GPT language model, but the technology did not fully reach public awareness until 2022. That year saw the launch of publicly available image generators, such as Dall-E and Midjourney, as well as the general release of ChatGPT. Since then, the abilities of LLM-powered chatbots such as ChatGPT and Claude -- along with image, video and audio generators -- have captivated the public. However, generative AI technology is still in its early stages, as evidenced by its ongoing tendency to hallucinate or skew answers. AI tools and services: Evolution and ecosystems AI tools and services are evolving at a rapid rate. Current innovations can be traced back to the 2012 AlexNet neural network, which ushered in a new era of high-performance AI built on GPUs and large data sets. The key advancement was the discovery that neural networks could be trained on massive amounts of data across multiple GPU cores in parallel, making the training process more scalable. In the 21st century, a symbiotic relationship has developed between algorithmic advancements at organizations like Google, Microsoft and OpenAI, on the one hand, and the hardware innovations pioneered by infrastructure providers like Nvidia, on the other. These developments have made it possible to run ever-larger AI models on more connected GPUs, driving game-changing improvements in performance and scalability. Collaboration among these AI luminaries was crucial to the success of ChatGPT, not to mention dozens of other breakout AI services. Here are some examples of the innovations that are driving the evolution of AI tools and services. Transformers Google led the way in finding a more efficient process for provisioning AI training across large clusters of commodity PCs with GPUs. This, in turn, paved the way for the discovery of transformers, which automate many aspects of training AI on unlabeled data. With the 2017 paper \"Attention Is All You Need,\" Google researchers introduced a novel architecture that uses self-attention mechanisms to improve model performance on a wide range of NLP tasks, such as translation, text generation and summarization. Hardware optimization Hardware is equally important to algorithmic architecture in developing effective, efficient and scalable AI. GPUs, originally designed for graphics rendering, have become essential for processing massive data sets. Tensor processing units, designed specifically for deep learning, have sped up the training of complex AI models. Vendors like Nvidia have optimized the microcode for running across multiple GPU cores in parallel for the most popular algorithms. Chipmakers are also working with major cloud providers to make this capability more accessible as AI as a service (AIaaS) through IaaS, SaaS and PaaS models. Generative pre-trained transformers The AI stack has evolved rapidly over the last few years. Now, vendors such as OpenAI, Nvidia, Microsoft and Google provide generative pre-trained transformers (GPTs) that can be fine-tuned for specific tasks with dramatically reduced costs, expertise and time. AI cloud services One of the biggest roadblocks preventing enterprises from effectively using AI is the complexity of data engineering and data science tasks required to weave AI capabilities into new or existing applications. All leading cloud providers are rolling out branded AIaaS offerings to streamline data prep, model development and application deployment. Top examples include Amazon AI , Google AI , Microsoft Azure AI, IBM Watson and Oracle Cloud \\'s AI features. Cutting-edge AI models as a service Leading AI model developers also offer cutting-edge AI models on top of these cloud services. OpenAI has multiple LLMs optimized for chat, NLP, multimodality and code generation that are provisioned through Azure. Nvidia has pursued a more cloud-agnostic approach by selling AI infrastructure and foundational models optimized for text, images and medical data across all cloud providers. Many smaller players also offer models customized for various industries and use cases. This was last updated in June 2024 Continue Reading About What is Artificial Intelligence (AI)? Top AI and machine learning trends How businesses can measure AI success with KPIs Steps to achieve AI implementation in your business What is trustworthy AI and why is it important? The future of AI: What to expect in the next 5 years Related Terms What are Google\\'s AI Overviews (Formerly SGE)? Google\\'s AI Overviews are a feature in Google search that uses generative AI (GenAI) to deliver short synopses of topics ... See complete definition What is a backpropagation algorithm? A backpropagation algorithm, or backward propagation of errors, is an algorithm that\\'s used to help train neural network models. See complete definition What is a voice user interface (VUI)? A voice user interface (VUI) is a type of interface that relies on speech recognition technology to enable users to interact with... See complete definition Dig Deeper on AI technologies Physical AI explained: Everything you need to know By: Andy Patrizio An explanation of the different types of AI By: Sabrina Polin Conversational AI vs. generative AI: What\\'s the difference? By: Amanda Hetler An introduction to AI By: Samantha Poutre Sponsored News Power Your Generative AI Initiatives With High-Performance, Reliable, ... –Dell Technologies and Intel A Generative AI Use Case Brought to Life with Solutions from Dell Technologies –Dell Technologies and Intel See More Vendor Resources Human-like AI quest drives general AI development efforts –TechTarget ComputerWeekly.com The Top 5 Benefits of AI in Banking and Finance –TechTarget Business Analytics You need analytics governance Analytics governance might not seem exciting, but it can improve innovation and mitigate risks. It\\'s also critical to responsible... Collaborative business intelligence helps users connect data dots Social BI enables users to interact with their organization\\'s data -- and data experts -- in applications where they already ... AR and VR data visualizations offer promising future AR and VR data visualizations offer a new perspective to capture patterns and trends in complex data sets that traditional data ... CIO Where 2024 U.S. presidential candidates stand on tech issues The next U.S. president will set the tone on tech issues such as AI regulation, data privacy and climate tech. This guide breaks ... EU, Calif. climate risk rules prompt companies to prepare A challenge companies are facing while preparing for compliance with climate risk reporting rules is a lack of consistency among ... Learn about the negative impacts of technical debt Key leadership decisions like poor architecture to rushed processes can lead to technical debt that will affect a company ... Data Management Use RAG with LLMs to democratize data analytics Pairing retrieval-augmented generation with an LLM helps improve prompts and outputs, democratizing data access and making ... 10 top vector database options for similarity searches Vector databases excel in different areas of vector searches, including sophisticated text and visual options. Choose the ... Generative AI shines spotlight on data governance and trust Generative AI creates new opportunities for how organizations use data. Strong data governance is necessary to build trust in the... ERP As AI evolves, manufacturing will face an old data problem New capabilities in AI technology hold promise for manufacturers, but companies should proceed carefully until issues such as ... 7 benefits of using a 3PL provider for reverse logistics A 3PL with experience working with supply chain partners and expertise in returns can help simplify a company\\'s operations. Learn... 8 top enterprise asset management software products Neglecting enterprise asset management can lead to higher equipment costs and delayed operations. Definition, Uses, and Types Written by Coursera Staff â\\x80¢ Updated on Apr 3, 2024 Learn what artificial intelligence actually is, how itâ\\x80\\x99s used today, and what it may do in the future. Artificial intelligence (AI) refers to computer systems capable of performing complex tasks that historically only a human could do, such as reasoning, making decisions, or solving problems.Â Today, the term â\\x80\\x9cAIâ\\x80\\x9d describes a wide range of technologies that power many of the services and goods we use every day â\\x80\\x93 from apps that recommend tv shows to chatbots that provide customer support in real time. But do all of these really constitute artificial intelligence as most of us envision it? And if not, then why do we use the term so often?Â In this article, youâ\\x80\\x99ll learn more about artificial intelligence, what it actually does, and different types of it. In the end, youâ\\x80\\x99ll also learn about some of its benefits and dangers and explore flexible courses that can help you expand your knowledge of AI even further.Â Â Want to try out your AI skills? In just 6 hours , you\\'ll gain foundational knowledge about AI terminology , strategy , and the workflow of machine learning projects . Artificial intelligence (AI) is the theory and development of computer systems capable of performing tasks that historically required human intelligence, such as recognizing speech, making decisions, and identifying patterns. AI is an umbrella term that encompasses a wide variety of technologies, including machine learning , deep learning , and natural language processing (NLP) .Â Although the term is commonly used to describe a range of different technologies in use today, many disagree on whether these actually constitute artificial intelligence. Instead, some argue that much of the technology used in the real world today actually constitutes highly advanced machine learning that is simply a first step towards true artificial intelligence, or â\\x80\\x9cgeneral artificial intelligenceâ\\x80\\x9d (GAI). Yet, despite the many philosophical disagreements over whether â\\x80\\x9ctrueâ\\x80\\x9d intelligent machines actually exist, when most people use the term AI today, theyâ\\x80\\x99re referring to a suite of machine learning-powered technologies, such as Chat GPT or computer vision, that enable machines to perform tasks that previously only humans can do like generating written content, steering a car, or analyzing data.Â Artificial intelligence examplesÂ Though the humanoid robots often associated with AI (think Star Trek: The Next Generationâ\\x80\\x99s Data or Terminatorâ\\x80\\x99s Â T-800) donâ\\x80\\x99t exist yet, youâ\\x80\\x99ve likely interacted with machine learning-powered services or devices many times before.Â At the simplest level, machine learning uses algorithms trained on data sets to create machine learning models that allow computer systems to perform tasks like making song recommendations, identifying the fastest way to travel to a destination, or translating text from one language to another. Some of the most common examples of AI in use today include:Â ChatGPT : Uses large language models (LLMs) to generate text in response to questions or comments posed to it.Â Google Translate: Uses deep learning algorithms to translate text from one language to another.Â Netflix: Uses machine learning algorithms to create personalized recommendation engines for users based on their previous viewing history.Â Tesla: Uses computer vision to power self-driving features on their cars.Â Read more: Deep Learning vs. Machine Learning: Beginnerâ\\x80\\x99s Guide The increasing accessibility of generative AI tools has made it an in-demand skill for many tech roles . If you\\'re interested in learning to work with AI for your career, you might consider a free, beginner-friendly online program like Google\\'s Introduction to Generative AI . Automating tasks that don\\'t require human intervention saves money and time, and can reduce the risk of human error. Here are a couple of ways AI could be employed in different industries: Finance industry. Fraud detection is a notable use case for AI in the finance industry. AI\\'s capability to analyze large amounts of data enables it to detect anomalies or patterns that signal fraudulent behavior. AI-powered robotics could support surgeries close to highly delicate organs or tissue to mitigate blood loss or risk of infection. It\\'s a low-commitment way to stay current with industry trends and skills you can use to guide your career path. What is artificial general intelligence (AGI)?Â Artificial general intelligence (AGI) refers to a theoretical state in which computer systems will be able to achieve or exceed human intelligence. In other words, AGI is â\\x80\\x9ctrueâ\\x80\\x9d artificial intelligence as depicted in countless science fiction novels, television shows, movies, and comics.Â As for the precise meaning of â\\x80\\x9cAIâ\\x80\\x9d itself, researchers donâ\\x80\\x99t quite agree on how we would recognize â\\x80\\x9ctrueâ\\x80\\x9d artificial general intelligence when it appears. However, the most famous approach to identifying whether a machine is intelligent or not is known as the Turing Test or Imitation Game, an experiment that was first outlined by influential mathematician, computer scientist, and cryptanalyst Alan Turing in a 1950 paper on computer intelligence. There, Turing described a three-player game in which a human â\\x80\\x9cinterrogatorâ\\x80\\x9d is asked to communicate via text with another human and a machine and judge who composed each response. If the interrogator cannot reliably identify the human, then Turing says the machine can be said to be intelligent [ 1 ].Â To complicate matters, researchers and philosophers also canâ\\x80\\x99t quite agree whether weâ\\x80\\x99re beginning to achieve AGI, if itâ\\x80\\x99s still far off, or just totally impossible. For example, while a recent paper from Microsoft Research and OpenAI argues that Chat GPT-4 is an early form of AGI, many other researchers are skeptical of these claims and argue that they were just made for publicity [ 2 , 3 ]. Regardless of how far we are from achieving AGI, you can assume that when someone uses the term artificial general intelligence, theyâ\\x80\\x99re referring to the kind of sentient computer programs and machines that are commonly found in popular science fiction.Â Strong AI vs. Weak AI When researching artificial intelligence, you might have come across the terms â\\x80\\x9cstrongâ\\x80\\x9d and â\\x80\\x9cweakâ\\x80\\x9d AI. Though these terms might seem confusing, you likely already have a sense of what they mean.Â Strong AI is essentially AI that is capable of human-level, general intelligence. In other words, itâ\\x80\\x99s just another way to say â\\x80\\x9cartificial general intelligence.â\\x80\\x9dÂ Weak AI , meanwhile, refers to the narrow use of widely available AI technology, like machine learning or deep learning, to perform very specific tasks, such as playing chess, recommending songs, or steering cars. Also known as Artificial Narrow Intelligence (ANI), weak AI is essentially the kind of AI we use daily. Read more: Machine Learning vs. AI: Differences, Uses, and Benefits The 4 Types of AIÂ As researchers attempt to build more advanced forms of artificial intelligence, they must also begin to formulate more nuanced understandings of what intelligence or even consciousness precisely mean. In their attempt to clarify these concepts, researchers have outlined four types of artificial intelligence . Hereâ\\x80\\x99s a summary of each AI type, according to Professor Arend Hintze of the University of Michigan [ 4 ]:Â 1. Reactive machines Reactive machines are the most basic type of artificial intelligence. Machines built in this way donâ\\x80\\x99t possess any knowledge of previous events but instead only â\\x80\\x9creactâ\\x80\\x9d to what is before them in a given moment. As a result, they can only perform certain advanced tasks within a very narrow scope, such as playing chess, and are incapable of performing tasks outside of their limited context.Â 2. Limited memory machines Machines with limited memory possess a limited understanding of past events. They can interact more with the world around them than reactive machines can. For example, self-driving cars use a form of limited memory to make turns, observe approaching vehicles, and adjust their speed. However, machines with only limited memory cannot form a complete understanding of the world because their recall of past events is limited and only used in a narrow band of time.Â 3. Theory of mind machines Machines that possess a â\\x80\\x9ctheory of mindâ\\x80\\x9d represent an early form of artificial general intelligence. In addition to being able to create representations of the world, machines of this type would also have an understanding of other entities that exist within the world. Self-aware machines Machines with self-awareness are the theoretically most advanced type of AI and would possess an understanding of the world, others, and itself. This is what most people mean when they talk about achieving AGI. Currently, this is a far-off reality.Â AI benefits and dangers AI has a range of applications with the potential to transform how we work and our daily lives. While many of these transformations are exciting, like self-driving cars, virtual assistants, or wearable devices in the healthcare industry, they also pose many challenges. Itâ\\x80\\x99s a complicated picture that often summons competing images: a utopia for some, a dystopia for others. Here are a few of the possible benefits and dangers AI may pose:Â Potential Benefits Potential Dangers Greater accuracy for certain repeatable tasks, such as assembling vehicles or computers. Potential for bias or discrimination as a result of the data set on which the AI is trained. Lack of transparency over how decisions are arrived at, resulting in less than optimal solutions. Ability to quickly generate new content, such as text or images. Potential to create misinformation, as well as inadvertently violate laws and regulations. These are just some of the ways that AI provides benefits and dangers to society. When using new technologies like AI, itâ\\x80\\x99s best to keep a clear mind about what it is and isnâ\\x80\\x99t. With great power comes great responsibility, after all.Â Read more: AI Ethics: What It Is and Why It Matters Build AI skills on Coursera Artificial Intelligence is quickly changing the world we live in. If youâ\\x80\\x99re interested in learning more about AI and how you can use it at work or in your own life, consider taking a relevant course on Coursera today.Â In DeepLearning.AIâ\\x80\\x99s AI For Everyone course , youâ\\x80\\x99ll learn what AI can realistically do and not do, how to spot opportunities to apply AI to problems in your own organization, and what it feels like to build machine learning and data science projects.Â In DeepLearning.AIâ\\x80\\x99s AI For Good Specialization , meanwhile, youâ\\x80\\x99ll build skills combining human and machine intelligence for positive real-world impact using AI in a beginner-friendly, three-course program.Â Article sources 1 .Â UMBC. â\\x80\\x9c Sparks of Artificial General Intelligence: Early experiments with GPT-4 , https://arxiv.org/abs/2303.12712.â\\x80\\x9d Accessed March 30, 2024. â\\x80\\x9c Understanding the Four Types of Artificial Intelligence , https://www.govtech.com/computing/understanding-the-four-types-of-artificial-intelligence.html.â\\x80\\x9d Accessed March 30, 2024. View all sources Keep reading Updated on Apr 3, 2024 Written by: C Coursera Staff Editorial Team Courseraâ\\x80\\x99s editorial team is comprised of highly experienced professional editors, writers, and fact... Learners are advised to conduct additional research to ensure that courses and other credentials pursued meet their personal, professional, and financial goals. | McKinsey Skip to main content What is AI (artificial intelligence)? April 3, 2024 | Article Artificial intelligence is a machineâ\\x80\\x99s ability to perform some cognitive functions we usually associate with human minds. 3D robotics hand (10 pages) Humans and machines: a match made in productivity Â heaven. From the wheel that revolutionized agriculture to the screw that held together increasingly complex construction projects to the robot-enabled assembly lines of today, machines have made life as we know it possible. And yet, despite their seemingly endless utility, humans have long feared machinesâ\\x80\\x94more specifically, the possibility that machines might someday acquire human intelligence Â and strike out on their own. Get to know and directly engage with senior McKinsey experts on AI Sven Blumberg is a senior partner in McKinsey’s Düsseldorf office; Michael Chui is a partner at the McKinsey Global Institute and is based in the Bay Area office, where Lareina Yee is a senior partner; Kia Javanmardian is a senior partner in the Chicago office, where Alex Singla , the global leader of QuantumBlack, AI by McKinsey, is also a senior partner; Kate Smaje and Alex Sukharevsky are senior partners in the London office. But we tend to view the possibility of sentient machines with fascination as well as fear. Twentieth-century theoreticians, like computer scientist and mathematician Alan Turing, envisioned a future where machines could perform functions faster than humans. The work of Turing and others soon made this a reality. Personal calculators became widely available in the 1970s, and by 2016, the US census showed that 89 percent of American households had a computer. Machinesâ\\x80\\x94 smart machines at thatâ\\x80\\x94are now just an ordinary part of our lives and culture. Those smart machines are also getting faster and more complex. Some computers have now crossed the exascale threshold, meaning they can perform as many calculations in a single second as an individual could in 31,688,765,000 years . And beyond computation, which machines have long been faster at than we have, computers and other devices are now acquiring skills and perception that were once unique to humans and a few other species. About QuantumBlack, AI by McKinsey QuantumBlack, McKinseyâ\\x80\\x99s AI arm, helps companies transform using the power of technology, technical expertise, and industry experts. With thousands of practitioners at QuantumBlack (data engineers, data scientists, product managers, designers, and software engineers) and McKinsey (industry and domain experts), we are working to solve the worldâ\\x80\\x99s most important AI challenges. QuantumBlack Labs is our center of technology development and client innovation, which has been driving cutting-edge advancements and developments in AI through locations across the globe. AI is a machineâ\\x80\\x99s ability to perform the cognitive functions we associate with human minds, such as perceiving, reasoning, learning, interacting with the environment, problem-solving, and even exercising creativity. Youâ\\x80\\x99ve probably interacted with AI even if you donâ\\x80\\x99t realize itâ\\x80\\x94voice assistants like Siri and Alexa are founded on AI technology, as are some customer service chatbots that pop up to help you navigate websites. Applied AI â\\x80\\x94simply, artificial intelligence applied to real-world problemsâ\\x80\\x94has serious implications for the business world. By using artificial intelligence, companies have the potential to make business more efficient and profitable. But ultimately, the value of AI isnâ\\x80\\x99t in the systems themselves. Rather, itâ\\x80\\x99s in how companies use these systems to assist humansâ\\x80\\x94and their ability to explain to shareholders and the public what these systems doâ\\x80\\x94in a way that builds trust and confidence. For more about AI, its history, its future, and how to apply it in business, read on. Machine learning is a form of artificial intelligence that can adapt to a wide range of inputs, including large sets of historical data, synthesized data, or human inputs. (Some machine learning algorithms are specialized in training themselves to detect patterns; this is called deep learning . These algorithms can detect patterns and learn how to make predictions and recommendations by processing data, rather than by receiving explicit programming instruction. Some algorithms can also adapt in response to new data and experiences to improve over time. 1 The volume and complexity of data that is now being generated, too vast for humans to process and apply efficiently, has increased the potential of machine learning, as well as the need for it. In the years since its widespread deployment, which began in the 1970s, machine learning has had an impact on a number of industries, including achievements in medical-imaging analysis Â and high-resolution weather forecasting. The volume and complexity of data that is now being generated, too vast for humans to process and apply efficiently, has increased the potential of machine learning, as well as the need for it. Deep learning is a more advanced version of machine learning that is particularly adept at processing a wider range of data resources (text as well as unstructured data including images), requires even less human intervention, and can often produce more accurate results than traditional machine learning. Deep learning uses neural networksâ\\x80\\x94based on the ways neurons interact in the human brain â\\x80\\x94to ingest data and process it through multiple neuron layers that recognize increasingly complex features of the data. For example, an early layer might recognize something as being in a specific shape; building on this knowledge, a later layer might be able to identify the shape as a stop sign. Similar to machine learning, deep learning uses iteration to self-correct and improve its prediction capabilities. For example, once it â\\x80\\x9clearnsâ\\x80\\x9d what a stop sign looks like, it can recognize a stop sign in a new image. Case study: Vistra and the Martin Lake Power Plant Vistra is a large power producer in the United States, operating plants in 12 states with a capacity to power nearly 20 million homes. In support of this goal, as well as to improve overall efficiency, QuantumBlack, AI by McKinsey worked with Vistra to build and deploy an AI-powered heat rate optimizer (HRO) at one of its plants. â\\x80\\x9cHeat rateâ\\x80\\x9d is a measure of the thermal efficiency of the plant; in other words, itâ\\x80\\x99s the amount of fuel required to produce each unit of electricity. To reach the optimal heat rate, plant operators continuously monitor and tune hundreds of variables, such as steam temperatures, pressures, oxygen levels, and fan speeds. Vistra and a McKinsey team, including data scientists and machine learning engineers, built a multilayered neural network model. The model combed through two yearsâ\\x80\\x99 worth of data at the plant and learned which combination of factors would attain the most efficient heat rate at any point in time. When the models were accurate to 99 percent or higher and run through a rigorous set of real-world tests, the team converted them into an AI-powered engine that generates recommendations every 30 minutes for operators to improve the plantâ\\x80\\x99s heat rate efficiency. One seasoned operations manager at the companyâ\\x80\\x99s plant in Odessa, Texas, said, â\\x80\\x9cThere are things that took me 20 years to learn about these power plants. This model learned them in an afternoon.â\\x80\\x9d Overall, the AI-powered HRO helped Vistra achieve the following: approximately 1.6 million metric tons of carbon abated annually 67 power generators optimized $60 million saved in about a year Read more about the Vistra story here . Generative AI (gen AI) is an AI model that generates content in response to a prompt. Itâ\\x80\\x99s clear that generative AI tools like ChatGPT and DALL-E (a tool for AI-generated art) have the potential to change how a range of jobs Â are performed. Much is still unknown about gen AIâ\\x80\\x99s potential, but there are some questions we can answerâ\\x80\\x94like how gen AI models are built, what kinds of problems they are best suited to solve, and how they fit into the broader category of AI and machine learning. For more on generative AI and how it stands to affect business and society, check out our Explainer â\\x80\\x9c What is generative AI? The term â\\x80\\x9cartificial intelligenceâ\\x80\\x9d was coined in 1956 Â by computer scientist John McCarthy for a workshop at Dartmouth. But he wasnâ\\x80\\x99t the first to write about the concepts we now describe as AI. Alan Turing introduced the concept of the â\\x80\\x9c imitation game â\\x80\\x9d in a 1950 paper. Thatâ\\x80\\x99s the test of a machineâ\\x80\\x99s ability to exhibit intelligent behavior, now known as the â\\x80\\x9cTuring test.â\\x80\\x9d He believed researchers should focus on areas that donâ\\x80\\x99t require too much sensing and action, things like games and language translation. Research communities dedicated to concepts like computer vision, natural language understanding, and neural networks are, in many cases, several decades old. MIT physicist Rodney Brooks shared details on the four previous stages of AI: Symbolic AI . Symbolic AI is also known as classical AI, or even GOFAI (good old-fashioned AI). The key concept here is the use of symbols and logical reasoning to solve problems. For example, we know a German shepherd is a dog , which is a mammal; all mammals are warm-blooded; therefore, a German shepherd should be warm-blooded. The main problem with symbolic AI is that humans still need to manually encode their knowledge of the world into the symbolic AI system, rather than allowing it to observe and encode relationships on its own. As a result, symbolic AI systems struggle with situations involving real-world complexity. They also lack the ability to learn from large amounts of data. Symbolic AI was the dominant paradigm of AI research until the late 1980s. Neural networks are the technology behind the recent explosive growth of gen AI. Loosely modeling the ways neurons interact in the human brain , neural networks ingest data and process it through multiple iterations that learn increasingly complex features of the data. The neural network can then make determinations about the data, learn whether a determination is correct, and use what it has learned to make determinations about new data. For example, once it â\\x80\\x9clearnsâ\\x80\\x9d what an object looks like, it can recognize the object in a new image. Neural networks were first proposed in 1943 in an academic paper by neurophysiologist Warren McCulloch and logician Walter Pitts. Decades later, in 1969, two MIT researchers mathematically demonstrated that neural networks could perform only very basic tasks. In 1986, there was another reversal, when computer scientist and cognitive psychologist Geoffrey Hinton and colleagues solved the neural network problem presented by the MIT researchers. In the 1990s, computer scientist Yann LeCun made major advancements in neural networksâ\\x80\\x99 use in computer vision, while JÃ¼rgen Schmidhuber advanced the application of recurrent neural networks as used in language processing. In 2012, Hinton and two of his students highlighted the power of deep learning. They applied Hintonâ\\x80\\x99s algorithm to neural networks with many more layers than was typical, sparking a new focus on deep neural networks. These have been the main AI approaches of recent years. During the first few decades of AI, researchers built robots to advance research. Some robots were mobile, moving around on wheels, while others were fixed, with articulated arms. Robots used the earliest attempts at computer vision to identify and navigate through their environments or to understand the geometry of objects and maneuver them. This could include moving around blocks of various shapes and colors. Most of these robots, just like the ones that have been used in factories for decades, rely on highly controlled environments with thoroughly scripted behaviors that they perform repeatedly. They have not contributed significantly to the advancement of AI itself. But traditional robotics did have significant impact in one area, through a process called â\\x80\\x9csimultaneous localization and mappingâ\\x80\\x9d (SLAM). SLAM algorithms helped contribute to self-driving cars and are used in consumer products like vacuum cleaning robots and quadcopter drones. Today, this work has evolved into behavior-based robotics, also referred to as haptic technology because it responds to human touch. In the real world, there arenâ\\x80\\x99t always clear instructions for navigation, decision making, or problem-solving. Behavior-based robotics researchers took inspiration from this, looking for ways robots could solve problems with partial knowledge and conflicting instructions. The term â\\x80\\x9cartificial general intelligenceâ\\x80\\x9d (AGI) was coined to describe AI systems that possess capabilities comparable to those of a human . In theory, AGI could someday replicate human-like cognitive abilities including reasoning, problem-solving, perception, learning, and language comprehension. But letâ\\x80\\x99s not get ahead of ourselves: the key word here is â\\x80\\x9csomeday.â\\x80\\x9d Most researchers and academics believe we are decades away from realizing AGI; some even predict we wonâ\\x80\\x99t see AGI this century, or ever. Rodney Brooks, an MIT roboticist and cofounder of iRobot, doesnâ\\x80\\x99t believe AGI will arrive until the year 2300 . But when it does emergeâ\\x80\\x94and it likely willâ\\x80\\x94itâ\\x80\\x99s going to be a very big deal, in every aspect of our lives. Executives should begin working to understand the path to machines achieving human-level intelligence now and making the transition to a more automated world. For more on AGI, including the four previous attempts at AGI, read our Explainer . Narrow AI is the application of AI techniques to a specific and well-defined problem, such as chatbots like ChatGPT, algorithms that spot fraud in credit card transactions, and natural-language-processing engines that quickly process thousands of legal documents. Most current AI applications fall into the category of narrow AI. AGI is, by contrast, AI thatâ\\x80\\x99s intelligent enough to perform a broad range of tasks. AI is a big story for all kinds of businesses, but some companies are clearly moving ahead of the pack . Our state of AI in 2022 survey showed that adoption of AI models has more than doubled since 2017â\\x80\\x94and investment has increased apace. Whatâ\\x80\\x99s more, the specific areas in which companies see value from AI have evolved, from manufacturing and risk to the following: marketing and sales product and service development strategy and corporate finance One group of companies is pulling ahead of its competitors. Leaders of these organizations consistently make larger investments in AI, level up their practices to scale faster, and hire and upskill the best AI talent. More specifically, they link AI strategy to business outcomes and â\\x80\\x9c industrialize â\\x80\\x9d AI operations by designing modular data architecture that can quickly accommodate new applications. We have yet to see the longtail effect of gen AI models. This means there are some inherent risks involved in using themâ\\x80\\x94both known and unknown. The outputs gen AI models produce may often sound extremely convincing. Worse, sometimes itâ\\x80\\x99s biased (because itâ\\x80\\x99s built on the gender, racial, and other biases of the internet and society more generally). Since gen AI models burst onto the scene, organizations have become aware of users trying to â\\x80\\x9cjailbreakâ\\x80\\x9d the modelsâ\\x80\\x94that means trying to get them to break their own rules and deliver biased, harmful, misleading, or even illegal content. Gen AI organizations are responding to this threat in two ways: for one thing, theyâ\\x80\\x99re collecting feedback from users on inappropriate content. Theyâ\\x80\\x99re also combing through their databases, identifying prompts that led to inappropriate content, and training the model against these types of generations. Organizations that rely on gen AI models should be aware of the reputational and legal risks involved in unintentionally publishing biased, offensive, or copyrighted content. â\\x80\\x9cWhenever you use a model,â\\x80\\x9d says McKinsey partner Marie El Hoyek, â\\x80\\x9cyou need to be able to counter biases Â and instruct it not to use inappropriate or flawed sources, or things you donâ\\x80\\x99t trust.â\\x80\\x9d How? For one thing, itâ\\x80\\x99s crucial to carefully select the initial data used to train these models to avoid including toxic or biased content. Next, rather than employing an off-the-shelf gen AI model, organizations could consider using smaller, specialized models. Organizations with more resources could also customize a general model based on their own data to fit their needs and minimize biases. Itâ\\x80\\x99s also important to keep a human in the loop (that is, to make sure a real human checks the output of a gen AI model before it is published or used) and avoid using gen AI models for critical decisions, such as those involving significant resources or human welfare. The landscape of risks and opportunities is likely to continue to change rapidly in the coming years. As gen AI becomes increasingly incorporated into business, society, and our personal lives, we can also expect a new regulatory climate to take shape. As organizations experimentâ\\x80\\x94and create valueâ\\x80\\x94with these tools, leaders will do well to keep a finger on the pulse of regulation and risk. The Blueprint for an AI Bill of Rights, prepared by the US government in 2022, provides a framework for how government, technology companies, and citizens can collectively ensure more accountable AI. As AI has become more ubiquitous, concerns have surfaced Â about a potential lack of transparency surrounding the functioning of gen AI systems, the data used to train them, issues of bias and fairness, potential intellectual property infringements, privacy violations, and more. The Blueprint comprises five principles that the White House says should â\\x80\\x9cguide the design, use, and deployment of automated systems to protect [users] in the age of artificial intelligence.â\\x80\\x9d They are as follows: The right to safe and effective systems. Systems should undergo predeployment testing, risk identification and mitigation, and ongoing monitoring to demonstrate that they are adhering to their intended use. Algorithmic discrimination is when automated systems contribute to unjustified different treatment of people based on their race, color, ethnicity, sex, religion, age, and more. Users should also have agency over how their data is used. The right to know that an automated system is being used, and a clear explanation of how and why it contributes to outcomes that affect the user. The right to opt out, and access to a human who can quickly consider and fix problems. At present, more than 60 countries or blocs have national strategies governing the responsible use of AIÂ (Exhibit 2). These include Brazil, China, the European Union, Singapore, South Korea, and the United States. The approaches taken vary from guidelines-based approaches, such as the Blueprint for an AI Bill of Rights in the United States, to comprehensive AI regulations that align with existing data protection and cybersecurity regulations, such as the EUâ\\x80\\x99s AI Act, due in 2024. 2 There are also collaborative efforts between countries to set out standards for AI use. The USâ\\x80\\x93EU Trade and Technology Council is working toward greater alignment between Europe and the United States. The Global Partnership on Artificial Intelligence, formed in 2020, has 29 members including Brazil, Canada, Japan, the United States, and several European countries. Even though AI regulations are still being developed, organizations should act now to avoid legal, reputational, organizational, and financial risks. Create an inventory of models, classifying them in accordance with regulation, and record all usage across the organization that is clear to those inside and outside the organization. Implement a governance structure for AI and gen AI that ensures sufficient oversight, authority, and accountability both within the organization and with third parties and regulators. Proper data management includes awareness of data sources, data classification, data quality and lineage, intellectual property, and privacy management. Organizations should establish principles and guardrails for AI development and use them to ensure all AI models uphold fairness and bias controls. Establish strong cybersecurity and technology to ensure a secure environment where unauthorized access or misuse is prevented. Make users aware when they are interacting with an AI system, and provide clear instructions for use. How can organizations scale up their AI efforts from ad hoc projects to full integration? Slow progress toward widespread adoption is likely due to cultural and organizational barriers. But leaders who effectively break down these barriers will be best placed to capture the opportunities of the AI era. Andâ\\x80\\x94cruciallyâ\\x80\\x94companies that canâ\\x80\\x99t take full advantage of AI are already being sidelined by those that can, in industries like auto manufacturing and financial services. To scale up AI, organizations can make three major shifts : Move from siloed work to interdisciplinary collaboration. Rather, AI has the biggest impact when itâ\\x80\\x99s employed by cross-functional teams with a mix of skills and perspectives, enabling AI to address broad business priorities. AI has the potential to enable faster, better decisions at all levels of an organization. But for this to work, people at all levels need to trust the algorithmsâ\\x80\\x99 suggestions and feel empowered to make decisions. (Equally, people should be able to override the algorithm or make suggestions for improvement when necessary.) The agile test-and-learn mindset will help reframe mistakes as sources of discovery, allaying the fear of failure and speeding up development. Learn more about QuantumBlack, AI by McKinsey , and check out AI-related job opportunities if youâ\\x80\\x99re interested in working at McKinsey. Pop quiz Articles referenced: â\\x80\\x9c As gen AI advances, regulatorsâ\\x80\\x94and risk functionsâ\\x80\\x94rush to keep pace ,â\\x80\\x9d December 21, 2023, Andreas Kremer, Angela Luget , Daniel Mikkelsen , Henning Soller , Malin Strandell-Jansson, and Sheila Zingg â\\x80\\x9c What is generative AI? ,â\\x80\\x9d January 19, 2023 â\\x80\\x9c Tech highlights from 2022â\\x80\\x94in eight charts ,â\\x80\\x9d December 22, 2022 â\\x80\\x9c Generative AI is here: How tools like ChatGPT could change your business ,â\\x80\\x9d December 20, 2022, Michael Chui , Roger Roberts , and Lareina Yee Â â\\x80\\x9c The state of AI in 2022â\\x80\\x94and a half decade in review ,â\\x80\\x9d December 6, 2022, Michael Chui , Bryce Hall , Helen Mayhew , Alex Singla , and Alex Sukharevsky Â â\\x80\\x9c Why businesses need explainable AIâ\\x80\\x94and how to deliver it ,â\\x80\\x9d September 29, 2022, Liz Grennan , Andreas Kremer, Alex Singla , and Peter Zipparo â\\x80\\x9c Why digital trust truly matters ,â\\x80\\x9d September 12, 2022, Jim Boehm , Liz Grennan , Alex Singla , and Kate Smaje â\\x80\\x9c McKinsey Technology Trends Outlook 2023 ,â\\x80\\x9d July 20, 2023, Michael Chui , Mena Issler, Roger Roberts , and Lareina Yee Â â\\x80\\x9c An AI power play: Fueling the next wave of innovation in the energy sector ,â\\x80\\x9d May 12, 2022, Barry Boswell, Sean Buckley, Ben Elliott, Matias Melero , and Micah Smith Â â\\x80\\x9c Scaling AI like a tech native: The CEOâ\\x80\\x99s role ,â\\x80\\x9d October 13, 2021, Jacomo Corbo, David Harvey, Nicolas Hohn, Kia Javanmardian , and Nayur Khan â\\x80\\x9c What the draft European Union AI regulations mean for business ,â\\x80\\x9d August 10, 2021, Misha Benjamin, Kevin Buehler , Rachel Dooley, and Peter Zipparo â\\x80\\x9c Winning with AI is a state of mind ,â\\x80\\x9d April 30, 2021, Thomas Meakin , Jeremy Palmer, Valentina Sartori , and Jamie Vickers â\\x80\\x9c Breaking through data-architecture gridlock to scale AI ,â\\x80\\x9d January 26, 2021, Sven Blumberg , Jorge Machado , Henning Soller , and Asin Tavakoli Â â\\x80\\x9c An executiveâ\\x80\\x99s guide to AI ,â\\x80\\x9d November 17, 2020, Michael Chui , Brian McCarthy, and Vishnu Kamalnath â\\x80\\x9c Executiveâ\\x80\\x99s guide to developing AI at scale ,â\\x80\\x9d October 28, 2020, Nayur Khan , Brian McCarthy, and Adi Pradhan â\\x80\\x9c An executive primer on artificial general intelligence ,â\\x80\\x9d April 29, 2020, Federico Berruti , Pieter Nel, and Rob Whiteman â\\x80\\x9c The analytics academy: Bridging the gap between human and artificial intelligence ,â\\x80\\x9d McKinsey Quarterly , September 25, 2019, Solly Brown, Darshit Gandhi, Louise Herring , and Ankur Puri Â This article was updated in April 2024; it was originally published in April 2023. Related Articles Article Ten unsung digital and AI ideas shaping business Podcast Driving innovation with generative AI Article As gen AI advances, regulators—and risk functions—rush to keep pace What Is Artificial Intelligence (AI)? Artificial Intelligence Definition Artificial intelligence (AI) is a wide-ranging branch of computer science that aims to build machines capable of performing tasks that typically require human intelligence. While AI is an interdisciplinary science with multiple approaches, advancements in machine learning and deep learning, in particular, are creating a paradigm shift in virtually every industry. Artificial intelligence allows machines to match, or even improve upon, the capabilities of the human mind. From the development of self-driving cars to the proliferation of generative AI tools, AI is increasingly becoming part of everyday life. What AI Is, Why It Matters, How It Works AI Benefits & Disadvantages, Applications & Examples AI Today & Tomorrow History of AI What AI Is, Why It Matters, How It Works Image: Shutterstock What Is Artificial Intelligence? Artificial intelligence refers to computer systems that are capable of performing tasks traditionally associated with human intelligence — such as making predictions, identifying objects, interpreting speech and generating natural language. AI systems learn how to do so by processing massive amounts of data and looking for patterns to model in their own decision-making. In many cases, humans will supervise an AI’s learning process, reinforcing good decisions and discouraging bad ones, but some AI systems are designed to learn without supervision. Over time, AI systems improve on their performance of specific tasks, allowing them to adapt to new inputs and make decisions without being explicitly programmed to do so. In essence, artificial intelligence is about teaching machines to think and learn like humans, with the goal of automating work and solving problems more efficiently. Artificial intelligence aims to provide machines with similar processing and analysis capabilities as humans, making AI a useful counterpart to people in everyday life. AI is able to interpret and sort data at scale, solve complicated problems and automate various tasks simultaneously, which can save time and fill in operational gaps missed by humans. AI serves as the foundation for computer learning and is used in almost every industry — from healthcare and finance to manufacturing and education — helping to make data-driven decisions and carry out repetitive or computationally intensive tasks. We see it in smartphones with AI assistants, e-commerce platforms with recommendation systems and vehicles with autonomous driving abilities. AI also helps protect people by piloting fraud detection systems online and robots for dangerous jobs, as well as leading research in healthcare and climate initiatives. First, a massive amount of data is collected and applied to mathematical models, or algorithms, which use the information to recognize patterns and make predictions in a process known as training. Once algorithms have been trained, they are deployed within various applications, where they continuously learn from and adapt to new data. This allows AI systems to perform complex tasks like image recognition, language processing and data analysis with greater accuracy and efficiency over time. Machine Learning The primary approach to building AI systems is through machine learning (ML), where computers learn from large datasets by identifying patterns and relationships within the data. A machine learning algorithm uses statistical techniques to help it “learn” how to get progressively better at a task, without necessarily having been programmed for that certain task. It uses historical data as input to predict new output values. Machine learning consists of both supervised learning (where the expected output for the input is known thanks to labeled data sets) and unsupervised learning (where the expected outputs are unknown due to the use of unlabeled data sets). Neural Networks Machine learning is typically done using neural networks , a series of algorithms that process data by mimicking the structure of the human brain. These networks consist of layers of interconnected nodes, or “neurons,” that process information and pass it between each other. By adjusting the strength of connections between these neurons, the network can learn to recognize complex patterns within data, make predictions based on new inputs and even learn from mistakes. This makes neural networks useful for recognizing images, understanding human speech and translating words between languages. Deep Learning Deep learning is an important subset of machine learning. It uses a type of artificial neural network known as deep neural networks, which contain a number of hidden layers through which data is processed, allowing a machine to go “deep” in its learning and recognize increasingly complex patterns, making connections and weighting input for the best results. Deep learning is particularly effective at tasks like image and speech recognition and natural language processing, making it a crucial component in the development and advancement of AI systems. Natural Language Processing Natural language processing (NLP) involves teaching computers to understand and produce written and spoken language in a similar manner as humans. NLP combines computer science, linguistics, machine learning and deep learning concepts to help computers analyze unstructured text or voice data and extract relevant information from it. NLP mainly tackles speech recognition and natural language generation , and it’s leveraged for use cases like spam detection and virtual assistants . Computer Vision Computer vision is another prevalent application of machine learning techniques, where machines process raw images, videos and visual media, and extract useful insights from them. Deep learning and convolutional neural networks are used to break down images into pixels and tag them accordingly, which helps computers discern the difference between visual shapes and patterns. Computer vision is used for image recognition , image classification and object detection, and completes tasks like facial recognition and detection in self-driving cars and robots. Types of Artificial Intelligence Artificial intelligence can be classified in several different ways. Strong AI vs. Weak AI AI can be organized into two broad categories: weak AI and strong AI . Weak AI (or narrow AI) refers to AI that automates specific tasks. It typically outperforms humans, but it operates within a limited context and is applied to a narrowly defined problem. For now, all AI systems are examples of weak AI, ranging from email inbox spam filters to recommendation engines to chatbots . Strong AI , often referred to as artificial general intelligence (AGI) , is a hypothetical benchmark at which AI could possess human-like intelligence and adaptability, solving problems it’s never been trained to work on. AGI does not actually exist yet, and it is unclear whether it ever will. The 4 Kinds of AI AI can then be further categorized into four main types : reactive machines, limited memory, theory of mind and self-awareness. Reactive machines perceive the world in front of them and react. They can carry out specific commands and requests, but they cannot store memory or rely on past experiences to inform their decision making in real time. This makes reactive machines useful for completing a limited number of specialized duties. Examples include Netflix’s recommendation engine and IBM’s Deep Blue (used to play chess). Limited memory AI has the ability to store previous data and predictions when gathering information and making decisions. Essentially, it looks into the past for clues to predict what may come next. Limited memory AI is created when a team continuously trains a model in how to analyze and utilize new data, or an AI environment is built so models can be automatically trained and renewed. Theory of mind is a type of AI that does not actually exist yet, but it describes the idea of an AI system that can perceive and understand human emotions , and then use that information to predict future actions and make decisions on its own. Self-aware AI refers to artificial intelligence that has self-awareness , or a sense of self. In theory, though, self-aware AI possesses human-like consciousness and understands its own existence in the world, as well as the emotional state of others. AI Benefits & Disadvantages, Applications & Examples Image: Shutterstock Benefits of AI AI is beneficial for automating repetitive tasks, solving complex problems, reducing human error and much more. Automating Repetitive Tasks Repetitive tasks such as data entry and factory work , as well as customer service conversations, can all be automated using AI technology. Solving Complex Problems AI’s ability to process large amounts of data at once allows it to quickly find patterns and solve complex problems that may be too difficult for humans, such as predicting financial outlooks or optimizing energy solutions. Improving Customer Experience AI can be applied through user personalization, chatbots and automated self-service technologies, making the customer experience more seamless and increasing customer retention for businesses. Advancing Healthcare and Medicine AI works to advance healthcare by accelerating medical diagnoses, drug discovery and development and medical robot implementation throughout hospitals and care centers. Reducing Human Error The ability to quickly identify relationships in data makes AI effective for catching mistakes or anomalies among mounds of digital information, overall reducing human error and ensuring accuracy. Disadvantages of AI While artificial intelligence has its benefits, the technology also comes with risks and potential dangers to consider. Job Displacement AI’s abilities to automate processes, generate rapid content and work for long periods of time can mean job displacement for human workers. Bias and Discrimination AI models may be trained on data that reflects biased human decisions, leading to outputs that are biased or discriminatory against certain demographics. Hallucinations AI systems may inadvertently “ hallucinate ” or produce inaccurate outputs when trained on insufficient or biased data, leading to the generation of false information. Privacy Concerns The data collected and stored by AI systems may be done so without user consent or knowledge, and may even be accessed by unauthorized individuals in the case of a data breach. Ethical Concerns AI systems may be developed in a manner that isn’t transparent, inclusive or sustainable , resulting in a lack of explanation for potentially harmful AI decisions as well as a negative impact on users and businesses. Environmental Costs Large-scale AI systems can require a substantial amount of energy to operate and process data, which increases carbon emissions and water consumption. Artificial Intelligence Applications Artificial intelligence has applications across multiple industries, ultimately helping to streamline processes and boost business efficiency. Healthcare AI is used in healthcare to improve the accuracy of medical diagnoses, facilitate drug research and development, manage sensitive healthcare data and automate online patient experiences. It is also a driving factor behind medical robots, which work to provide assisted therapy or guide surgeons during surgical procedures. Retail AI in retail amplifies the customer experience by powering user personalization, product recommendations, shopping assistants and facial recognition for payments. For retailers and suppliers, AI helps automate retail marketing, identify counterfeit products on marketplaces, manage product inventories and pull online data to identify product trends. Customer Service In the customer service industry , AI enables faster and more personalized support. AI-powered chatbots and virtual assistants can handle routine customer inquiries, provide product recommendations and troubleshoot common issues in real-time. And through NLP, AI systems can understand and respond to customer inquiries in a more human-like way, improving overall satisfaction and reducing response times. Manufacturing AI in manufacturing can reduce assembly errors and production times while increasing worker safety. Factory floors may be monitored by AI systems to help identify incidents, track quality control and predict potential equipment failure. AI also drives factory and warehouse robots, which can automate manufacturing workflows and handle dangerous tasks. Finance The finance industry utilizes AI to detect fraud in banking activities, assess financial credit standings, predict financial risk for businesses plus manage stock and bond trading based on market patterns. AI is also implemented across fintech and banking apps, working to personalize banking and provide 24/7 customer service support. Marketing In the marketing industry , AI plays a crucial role in enhancing customer engagement and driving more targeted advertising campaigns. Advanced data analytics allows marketers to gain deeper insights into customer behavior, preferences and trends, while AI content generators help them create more personalized content and recommendations at scale. AI can also be used to automate repetitive tasks such as email marketing and social media management. Gaming Video game developers apply AI to make gaming experiences more immersive . Non-playable characters (NPCs) in video games use AI to respond accordingly to player interactions and the surrounding environment, creating game scenarios that can be more realistic, enjoyable and unique to each player. Military AI assists militaries on and off the battlefield, whether it\\'s to help process military intelligence data faster, detect cyberwarfare attacks or automate military weaponry, defense systems and vehicles. Drones and robots in particular may be imbued with AI , making them applicable for autonomous combat or search and rescue operations. Artificial Intelligence Examples Specific examples of AI include: Generative AI Tools Generative AI tools, sometimes referred to as AI chatbots — including ChatGPT , Gemini , Claude and Grok — use artificial intelligence to produce written content in a range of formats, from essays to code and answers to simple questions. Smart Assistants Personal AI assistants , like Alexa and Siri, use natural language processing to receive instructions from users to perform a variety of “ smart tasks .” They can carry out commands like setting reminders, searching for online information or turning off your kitchen lights. Self-Driving Cars Self-driving cars are a recognizable example of deep learning, since they use deep neural networks to detect objects around them, determine their distance from other cars, identify traffic signals and much more. Wearables Many wearable sensors and devices used in the healthcare industry apply deep learning to assess the health condition of patients, including their blood sugar levels, blood pressure and heart rate. They can also derive patterns from a patient’s prior medical data and use that to anticipate any future health conditions. Visual Filters Filters used on social media platforms like TikTok and Snapchat rely on algorithms to distinguish between an image’s subject and the background, track facial movements and adjust the image on the screen based on what the user is doing. AI Today & Tomorrow Image: Shutterstock The Rise of Generative AI Generative AI describes artificial intelligence systems that can create new content — such as text, images, video or audio — based on a given user prompt. To work, a generative AI model is fed massive data sets and trained to identify patterns within them, then subsequently generates outputs that resemble this training data. Generative AI has gained massive popularity in the past few years, especially with chatbots and image generators arriving on the scene. These kinds of tools are often used to create written copy, code, digital art and object designs, and they are leveraged in industries like entertainment, marketing, consumer goods and manufacturing. For instance, it can be used to create fake content and deepfakes , which could spread disinformation and erode social trust. And some AI-generated material could potentially infringe on people’s copyright and intellectual property rights. AI Regulation As AI grows more complex and powerful, lawmakers around the world are seeking to regulate its use and development. The first major step to regulate AI occurred in 2024 in the European Union with the passing of its sweeping Artificial Intelligence Act , which aims to ensure that AI systems deployed there are “safe, transparent, traceable, non-discriminatory and environmentally friendly.” Countries like China and Brazil have also taken steps to govern artificial intelligence. Meanwhile, AI regulation in the United States is still a work in progress. The Biden-Harris administration introduced a non-enforceable AI Bill of Rights in 2022, and then The Executive Order on Safe, Secure and Trustworthy AI in 2023, which aims to regulate the AI industry while maintaining the country’s status as a leader in the industry. Congress has made several attempts to establish more robust legislation, but it has largely failed, leaving no laws in place that specifically limit the use of AI or regulate its risks. For now, all AI legislation in the United States exists only on the state level. Future of Artificial Intelligence The future of artificial intelligence holds immense promise, with the potential to revolutionize industries, enhance human capabilities and solve complex challenges. It can be used to develop new drugs, optimize global supply chains and create exciting new art — transforming the way we live and work. Looking ahead, one of the next big steps for artificial intelligence is to progress beyond weak or narrow AI and achieve artificial general intelligence (AGI). With AGI, machines will be able to think, learn and act the same way as humans do, blurring the line between organic and machine intelligence. This could pave the way for increased automation and problem-solving capabilities in medicine, transportation and more — as well as sentient AI down the line. On the other hand, the increasing sophistication of AI also raises concerns about heightened job loss, widespread disinformation and loss of privacy. And questions persist about the potential for AI to outpace human understanding and intelligence — a phenomenon known as technological singularity that could lead to unforeseeable risks and possible moral dilemmas. For now, society is largely looking toward federal and business-level AI regulations to help guide the technology’s future. History of AI Image: Shutterstock History of AI Artificial intelligence as a concept began to take off in the 1950s when computer scientist Alan Turing released the paper “ Computing Machinery and Intelligence ,” which questioned if machines could think and how one would test a machine’s intelligence. This paper set the stage for AI research and development, and was the first proposal of the Turing test , a method used to assess machine intelligence. The term “artificial intelligence” was coined in 1956 by computer scientist John McCartchy in an academic conference at Dartmouth College. Following McCarthy’s conference and throughout the 1970s, interest in AI research grew from academic institutions and U.S. government funding. Innovations in computing allowed several AI foundations to be established during this time, including machine learning, neural networks and natural language processing. Despite its advances, AI technologies eventually became more difficult to scale than expected and declined in interest and funding, resulting in the first AI winter until the 1980s. In the mid-1980s, AI interest reawakened as computers became more powerful, deep learning became popularized and AI-powered “expert systems” were introduced. However, due to the complication of new systems and an inability of existing technologies to keep up, the second AI winter occurred and lasted until the mid-1990s. By the mid-2000s, innovations in processing power, big data and advanced deep learning techniques resolved AI’s previous roadblocks, allowing further AI breakthroughs. Modern AI technologies like virtual assistants, driverless cars and generative AI began entering the mainstream in the 2010s, making AI what it is today. Artificial Intelligence Timeline Warren McCullough and Walter Pitts publish the paper “ A Logical Calculus of Ideas Immanent in Nervous Activity ,” which proposes the first mathematical model for building a neural network. In his book The Organization of Behavior: A Neuropsychological Theory , Donald Hebb proposes the theory that neural pathways are created from experiences and that connections between neurons become stronger the more frequently they’re used. Alan Turing publishes the paper “Computing Machinery and Intelligence,” proposing what is now known as the Turing Test, a method for determining if a machine is intelligent. The phrase “artificial intelligence” is coined at the Dartmouth Summer Research Project on Artificial Intelligence. Led by John McCarthy, the conference is widely considered to be the birthplace of AI. John McCarthy develops the AI programming language Lisp and publishes “ Programs with Common Sense ,” a paper proposing the hypothetical Advice Taker, a complete AI system with the ability to learn from experience as effectively as humans. Arthur Samuel coins the term “machine learning” while at IBM. Daniel Bobrow develops STUDENT, an early natural language processing program designed to solve algebra word problems, as a doctoral candidate at MIT. MIT professor Joseph Weizenbaum creates Eliza, one of the first chatbots to successfully mimic the conversational patterns of users, creating the illusion that it understood more than it did. This introduced the Eliza effect , a common phenomenon where people falsely attribute humanlike thought processes and emotions to AI systems. The first successful expert systems, DENDRAL and MYCIN, are created at the AI Lab at Stanford University. The Lighthill Report, detailing the disappointments in AI research, is released by the British government and leads to severe cuts in funding for AI projects. (1974-1980) Frustration with the progress of AI development leads to major DARPA cutbacks in academic grants. Combined with the earlier ALPAC report and the previous year’s Lighthill Report, AI funding dries up and research stalls. This period is known as the “ First AI Winter .” Digital Equipment Corporations develops R1 (also known as XCON), the first successful commercial expert system. Designed to configure orders for new computer systems, R1 kicks off an investment boom in expert systems that will last for much of the decade, effectively ending the first AI winter. Companies are spending more than a billion dollars a year on expert systems and an entire industry known as the Lisp machine market springs up to support them. Companies like Symbolics and Lisp Machines Inc. build specialized computers to run on the AI programming language Lisp. (1987-1993) As computing technology improved, cheaper alternatives emerged and the Lisp machine market collapsed in 1987, ushering in the “ Second AI Winter .” During this period, expert systems proved too expensive to maintain and update, eventually falling out of favor. IBM’s Deep Blue beats world chess champion Gary Kasparov. Fei-Fei Li starts working on the ImageNet visual database, introduced in 2009. This became the catalyst for the AI boom, and the basis on which image recognition grew. Google makes breakthroughs in speech recognition and introduces the feature in its iPhone app. Apple releases Siri, an AI-powered virtual assistant through its iOS operating system. Andrew Ng, founder of the Google Brain Deep Learning project, feeds a neural network using deep learning algorithms 10 million YouTube videos as a training set. The neural network learned to recognize a cat without being told what a cat is, ushering in the breakthrough era for neural networks and deep learning funding. Amazon’s Alexa, a virtual home smart device , is released. Google DeepMind’s AlphaGo defeats world champion Go player Lee Sedol. The complexity of the ancient Chinese game was seen as a major hurdle to clear in AI. Google releases natural language processing engine BERT , reducing barriers in translation and understanding by ML applications. Baidu releases its LinearFold AI algorithm to scientific and medical teams working to develop a vaccine during the early stages of the SARS-CoV-2 pandemic. The algorithm is able to predict the RNA sequence of the virus in just 27 seconds, 120 times faster than other methods. OpenAI releases natural language processing model GPT-3 , which is able to produce text modeled after the way people speak and write. OpenAI builds on GPT-3 to develop DALL-E , which is able to create images from text prompts. The National Institute of Standards and Technology releases the first draft of its AI Risk Management Framework , voluntary U.S. guidance “to better manage risks to individuals, organizations, and society associated with artificial intelligence.” OpenAI launches ChatGPT, a chatbot powered by a large language model that gains more than 100 million users in just a few months. The White House introduces an AI Bill of Rights outlining principles for the responsible development and use of AI. Microsoft launches an AI-powered version of Bing, its search engine, built on the same technology that powers ChatGPT. The Biden-Harris administration issues The Executive Order on Safe, Secure and Trustworthy AI , calling for safety testing, labeling of AI-generated content and increased efforts to create international standards for the development and use of AI. The order also stresses the importance of ensuring that artificial intelligence is not used to circumvent privacy protections, exacerbate discrimination or violate civil rights or the rights of consumers. The chatbot Grok is released by Elon Musk’s AI company xAI. The European Union passes the Artificial Intelligence Act, which aims to ensure that AI systems deployed within the EU are “safe, transparent, traceable, non-discriminatory and environmentally friendly. Claude 3 Opus, a large language model developed by AI company Anthropic, outperforms GPT-4 — the first LLM to do so. 92 Artificial Intelligence (AI) Companies to Know They may not be household names, but these 42 artificial intelligence companies are working on some very smart technology. Read Article The Future of AI: How Artificial Intelligence Will Change the World More Stories Back to Top AI Detection: What It Is, How It Works, Top Tools to Know Read Article Unsupervised Clustering: A Guide GPT-4o Mini Is Cheaper. How AI Can Help More People Have Babies Will the Billions in AI Investment Ever Pay Off? How AI Can Help Detect Lung Cancer Sooner Read Article An LLM Is Like a Hungry Bear. This Industry Is Most at Risk of Being Replaced by AI How to Hire to Win in the Age of AI Using AI to Create Content? Read Article How LLMs Are Transforming Rich Text Editors Continue Reading How AI Can Support Social Media Marketing How to Prepare Your Website for AI Overviews What Is AI Art? Llama 3.1: What\\'s New With Meta’s Large Language Model Why Your Customers Might Stage a Chatbot Revolt How to Prepare Your Engineers for the Wave of Incoming AI-Powered Cyberattacks Go Ahead. Singular Value Decomposition (SVD) Algorithm Explained How AI Can Help With Security at Olympic-Sized Events Anthropic: What We Know About the Company Behind Claude AI Mean Average Precision (mAP) Explained How Generative AI Is Transforming Text Data Extraction Want Better AI Outputs? Multimodal AI: What It Is and How It Works 9 AI Search Engines to Know Healthcare Chatbots: When Do They Help and When Do They Hurt? How Automated Healthcare Marketing Can Encourage Healthy Behavior Why AI Will Never Replace Software Developers 1 Reason Why You Shouldn’t Invest in Responsible AI — and 1 Reason Why You Should What Is an AI Model? How to Install cuDNN and CUDA for Windows and Linux Deep Convolutional Neural Networks (DCNN) Explained Without This Component, Your AI Solution Is Useless Why an Open-Source Future Can Make AI Work for Creatives Can We Figure Out What Apple Is Doing With AI? 20 AI Coding Tools and Assistants to Know The Future of AI Is Nuclear Just How Efficient Is AI? AI in Recruiting: What to Know Predictive AI Streamlines Operations In This Surprisingly Simple Way How AI Can Improve the Software Testing Process How Smart Chips Are Enhancing Engineers’ Productivity What Is Black Box AI? Data Engineers, Here’s How LLMs Can Make Your Lives Easier How to Get Better Results From an LLM GPT-4o: Here’s What You Need to Know 3 Steps to Building a Culture of Learning and Innovation Around AI Here’s How AI Feels About Refugees Why We Need AI Governance Now Here’s What You Need to Know About Llama 3 Mean Normalization Explained What Is Retrieval Augmented Generation (RAG)? Understanding the Derivative of the Sigmoid Function Which of the 5 AI Archetypes Are You? Understanding the Hidden Markov Model 22 AI Certifications to Know How Robo-Advisors Can Help You Manage Your Wealth What Is Nvidia’s Chat With RTX? AI Therapy: How Chatbots Are Transforming Mental Healthcare Why Full-Stack Developers Are the Future of Software Engineering Here’s How AI Is Building a Robot-Filled World How to Build Trust and Transparency in Your Generative AI Implementation Mistral AI: What to Know About Europe’s OpenAI Rival How Will Fiber Optic Networks Keep up With AI? How AI Can Supercharge Your Project Management Skills How to Fine-Tune LLMs AI Influencers, Explained Deep Tech Trends and Opportunities in 2024 How Deepfakes Threaten the Integrity of the 2024 Election 5 Ways to Amplify Product Management With GenAI When Should You Stop Calling Your Company a Startup? How AI Makes My Interns as Capable as Seasoned Staff 5 Qualities Leaders Need in the Age of AI What Is Robotic Process Automation (RPA)? Navigating the Hype, Hope and Doom of OpenAI’s Sora How to Build Your Own RAG System With LlamaIndex and MongoDB What Should You Look for When Hiring a Prompt Engineer? What Enterprises Need to Know Before Adopting a LLM If You’re Automating Business Processes, Don’t Overlook This Step 23 Companies Hiring AI Engineers What Is Google Gemini? 5 Ways AI Will Revolutionize Hardware Design in 2024 7 Ways AI Will Enhance UX Research Can AI Help People Overcome Dyslexia? How AI Is Diversifying VC Funding Should You Hire a Chief AI Officer? Understanding Overfitting vs. Underfitting in Machine Learning Here’s How to Build a Culture of Experimentation How AI Can Streamline the Real Estate Business Should You Build or Buy AI? Will This Election Year Be a Turning Point for AI Regulation? 3 Tech Predictions You Can Actually Measure Best Use-Cases for Generative AI in 2024 Could AI Bring Back the Metaverse? Facial Recognition, Explained Gaussian Mixture Model Explained How to Design a Product Ahead of Its Time Can AI Solve These Riddles? Why Artificial Intelligence Will Never Replace Your Back Office AI Is Accelerating the Post-Trust Era. What the Next President’s Economic Policies Will Mean for Tech Grok: What We Know About Elon Musk’s AI Chatbot 6 Reasons to Learn Julia in 2024\\u200d 5 Lessons I Learned Building a Generative AI Platform Here Are 5 Ways to Get the Most Out of MLOps Fast Fourier Transform Explained A Guide to TF-IDF How to Build a Logistic Regression Model for Classification 5 Steps to Remove Bias From Machine Learning Algorithms 3 Things to Know About AI Job Coaching How to Sell a Machine Learning Project What Is Claude AI and How Does It Compare to ChatGPT? Why AI Must Be Decentralized Why AI Is a Business Analyst’s Best Friend Are Generative AI Tools Worth the Investment? How to Get Ahead of 2024’s Biggest Challenges 9 Steps for a Successful Marketing Automation Strategy What Role Should Generative AI Play in Coding in 2024? How Web Intelligence Can Empower Environmental Activism Why Math Is Vital to Thrive In Your AI Career What Is GPT-4? A Guide to Image Captioning in Deep Learning Top Applications for Computer Vision in Sports AI Chips: What Are They? Stop Freaking Out About Generative AI Feature Engineering Explained How to Turn Your Service Company Into a Software Company 3 Steps to Create a Living Brand How Generative AI Takes Gamification to the Next Level 5 More Ways AI Will Evolve Product Management in 2024 What Is a Large Language Model (LLM)? 3 Ways to Prepare Your Business for an AI-Powered Future 4 Ways Unstructured Data Management Will Change in 2024 How to Make the Most of In-House Software Development How Lawsuits and Regulation Will Impact AI in 2024 4 Ways to Set Yourself up for Online Success in 2024 Why Consumers Don’t Trust AI-Driven Recommendations A Guide to Python Tesseract 5 Ways AI Will Evolve Product Management in 2024 Vision Transformer: An Introduction Can AI Help Identify Employees Who Want to Quit? How Generative AI Can Help Engineers Upskill How Tech Is Shaping the Future of E-Commerce Here’s How 3 Global Giants Improve the Supply Chain With AI When Will Quantum Computing Have Its AlphaGo and ChatGPT Moments? How Generative AI Is Transforming the Retail Industry Here’s What AI Does Best in the Hiring Process How Will AI Change UX Design? What Is Relational Trust in Tech, and Why Do We Need to Build It? 3 Ways AI Can Level Up Your Video Content Here’s How AI Can Enhance Your Mental Health How Generative AI Will Empower Self-Service Data Management How Automation Can Help You Retain Talent Machine Learning Basics Every Beginner Should Know What Is Transhumanism? How Generative AI Will Enable More Inclusive Digital Experiences How We Built a Chatbot That Uses Generative AI After Cruise Ban, What’s Next for Autonomous Vehicles? An Introduction to Support Vector Machine (SVM) in Python 5 Anomaly Detection Algorithms to Know What Is Model Deployment in Machine Learning? Understanding Feature Importance in Machine Learning All Machine Learning Models Explained Deep Tech, Explained 10 iOS App Development Trends Worth Knowing A Deep Dive Into Non-Maximum Suppression (NMS) Gaussian Naive Bayes Explained With Scikit-Learn 6 Ways Your Small Business Can Benefit From Machine Learning Solutions Introduction to Prolog: A Programming Language for AI 3 Reasons AI Should Be Open Source Here’s Why AI Can’t Solve Your Mental Health Issues 9 Ways to Improve Your Google Ad Campaigns What You Need to Know About the Biden-Harris Executive Order on AI Can AI Solve the Growing Cybersecurity Shortage? How IDPs and Golden Paths Can Close the Developer Productivity Gap How to Exorcise the Modern Horrors of IT How to Develop Large Language Model (LLM) Applications Load More Great Companies Need Great People. Recruit With Us Artificial intelligence (AI) | Definition, Examples, Types, Applications, Companies, & Facts | Britannica Ask the Chatbot Games & Quizzes History & Society Science & Tech Biographies Animals & Nature Geography & Travel Arts & Culture Money Videos artificial intelligence Table of Contents Introduction & Top Questions What is intelligence? Learning Reasoning Problem solving Perception Language Methods and goals in AI Symbolic vs. connectionist approaches Artificial general intelligence (AGI), applied AI, and cognitive simulation AI technology Machine learning Large language models and natural language processing Autonomous vehicles Virtual assistants Risks Is artificial general intelligence (AGI) possible? References & Edit History Quick Facts & Related Topics Images & Videos Quizzes Computers and Technology Quiz Related Questions What is artificial intelligence? Read Next Pro and Con: Artificial Intelligence The Future of Information and Education Discover What Is a Modern Pentathlon? The 10 Greatest Basketball Players of All Time 10 of the World’s Most Dangerous Fish What Is Known (and Not Known) About the Tunguska Event What Is Known (and Not Known) About the Bermuda Triangle Inventors and Inventions of the Industrial Revolution Which Religion Is the Oldest? Contents artificial intelligence Actions Cite verified Cite While every effort has been made to follow citation style rules, there may be some discrepancies. Please refer to the appropriate style manual or other sources if you have any questions. Select Citation Style MLA APA Chicago Manual of Style Copy Citation Share Share Share to social media Facebook X URL https://www.britannica.com/technology/artificial-intelligence Give Feedback External Websites Feedback Corrections? Let us know if you have suggestions to improve this article (requires login). Feedback Type Select a type (Required) Factual Correction Spelling/Grammar Correction Link Correction Additional Information Other Your Feedback Submit Feedback Thank you for your feedback Our editors will review what you’ve submitted and determine whether to revise the article. External Websites Harvard University - Science in the News - The History of Artificial Intelligence National Center for Biotechnology Information - PubMed Central - The rise of artificial intelligence in healthcare applications Lifewire - What is artificial intelligence? Computer History Museum - AI and Robotics Internet Encyclopedia of Philosophy - Artificial Intelligence Britannica Websites Articles from Britannica Encyclopedias for elementary and high school students. artificial intelligence - Children\\'s Encyclopedia (Ages 8-11) artificial intelligence (AI) - Student Encyclopedia (Ages 11 and up) Print print Print Please select which sections you would like to print: Table Of Contents Cite verified Cite While every effort has been made to follow citation style rules, there may be some discrepancies. Please refer to the appropriate style manual or other sources if you have any questions. Select Citation Style MLA APA Chicago Manual of Style Copy Citation Share Share Share to social media Facebook X URL https://www.britannica.com/technology/artificial-intelligence Feedback External Websites Feedback Corrections? Let us know if you have suggestions to improve this article (requires login). Feedback Type Select a type (Required) Factual Correction Spelling/Grammar Correction Link Correction Additional Information Other Your Feedback Submit Feedback Thank you for your feedback Our editors will review what you’ve submitted and determine whether to revise the article. External Websites Harvard University - Science in the News - The History of Artificial Intelligence National Center for Biotechnology Information - PubMed Central - The rise of artificial intelligence in healthcare applications Lifewire - What is artificial intelligence? Computer History Museum - AI and Robotics Internet Encyclopedia of Philosophy - Artificial Intelligence Britannica Websites Articles from Britannica Encyclopedias for elementary and high school students. artificial intelligence - Children\\'s Encyclopedia (Ages 8-11) artificial intelligence (AI) - Student Encyclopedia (Ages 11 and up) Also known as: AI Written by B.J. Copeland Professor of Philosophy and Director of the Turing Archive for the History of Computing, University of Canterbury, Christchurch, New Zealand. Copeland Fact-checked by The Editors of Encyclopaedia Britannica Encyclopaedia Britannica\\'s editors oversee subject areas in which they have extensive knowledge, whether from years of experience gained by working on that content or via study for an advanced degree. They write new content and verify and edit content received from contributors. The Editors of Encyclopaedia Britannica Last Updated: Aug 16, 2024 • Article History Table of Contents artificial intelligence See all media Key People: Geoffrey Hinton Marvin Minsky Edward Albert Feigenbaum Allen Newell John McCarthy (Show more) Related Topics: history of artificial intelligence (AI) computational aesthetics prompt engineering three laws of robotics generative AI (Show more) See all related content → Ask the Chatbot a Question Ask the Chatbot a Question Top Questions What is artificial intelligence? Artificial intelligence is the ability of a computer or computer-controlled robot to perform tasks that are commonly associated with the intellectual processes characteristic of humans , such as the ability to reason. Although there are as yet no AIs that match full human flexibility over wider domains or in tasks requiring much everyday knowledge, some AIs perform specific tasks as well as humans. No, artificial intelligence and machine learning are not the same, but they are closely related. Machine learning is the method to train a computer to learn from its inputs but without explicit programming for every circumstance. Recent News Aug. 16, 2024, 9:39 PM ET (AP) San Francisco goes after websites that make AI deepfake nudes of women and girls Aug. 1, 2024, 6:08 PM ET (AP) Apple breaks out of recent sales slump as it gears up to make its leap into the AI craze Aug. 1, 2024, 8:01 AM ET (AP) Hollywood\\'s video game performers head to the picket line over AI protections July 30, 2024, 9:41 AM ET (AP) Google\\'s partnership with AI startup Anthropic faces scrutiny from UK competition watchdog artificial intelligence (AI) , the ability of a digital computer or computer-controlled robot to perform tasks commonly associated with intelligent beings. The term is frequently applied to the project of developing systems endowed with the intellectual processes characteristic of humans, such as the ability to reason, discover meaning, generalize, or learn from past experience. Since their development in the 1940s, digital computers have been programmed to carry out very complex tasks—such as discovering proofs for mathematical theorems or playing chess —with great proficiency. Despite continuing advances in computer processing speed and memory capacity, there are as yet no programs that can match full human flexibility over wider domains or in tasks requiring much everyday knowledge. On the other hand, some programs have attained the performance levels of human experts and professionals in executing certain specific tasks, so that artificial intelligence in this limited sense is found in applications as diverse as medical diagnosis , computer search engines , voice or handwriting recognition, and chatbots . AI & Britannica Money Investing in AI stocks Using AI for Money Management How AI is Changing Work Ethical Questions and AI AI and Regulation All but the simplest human behavior is ascribed to intelligence, while even the most complicated insect behavior is usually not taken as an indication of intelligence. When the female wasp returns to her burrow with food, she first deposits it on the threshold , checks for intruders inside her burrow, and only then, if the coast is clear, carries her food inside. The real nature of the wasp’s instinctual behavior is revealed if the food is moved a few inches away from the entrance to her burrow while she is inside: on emerging, she will repeat the whole procedure as often as the food is displaced. Intelligence—conspicuously absent in the case of the wasp—must include the ability to adapt to new circumstances. (Read Ray Kurzweil’s Britannica essay on the future of “Nonbiological Man.”) Psychologists generally characterize human intelligence not by just one trait but by the combination of many diverse abilities. Research in AI has focused chiefly on the following components of intelligence: learning, reasoning, problem solving , perception , and using language. Learning Britannica Quiz Computers and Technology Quiz There are a number of different forms of learning as applied to artificial intelligence. For example, a simple computer program for solving mate-in-one chess problems might try moves at random until mate is found. The program might then store the solution with the position so that, the next time the computer encountered the same position, it would recall the solution. This simple memorizing of individual items and procedures—known as rote learning—is relatively easy to implement on a computer. More challenging is the problem of implementing what is called generalization . For example, a program that learns the past tense of regular English verbs by rote will not be able to produce the past tense of a word such as jump unless the program was previously presented with jumped , whereas a program that is able to generalize can learn the “add -ed ” rule for regular verbs ending in a consonant and so form the past tense of jump on the basis of experience with similar verbs. (Read Yuval Noah Harari’s Britannica essay on the future of “Nonconscious Man.”) Artificial intelligence - Wikipedia Jump to content From Wikipedia, the free encyclopedia Intelligence of machines \"AI\" redirects here. For other uses, see AI (disambiguation) , Artificial intelligence (disambiguation) , and Intelligent agent . Part of a series on Artificial intelligence Major goals Artificial general intelligence Intelligent agent Recursive self-improvement Planning Computer vision General game playing Knowledge reasoning Natural language processing Robotics AI safety Approaches Machine learning Symbolic Deep learning Bayesian networks Evolutionary algorithms Hybrid intelligent systems Systems integration Applications Projects Deepfake Machine translation Generative AI Art Audio Music Healthcare Mental health Government Industry Earth sciences Bioinformatics Physics Philosophy Artificial consciousness Chinese room Friendly AI Control problem / Takeover Ethics Existential risk Turing test Regulation History Timeline Progress AI winter AI boom Glossary Glossary v t e Artificial intelligence ( AI ), in its broadest sense, is intelligence exhibited by machines , particularly computer systems . It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Some high-profile applications of AI include advanced web search engines (e.g., Google Search ); recommendation systems (used by YouTube , Amazon , and Netflix ); interacting via human speech (e.g., Google Assistant , Siri , and Alexa ); autonomous vehicles (e.g., Waymo ); generative and creative tools (e.g., ChatGPT , Apple Intelligence , and AI art ); and superhuman play and analysis in strategy games (e.g., chess and Go ). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it\\'s not labeled AI anymore .\" [ 2 ] [ 3 ] Alan Turing was the first person to conduct substantial research in the field that he called \"machine intelligence\". [ 4 ] Artificial intelligence was founded as an academic discipline in 1956, [ 5 ] by those now considered the founding fathers of AI: John McCarthy , Marvin Minksy , Nathaniel Rochester , and Claude Shannon . [ 6 ] [ 7 ] The field went through multiple cycles of optimism, [ 8 ] [ 9 ] followed by periods of disappointment and loss of funding, known as AI winter . [ 10 ] [ 11 ] Funding and interest vastly increased after 2012 when deep learning surpassed all previous AI techniques, [ 12 ] and after 2017 with the transformer architecture . [ 13 ] This led to the AI boom of the early 2020s, with companies, universities, and laboratories overwhelmingly based in the United States pioneering significant advances in artificial intelligence . [ 14 ] The growing use of artificial intelligence in the 21st century is influencing a societal and economic shift towards increased automation , data-driven decision-making , and the integration of AI systems into various economic sectors and areas of life, impacting job markets , healthcare , government , industry , education, propaganda , and disinformation . This raises questions about the long-term effects , ethical implications , and risks of AI , prompting discussions about regulatory policies to ensure the safety and benefits of the technology . The various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning , knowledge representation , planning , learning , natural language processing , perception, and support for robotics . [ a ] General intelligence —the ability to complete any task performable by a human on an at least equal level—is among the field\\'s long-term goals. [ 15 ] To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization , formal logic , artificial neural networks , and methods based on statistics , operations research , and economics . [ b ] AI also draws upon psychology , linguistics , philosophy , neuroscience , and other fields. [ 16 ] Goals The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research. [ a ] Reasoning and problem-solving Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions . [ 17 ] By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics . [ 18 ] Many of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. [ 19 ] Even humans rarely use the step-by-step deduction that early AI research could model. Knowledge representation An ontology represents knowledge as a set of concepts within a domain and the relationships between those concepts Knowledge representation and knowledge engineering [ 21 ] allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, [ 22 ] scene interpretation, [ 23 ] clinical decision support, [ 24 ] knowledge discovery (mining \"interesting\" and actionable inferences from large databases ), [ 25 ] and other areas. [ 26 ] A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. [ 27 ] Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; [ 28 ] situations, events, states, and time; [ 29 ] causes and effects; [ 30 ] knowledge about knowledge (what we know about what other people know); [ 31 ] default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); [ 32 ] and many other aspects and domains of knowledge. Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); [ 33 ] and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally). [ 20 ] There is also the difficulty of knowledge acquisition , the problem of obtaining knowledge for AI applications. [ c ] Planning and decision-making An \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen. [ d ] [ 36 ] In automated planning , the agent has a specific goal. [ 37 ] In automated decision-making , the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \" utility \") that measures how much the agent prefers it. For each possible action, it can calculate the \" expected utility \": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility. [ 38 ] In classical planning , the agent knows exactly what the effect of any action will be. [ 39 ] In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked. [ 40 ] In some problems, the agent\\'s preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning ), or the agent can seek information to improve its preferences. [ 41 ] Information value theory can be used to weigh the value of exploratory or experimental actions. [ 42 ] The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be. A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. The policy could be calculated (e.g., by iteration ), be heuristic , or it can be learned. [ 43 ] Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents. [ 44 ] Learning Machine learning is the study of programs that can improve their performance on a given task automatically. [ 45 ] It has been a part of AI from the beginning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. [ 48 ] Supervised learning requires a human to label the input data first, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input). [ 49 ] In reinforcement learning , the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\". [ 50 ] Transfer learning is when the knowledge gained from one problem is applied to a new problem. [ 51 ] Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning. [ 52 ] Computational learning theory can assess learners by computational complexity , by sample complexity (how much data is required), or by other notions of optimization . [ 53 ] Natural language processing Natural language processing (NLP) [ 54 ] allows programs to read, write and communicate in human languages such as English . Specific problems include speech recognition , speech synthesis , machine translation , information extraction , information retrieval and question answering . [ 55 ] Early work, based on Noam Chomsky \\'s generative grammar and semantic networks , had difficulty with word-sense disambiguation [ f ] unless restricted to small domains called \" micro-worlds \" (due to the common sense knowledge problem [ 33 ] ). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure. Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), [ 56 ] transformers (a deep learning architecture using an attention mechanism), [ 57 ] and others. [ 58 ] In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, [ 59 ] [ 60 ] and by 2023, these models were able to get human-level scores on the bar exam , SAT test, GRE test, and many other real-world applications. [ 61 ] Perception Machine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar , sonar, radar, and tactile sensors ) to deduce aspects of the world. [ 62 ] The field includes speech recognition , [ 63 ] image classification , [ 64 ] facial recognition , object recognition , [ 65 ] object tracking , [ 66 ] and robotic perception . [ 67 ] Social intelligence Kismet , a robot head which was made in the 1990s; a machine that can recognize and simulate emotions [ 68 ] Affective computing is an interdisciplinary umbrella that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood . [ 69 ] For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction . However, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents. [ 70 ] Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis , wherein AI classifies the affects displayed by a videotaped subject. [ 71 ] General intelligence A machine with artificial general intelligence should be able to solve a wide variety of problems with breadth and versatility similar to human intelligence . [ 15 ] Techniques AI research uses a wide variety of techniques to accomplish the goals above. [ b ] Search and optimization AI can solve many problems by intelligently searching through many possible solutions. [ 72 ] There are two very different kinds of search used in AI: state space search and local search . State space search State space search searches through a tree of possible states to try to find a goal state. [ 73 ] For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis . [ 74 ] Simple exhaustive searches [ 75 ] are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers . The result is a search that is too slow or never completes. [ 19 ] \" Heuristics \" or \"rules of thumb\" can help prioritize choices that are more likely to reach a goal. [ 76 ] Adversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and counter-moves, looking for a winning position. [ 77 ] Local search Illustration of gradient descent for 3 different starting points; two parameters (represented by the plan coordinates) are adjusted in order to minimize the loss function (the height) Local search uses mathematical optimization to find a solution to a problem. [ 78 ] Gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function . Variants of gradient descent are commonly used to train neural networks. [ 79 ] Another type of local search is evolutionary computation , which aims to iteratively improve a set of candidate solutions by \"mutating\" and \"recombining\" them, selecting only the fittest to survive each generation. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking ) and ant colony optimization (inspired by ant trails ). [ 81 ] Logic Formal logic is used for reasoning and knowledge representation . [ 82 ] Formal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\") [ 83 ] and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \" Every X is a Y \" and \"There are some X s that are Y s\"). [ 84 ] Deductive reasoning in logic is the process of proving a new statement ( conclusion ) from other statements that are given and assumed to be true (the premises ). [ 85 ] Proofs can be structured as proof trees , in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules . Given a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms . In the case of Horn clauses , problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem. [ 86 ] In the more general case of the clausal form of first-order logic , resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved. [ 87 ] Inference in both Horn clause logic and first-order logic is undecidable , and therefore intractable . However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog , is Turing complete . Moreover, its efficiency is competitive with computation in other symbolic programming languages. [ 89 ] Non-monotonic logics , including logic programming with negation as failure , are designed to handle default reasoning . [ 32 ] Other specialized versions of logic have been developed to describe many complex domains. Probabilistic methods for uncertain reasoning A simple Bayesian network , with the associated conditional probability tables Many problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics. [ 90 ] Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory , decision analysis , [ 91 ] and information value theory . [ 92 ] These tools include models such as Markov decision processes , [ 93 ] dynamic decision networks , [ 94 ] game theory and mechanism design . [ 95 ] Bayesian networks [ 96 ] are a tool that can be used for reasoning (using the Bayesian inference algorithm), [ g ] [ 98 ] learning (using the expectation–maximization algorithm ), [ h ] [ 100 ] planning (using decision networks ) [ 101 ] and perception (using dynamic Bayesian networks ). [ 94 ] Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters ). [ 94 ] Expectation–maximization clustering of Old Faithful eruption data starts from a random guess but then successfully converges on an accurate clustering of the two physically distinct modes of eruption Classifiers and statistical learning methods The simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand. Classifiers [ 102 ] are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning . Each pattern (also called an \" observation \") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set . When a new observation is received, that observation is classified based on previous experience. [ 103 ] The decision tree is the simplest and most widely used symbolic machine learning algorithm. [ 104 ] K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s. [ 105 ] The naive Bayes classifier is reportedly the \"most widely used learner\" [ 106 ] at Google, due in part to its scalability. [ 108 ] Artificial neural networks A neural network is an interconnected group of nodes, akin to the vast network of neurons in the human brain An artificial neural network is based on a collection of nodes also known as artificial neurons , which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers. [ 108 ] Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. [ 109 ] Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. [ 110 ] In feedforward neural networks the signal passes in only one direction. [ 111 ] Recurrent neural networks feed the output signal back into the input, which allows short-term memories of previous input events. Long short term memory is the most successful network architecture for recurrent networks. [ 112 ] Perceptrons [ 113 ] use only a single layer of neurons, deep learning [ 114 ] uses multiple layers. Convolutional neural networks strengthen the connection between neurons that are \"close\" to each other—this is especially important in image processing , where a local set of neurons must identify an \"edge\" before the network can identify an object. [ 115 ] Deep learning Deep learning [ 114 ] uses several layers of neurons between the network\\'s inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing , lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces. [ 116 ] Deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision , speech recognition , natural language processing , image classification , [ 117 ] and others. The reason that deep learning performs so well in so many applications is not known as of 2023. [ 118 ] The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) [ i ] but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs ) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet . [ j ] GPT Generative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pretrained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \" hallucinations \", although this can be reduced with RLHF and quality data. They are used in chatbots , which allow people to ask a question or request a task in simple text. [ 126 ] [ 127 ] Current models and services include Gemini (formerly Bard), ChatGPT , Grok , Claude , Copilot , and LLaMA . [ 128 ] Multimodal GPT models can process different types of data ( modalities ) such as images, videos, sound, and text. [ 129 ] Specialized hardware and software Main articles: Programming languages for artificial intelligence and Hardware for artificial intelligence In the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models\\' training. [ 130 ] Specialized programming languages such as Prolog were used in early AI research, [ 131 ] but general-purpose programming languages like Python have become predominant. [ 132 ] Applications Main article: Applications of artificial intelligence AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search ), targeting online advertisements , recommendation systems (offered by Netflix , YouTube or Amazon ), driving internet traffic , targeted advertising ( AdSense , Facebook ), virtual assistants (such as Siri or Alexa ), autonomous vehicles (including drones , ADAS and self-driving cars ), automatic language translation ( Microsoft Translator , Google Translate ), facial recognition ( Apple \\'s Face ID or Microsoft \\'s DeepFace and Google \\'s FaceNet ) and image labeling (used by Facebook , Apple\\'s iPhoto and TikTok ). The deployment of AI may be overseen by a Chief automation officer (CAO). Health and medicine Main article: Artificial intelligence in healthcare The application of AI in medicine and medical research has the potential to increase patient care and quality of life. [ 133 ] Through the lens of the Hippocratic Oath , medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients. For medical research, AI is an important tool for processing and integrating big data . This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication. [ 134 ] It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research. For example, AlphaFold 2 demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein . [ 135 ] In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria. [ 136 ] In 2024, researchers used machine learning to accelerate the search for Parkinson\\'s disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson\\'s disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold. [ 137 ] [ 138 ] Games Main article: Game artificial intelligence Game playing programs have been used since the 1950s to demonstrate and test AI\\'s most advanced techniques. [ 139 ] Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov , on 11 May 1997. quiz show exhibition match, IBM \\'s question answering system , Watson , defeated the two greatest Jeopardy! [ 141 ] In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol , becoming the first computer Go -playing system to beat a professional Go player without handicaps . Then in 2017 it defeated Ke Jie , who was the best Go player in the world. [ 143 ] DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero , which could be trained to play chess, Go, or Atari games. [ 144 ] In 2019, DeepMind\\'s AlphaStar achieved grandmaster level in StarCraft II , a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map. [ 145 ] In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world\\'s best Gran Turismo drivers using deep reinforcement learning. [ 146 ] In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions. [ 147 ] Mathematics In mathematics, special forms of formal step-by-step reasoning are used. In contrast, LLMs such as GPT-4 Turbo , Gemini Ultra , Claude Opus , LLaMa-2 or Mistral Large are working with probabilistic models, which can produce wrong answers in the form of hallucinations . Therefore, they need not only a large database of mathematical problems to learn from but also methods such as supervised fine-tuning or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections. [ 148 ] A 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data. [ 149 ] Alternatively, dedicated models for mathematic problem solving with higher precision for the outcome including proof of theorems have been developed such as Alpha Tensor , Alpha Geometry and Alpha Proof all from Google DeepMind , [ 150 ] Llemma from eleuther [ 151 ] or Julius . [ 152 ] When natural language is used to describe mathematical problems, converters transform such prompts into a formal language such as Lean to define mathematic tasks. Some models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics. [ 153 ] Finance Finance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated \"robot advisers\" have been in use for some years. [ 154 ] World Pensions experts like Nicolas Firzli insist it may be too early to see the emergence of highly innovative AI-informed financial products and services: \"the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I’m not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.\" [ 155 ] Military Main article: Military artificial intelligence Various countries are deploying AI military applications. [ 157 ] Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles . [ 156 ] AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition , coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams. [ 156 ] In November 2023, US Vice President Kamala Harris disclosed a declaration signed by 31 nations to set guardrails for the military use of AI. The commitments include using legal reviews to ensure the compliance of military AI with international laws, and being cautious and transparent in the development of this technology. [ 158 ] Generative AI Main article: Generative artificial intelligence Vincent van Gogh in watercolour created by generative AI software In the early 2020s, generative AI gained widespread prominence. [ 159 ] The increasing realism and ease-of-use of AI-based text-to-image generators such as Midjourney , DALL-E , and Stable Diffusion sparked a trend of viral AI-generated photos. Widespread attention was gained by a fake photo of Pope Francis wearing a white puffer coat, the fictional arrest of Donald Trump , and a hoax of an attack on the Pentagon , as well as the usage in professional creative arts. [ 160 ] [ 161 ] Agents Artificial intelligent (AI) agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants , chatbots , autonomous vehicles , game-playing systems , and industrial robotics . AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks. [ 162 ] [ 163 ] [ 164 ] Other industry-specific tasks There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated \"AI\" in some offerings or processes. [ 165 ] A few examples are energy storage , medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy , or supply chain management. AI has been used to investigate if and how people evacuated in large scale and small scale evacuations using historical data from GPS, videos or social media. Further, AI can provide real time information on the real time evacuation conditions. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water. Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights\" for example for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. It could also be used for activities in space such as space exploration, including analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation. Ethics Main article: Ethics of artificial intelligence AI has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of Deep Mind hopes to \"solve intelligence, and then use that to solve everything else\". [ 169 ] However, as the use of AI has become widespread, several unintended consequences and risks have been identified. [ 170 ] In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning. [ 171 ] Risks and harm Privacy and copyright Further information: Information privacy and Artificial intelligence and copyright Machine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy , surveillance and copyright . AI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI\\'s ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency. Sensitive user data collected may include online activity records, geolocation data, video or audio. [ 172 ] For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them. [ 173 ] Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy . [ 174 ] AI developers argue that this is the only way to deliver valuable applications. and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation , de-identification and differential privacy . [ 175 ] Since 2016, some privacy experts, such as Cynthia Dwork , have begun to view privacy in terms of fairness . Brian Christian wrote that experts have pivoted \"from the question of \\'what they know\\' to the question of \\'what they\\'re doing with it\\'.\" [ 176 ] Generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \" fair use \". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\". [ 177 ] [ 178 ] Website owners who do not wish to have their content scraped can indicate it in a \" robots.txt \" file. [ 179 ] In 2023, leading authors (including John Grisham and Jonathan Franzen ) sued AI companies for using their work to train generative AI. [ 180 ] [ 181 ] Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors. [ 182 ] Dominance by tech giants The commercial AI scene is dominated by Big Tech companies such as Alphabet Inc. , Amazon , Apple Inc. , Meta Platforms , and Microsoft . [ 183 ] [ 184 ] [ 185 ] Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers , allowing them to entrench further in the marketplace. [ 186 ] [ 187 ] Substantial power needs and other environmental impacts See also: Environmental impacts of artificial intelligence In January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026 , forecasting electric power use. [ 188 ] This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation. [ 189 ] Prodigious power consumption by AI is responsible for the growth of fossil fuels use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources – from nuclear energy to geothermal to fusion. The tech firms argue that – in the long view – AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and \"intelligent\", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms. [ 190 ] A 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge , found \"US power demand (is) likely to experience growth not seen in a generation….\" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means. [ 191 ] Data centers\\' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all. [ 192 ] In 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. [ 193 ] Misinformation See also: YouTube § Moderation and offensive content YouTube , Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation , conspiracy theories , and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. [ 194 ] This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government. [ 195 ] The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took steps to mitigate the problem [ citation needed ] . In 2022, generative AI began to create images, audio, video and text that are indistinguishable from real photographs, recordings, films, or human writing. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda. [ 196 ] AI pioneer Geoffrey Hinton expressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks. [ 197 ] Algorithmic bias and fairness Main articles: Algorithmic bias and Fairness (machine learning) In statistics, a bias is a systematic error or deviation from the correct value. But in the context of fairness , it often refers to a tendency in favor or against a certain group or individual characteristic, usually in a way that is considered unfair or harmful. A statistically unbiased AI system that produces disparate outcomes for different demographic groups may thus be viewed as biased in the ethical sense. [ 198 ] The field of fairness studies how to prevent harms from algorithmic biases. There are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems don\\'t reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws . [ 198 ] Machine learning applications will be biased if they learn from biased data. [ 199 ] The developers may not be aware that the bias exists. [ 200 ] Bias can be introduced by the way training data is selected and by the way a model is deployed. [ 201 ] [ 199 ] If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine , finance , recruitment , housing or policing ) then the algorithm may cause discrimination . [ 202 ] On June 28, 2015, Google Photos \\'s new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few images of black people, [ 203 ] a problem called \"sample size disparity\". [ 204 ] Google \"fixed\" this problem by preventing the system from labelling anything as a \"gorilla\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon. [ 205 ] COMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist . In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different—the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend. [ 206 ] In 2017, several researchers [ k ] showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data. [ 208 ] A program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \"race\" or \"gender\"). The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\". [ 209 ] Moritz Hardt said \"the most robust fact in this research area is that fairness through blindness doesn\\'t work.\" [ 210 ] Criticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations , some of these \"recommendations\" will likely be racist. [ 211 ] Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. [ l ] Bias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women. [ 204 ] At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery , in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed. [ dubious – discuss ] [ 213 ] Lack of transparency See also: Explainable AI , Algorithmic transparency , and Right to explanation Many AI systems are so complex that their designers cannot explain how they reach their decisions. [ 214 ] Particularly with deep neural networks , in which there are a large amount of non- linear relationships between inputs and outputs. [ 215 ] It is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as \"cancerous\", because pictures of malignancies typically include a ruler to show the scale. [ 216 ] Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading. [ 217 ] People who have been harmed by an algorithm\\'s decision have a right to an explanation. [ 218 ] Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union\\'s General Data Protection Regulation in 2016 included an explicit statement that this right exists. [ m ] Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used. [ 219 ] DARPA established the XAI (\"Explainable Artificial Intelligence\") program in 2014 to try and solve these problems. [ 222 ] Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned. [ 223 ] Deconvolution , DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning. [ 224 ] For generative pre-trained transformers , Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts. [ 225 ] Bad actors and weaponized AI Main articles: Lethal autonomous weapon , Artificial intelligence arms race , and AI safety Artificial intelligence provides a number of tools that are useful to bad actors , such as authoritarian governments , terrorists , criminals or rogue states . A lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision. [ n ] Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction . [ 227 ] Even when used in conventional warfare, it is unlikely that they will be unable to reliably choose targets and could potentially kill an innocent person . [ 227 ] In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations \\' Convention on Certain Conventional Weapons , however the United States and others disagreed. [ 229 ] AI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Machine learning , operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Advanced AI can make authoritarian centralized decision making more competitive than liberal and decentralized systems such as markets . [ 230 ] All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mass surveillance in China. [ 231 ] [ 232 ] There many other ways that AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours. [ 233 ] Technological unemployment Main articles: Workplace impact of artificial intelligence and Technological unemployment Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment. [ 234 ] In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we\\'re in uncharted territory\" with AI. [ 235 ] A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment , but they generally agree that it could be a net benefit if productivity gains are redistributed . [ 236 ] Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\". [ o ] [ 238 ] The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies. [ 234 ] In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence. [ 239 ] [ 240 ] Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\". [ 241 ] Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy. [ 242 ] From the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum , about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement. [ 243 ] Existential risk Main article: Existential risk from artificial general intelligence It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, \" spell the end of the human race \". [ 244 ] This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character. First, AI does not require human-like \" sentience \" to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip factory manager ). [ 246 ] Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can\\'t fetch the coffee if you\\'re dead.\" [ 247 ] In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity\\'s morality and values so that it is \"fundamentally on our side\". [ 248 ] Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. Things like ideologies , law , government , money and the economy are made of language ; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive. [ 249 ] The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. [ 250 ] Personalities such as Stephen Hawking , Bill Gates , and Elon Musk , [ 251 ] as well as AI pioneers such as Yoshua Bengio , Stuart Russell , Demis Hassabis , and Sam Altman , have expressed concerns about existential risk from AI. In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google.\" [ 252 ] He notably mentioned risks of an AI takeover , [ 253 ] and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI. [ 254 ] In 2023, many leading AI experts issued the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\". AI pioneer Juergen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\" [ 256 ] While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\" [ 257 ] [ 258 ] Andrew Ng also argued that \"it\\'s a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests.\" [ 259 ] Yann LeCun \"scoffs at his peers\\' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\" [ 260 ] In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine. [ 261 ] However, after 2016, the study of current and future risks and possible solutions became a serious area of research. [ 262 ] Ethical machines and alignment Main articles: Machine ethics , AI safety , Friendly artificial intelligence , Artificial moral agents , and Human Compatible Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky , who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk. [ 263 ] Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas. [ 264 ] The field of machine ethics is also called computational morality, [ 264 ] and was founded at an AAAI symposium in 2005. [ 265 ] Other approaches include Wendell Wallach \\'s \"artificial moral agents\" [ 266 ] and Stuart J. Russell \\'s three principles for developing provably beneficial machines. [ 267 ] Open source Active organizations in the AI open-source community include Hugging Face , [ 268 ] Google , [ 269 ] EleutherAI and Meta . [ 270 ] Various AI models, such as Llama 2 , Mistral or Stable Diffusion , have been made open-weight, [ 271 ] [ 272 ] meaning that their architecture and trained parameters (the \"weights\") are publicly available. Open-weight models can be freely fine-tuned , which allows companies to specialize them with their own data and for their own use-case. [ 273 ] Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism ) and that once released on the Internet, they can\\'t be deleted everywhere if needed. [ 274 ] Frameworks Artificial Intelligence projects can have their ethical permissibility tested while designing, developing, and implementing an AI system. An AI framework such as the Care and Act Framework containing the SUM values—developed by the Alan Turing Institute tests projects in four main areas: [ 275 ] [ 276 ] Respect the dignity of individual people Connect with other people sincerely, openly, and inclusively Care for the wellbeing of everyone Protect social values, justice, and the public interest Other developments in ethical frameworks include those decided upon during the Asilomar Conference , the Montreal Declaration for Responsible AI, and the IEEE\\'s Ethics of Autonomous Systems initiative, among others; [ 277 ] however, these principles do not go without their criticisms, especially regards to the people chosen contributes to these frameworks. [ 278 ] Promotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers. [ 279 ] The UK AI Safety Institute released in 2024 a testing toolset called \\'Inspect\\' for AI safety evaluations available under a MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities. [ 280 ] Regulation Main articles: Regulation of artificial intelligence , Regulation of algorithms , and AI safety The first global AI Safety Summit was held in 2023 with a declaration calling for international co-operation The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms. [ 281 ] The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. [ 282 ] According to AI Index at Stanford , the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone. [ 283 ] [ 284 ] Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI. [ 285 ] Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. [ 285 ] The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. [ 285 ] Henry Kissinger , Eric Schmidt , and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. [ 286 ] In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years. [ 287 ] In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, governments officials and academics. [ 288 ] In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\". [ 283 ] A 2023 Reuters /Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. [ 289 ] In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\". [ 290 ] [ 291 ] In November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks. [ 292 ] 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence. [ 293 ] [ 294 ] In May 2024 at the AI Seoul Summit , 16 global AI tech companies agreed to safety commitments on the development of AI. [ 295 ] [ 296 ] History Main article: History of artificial intelligence For a chronological guide, see Timeline of artificial intelligence . The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing \\'s theory of computation , which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning. [ 297 ] [ 4 ] This, along with concurrent discoveries in cybernetics , information theory and neurobiology , led researchers to consider the possibility of building an \"electronic brain\". [ q ] They developed several areas of research that would become part of AI, [ 299 ] such as McCullouch and Pitts design for \"artificial neurons\" in 1943, [ 119 ] and Turing\\'s influential 1950 paper \\' Computing Machinery and Intelligence \\', which introduced the Turing test and showed that \"machine intelligence\" was plausible. [ 300 ] [ 4 ] The field of AI research was founded at a workshop at Dartmouth College in 1956. [ s ] They and their students produced programs that the press described as \"astonishing\": [ t ] computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. [ u ] [ 8 ] Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s. [ 4 ] Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field. [ 304 ] In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". [ 305 ] In 1967 Marvin Minsky agreed, writing, \"within a generation ... the problem of creating \\'artificial intelligence\\' will substantially be solved\". [ v ] In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill [ 308 ] and ongoing pressure from the U.S. Congress to fund more productive projects . [ 309 ] Minsky \\'s and Papert \\'s book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether. [ 310 ] The \" AI winter \", a period when obtaining funding for AI projects was difficult, followed. [ 10 ] In the early 1980s, AI research was revived by the commercial success of expert systems , [ 311 ] a form of AI program that simulated the knowledge and analytical skills of human experts. At the same time, Japan\\'s fifth generation computer project inspired the U.S. and British governments to restore funding for academic research . [ 9 ] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began. [ 11 ] Up to this point, most of AI\\'s funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception , robotics , learning and pattern recognition , [ 312 ] and began to look into \"sub-symbolic\" approaches. [ 313 ] Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive. [ w ] Judea Pearl , Lofti Zadeh and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic. [ 90 ] [ 318 ] But the most important development was the revival of \" connectionism \", including neural network research, by Geoffrey Hinton and others. [ 319 ] In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks. [ 320 ] AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \" narrow \" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics , economics and mathematics ). [ 321 ] By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\". [ 322 ] However, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s. [ 15 ] Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field. [ x ] Deep learning\\'s success was based on both hardware improvements ( faster computers , [ 324 ] graphics processing units , cloud computing [ 325 ] ) and access to large amounts of data [ 326 ] (including curated datasets, [ 325 ] such as ImageNet ). Deep learning\\'s success led to an enormous increase in interest and funding in AI. [ y ] The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019. [ 285 ] In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study. [ 262 ] In the late teens and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo , developed by DeepMind , beat the world champion Go player . The program was taught only the rules of the game and developed strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. [ 327 ] These programs, and others, inspired an aggressive AI boom , where large companies began investing billions in AI research. According to AI Impacts, about $50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\". [ 329 ] Philosophy Main article: Philosophy of artificial intelligence Defining artificial intelligence Main articles: Turing test , Intelligent agent , Dartmouth workshop , and Synthetic intelligence Alan Turing wrote in 1950 \"I propose to consider the question \\'can machines think\\'?\" [ 330 ] He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\". [ 330 ] He devised the Turing test, which measures the ability of a machine to simulate human conversation. [ 300 ] Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks.\" [ 331 ] The Turing test can provide some evidence of intelligence, but it penalizes non-human intelligent behavior. [ 332 ] Russell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure. [ 1 ] However, they are critical that the test requires the machine to imitate humans. \" Aeronautical engineering texts,\" they wrote, \"do not define the goal of their field as making \\'machines that fly so exactly like pigeons that they can fool other pigeons. \\' \" [ 333 ] AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\". [ 334 ] McCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\". [ 335 ] Another AI founder, Marvin Minsky similarly describes it as \"the ability to solve hard problems\". [ 336 ] The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals. [ 1 ] These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine—and no other philosophical discussion is required, or may not even be possible. Another definition has been adopted by Google, [ 337 ] a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence. Some authors have suggested in practice, that the definition of AI is vague and difficult to define, with contention as to whether classical algorithms should be categorised as AI, [ 338 ] with many companies during the early 2020s AI boom using the term as a marketing buzzword , often even if they did \"not actually use AI in a material way\". [ 339 ] Evaluating approaches to AI No established unifying theory or paradigm has guided AI research for most of its history. [ z ] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). Critics argue that these questions may have to be revisited by future generations of AI researchers. Symbolic AI and its limits Symbolic AI (or \" GOFAI \") [ 341 ] simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis : \"A physical symbol system has the necessary and sufficient means of general intelligent action.\" [ 342 ] However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec\\'s paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult. [ 343 ] Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge. [ 344 ] Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him. [ aa ] [ 20 ] The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias . Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, [ 346 ] [ 347 ] in part because sub-symbolic AI is a move away from explainable AI : it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. Neat vs. scruffy Main article: Neats and scruffies \"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic , optimization , or neural networks ). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s, [ 348 ] but eventually was seen as irrelevant. Soft vs. hard computing Main article: Soft computing Finding a provably correct or optimal solution is intractable for many important problems. [ 19 ] Soft computing is a set of techniques, including genetic algorithms , fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks. Narrow vs. general AI Main articles: Weak artificial intelligence and Artificial general intelligence AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field\\'s long-term goals. [ 349 ] [ 350 ] General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. Machine consciousness, sentience, and mind Main articles: Philosophy of artificial intelligence and Artificial consciousness The philosophy of mind does not know whether a machine can have a mind , consciousness and mental states , in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\" [ 351 ] However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction . Consciousness Main articles: Hard problem of consciousness and Theory of mind David Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness. [ 352 ] The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett\\'s consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like . [ 353 ] Computationalism and functionalism Main articles: Computational theory of mind , Functionalism (philosophy of mind) , and Chinese room Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem . This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam . [ 354 ] Philosopher John Searle characterized this position as \" strong AI \": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\" [ ab ] Searle counters this assertion with his Chinese room argument, which attempts to show that, even if a machine perfectly simulates human behavior, there is still no reason to suppose it also has a mind. [ 358 ] AI welfare and rights It is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree. [ 359 ] But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals. [ 360 ] [ 361 ] Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness ) may provide another moral basis for AI rights. [ 360 ] Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society. [ 362 ] In 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities. [ 363 ] Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights , and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part to society on their own. [ 364 ] [ 365 ] In 2019, Soenke Ziesche and Roman Yampolskiy coined the term “AI welfare” and outlined the new field of AI welfare science, which is derived from animal welfare science . Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming , which could lead to large-scale suffering if sentient AI is created and carelessly exploited. [ 361 ] [ 360 ] Future Superintelligence and the singularity A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. [ 350 ] If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself . The improved software would be even better at improving itself, leading to what I. J. Good called an \" intelligence explosion \" and Vernor Vinge called a \" singularity \". [ 367 ] However, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve , slowing when they reach the physical limits of what the technology can do. [ 368 ] Transhumanism Robot designer Hans Moravec , cyberneticist Kevin Warwick , and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. [ 369 ] Edward Fredkin argues that \"artificial intelligence is the next stage in evolution\", an idea first proposed by Samuel Butler \\'s \" Darwin among the Machines \" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence . [ 370 ] In fiction Main article: Artificial intelligence in fiction The word \"robot\" itself was coined by Karel Čapek in his 1921 play R.U.R. , the title standing for \"Rossum\\'s Universal Robots\" Thought-capable artificial beings have appeared as storytelling devices since antiquity, [ 371 ] and have been a persistent theme in science fiction . [ 372 ] A common trope in these works began with Mary Shelley \\'s Frankenstein , where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke\\'s and Stanley Kubrick\\'s 2001: A Space Odyssey (both 1968), with HAL 9000 , the murderous computer in charge of the Discovery One spaceship, as well as The Terminator and The Matrix . In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still and Bishop from Aliens are less prominent in popular culture. [ 373 ] Isaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the \" Multivac \" super-intelligent computer. Asimov\\'s laws are often brought up during lay discussions of machine ethics; [ 374 ] while almost all artificial intelligence researchers are familiar with Asimov\\'s laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity. [ 375 ] Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel , and thus to suffer. Artificial Intelligence and Ex Machina , as well as the novel Do Androids Dream of Electric Sheep? Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence. [ 376 ] See also Artificial intelligence detection software – Software to detect AI-generated content Pages displaying short descriptions of redirect targets Behavior selection algorithm – Algorithm that selects actions for intelligent agents Business process automation – Automation of business processes Case-based reasoning – Process of solving new problems based on the solutions of similar past problems Computational intelligence – Ability of a computer to learn a specific task from data or experimental observation Digital immortality – Hypothetical concept of storing a personality in digital form Emergent algorithm – Algorithm exhibiting emergent behavior Female gendering of AI technologies – Gender biases in digital technology Pages displaying short descriptions of redirect targets Glossary of artificial intelligence – List of definitions of terms and concepts commonly used in the study of artificial intelligence Intelligence amplification – Use of information technology to augment human intelligence Mind uploading – Hypothetical process of digitally emulating a brain Robotic process automation – Form of business process automation technology Weak artificial intelligence – Form of artificial intelligence Wetware computer – Computer composed of organic material Explanatory notes ^ a b This list of intelligent traits is based on the topics covered by the major AI textbooks, including: Russell & Norvig , Luger & Stubblefield , Poole, Mackworth & Goebel and Nilsson ^ a b This list of tools is based on the topics covered by the major AI textbooks, including: Russell & Norvig , Luger & Stubblefield , Poole, Mackworth & Goebel and Nilsson ^ It is among the reasons that expert systems proved to be inefficient for capturing knowledge. [ 34 ] [ 35 ] ^ \"Rational agent\" is general term used in economics , philosophy and theoretical artificial intelligence. It can refer to anything that directs its behavior to accomplish goals, such as a person, an animal, a corporation, a nation, or in the case of AI, a computer program. ^ Alan Turing discussed the centrality of learning as early as 1950, in his classic paper \" Computing Machinery and Intelligence \". [ 46 ] In 1956, at the original Dartmouth AI summer conference, Ray Solomonoff wrote a report on unsupervised probabilistic machine learning: \"An Inductive Inference Machine\". [ 47 ] ^ See AI winter § Machine translation and the ALPAC report of 1966 ^ Compared with symbolic logic, formal Bayesian inference is computationally expensive. AdSense uses a Bayesian network with over 300 million edges to learn which ads to serve. [ 97 ] ^ Expectation–maximization, one of the most popular algorithms in machine learning, allows clustering in the presence of unknown latent variables . [ 99 ] ^ Some form of deep neural networks (without a specific learning algorithm) were described by: Warren S. McCulloch and Walter Pitts [ 119 ] Alan Turing ; [ 120 ] Karl Steinbuch and Roger David Joseph . [ 121 ] Deep or recurrent networks that learned (or used gradient descent) were developed by: Frank Rosenblatt ; [ 120 ] Oliver Selfridge ; [ 121 ] Alexey Ivakhnenko and Valentin Lapa ; [ 122 ] Kaoru Nakano ; [ 123 ] Shun-Ichi Amari ; [ 123 ] John Joseph Hopfield . [ 123 ] Precursors to backpropagation were developed by: Henry J. Kelley ; [ 120 ] Arthur E. Bryson ; [ 120 ] Stuart Dreyfus ; [ 120 ] Arthur E. Bryson and Yu-Chi Ho ; [ 120 ] Backpropagation was independently developed by: Seppo Linnainmaa ; [ 124 ] Paul Werbos . [ 120 ] ^ Geoffrey Hinton said, of his work on neural networks in the 1990s, \"our labeled datasets were thousands of times too small. [ 125 ] ^ Including Jon Kleinberg ( Cornell University ), Sendhil Mullainathan ( University of Chicago ), Cynthia Chouldechova ( Carnegie Mellon ) and Sam Corbett-Davis ( Stanford ) [ 207 ] ^ Moritz Hardt (a director at the Max Planck Institute for Intelligent Systems ) argues that machine learning \"is fundamentally the wrong tool for a lot of domains, where you\\'re trying to design interventions and mechanisms that change the world.\" [ 212 ] ^ When the law was passed in 2018, it still contained a form of this provision. ^ This is the United Nations \\' definition, and includes things like land mines as well. [ 226 ] ^ See table 4; 9% is both the OECD average and the U.S. average. [ 237 ] ^ Sometimes called a \" robopocalypse \" [ 245 ] ^ \"Electronic brain\" was the term used by the press around this time. [ 297 ] [ 298 ] ^ Daniel Crevier wrote, \"the conference is generally recognized as the official birthdate of the new science.\" [ 119 ] ^ Russell and Norvig wrote \"for the next 20 years the field would be dominated by these people and their students.\" [ 302 ] ^ Russell and Norvig wrote \"it was astonishing whenever a computer did anything kind of smartish\". [ 303 ] ^ The programs described are Arthur Samuel \\'s checkers program for the IBM 701 , Daniel Bobrow \\'s STUDENT , Newell and Simon \\'s Logic Theorist and Terry Winograd \\'s SHRDLU . ^ Russell and Norvig write: \"in almost all cases, these early systems failed on more difficult problems\" [ 307 ] ^ Embodied approaches to AI [ 314 ] were championed by Hans Moravec [ 315 ] and Rodney Brooks [ 316 ] and went by many names: Nouvelle AI . [ 317 ] ^ Matteo Wong wrote in The Atlantic : \"Whereas for decades, computer-science fields such as natural-language processing, computer vision, and robotics used extremely different methods, now they all use a programming method called \"deep learning.\" As a result, their code and approaches have become more similar, and their models are easier to integrate into one another.\" [ 323 ] ^ Jack Clark wrote in Bloomberg : \"After a half-decade of quiet breakthroughs in artificial intelligence, 2015 has been a landmark year. Computers are smarter and learning faster than ever\", and noted that the number of software projects that use machine learning at Google increased from a \"sporadic usage\" in 2012 to more than 2,700 projects in 2015. [ 325 ] ^ Nils Nilsson wrote in 1983: \"Simply put, there is wide disagreement in the field about what AI is all about.\" [ 340 ] ^ Daniel Crevier wrote that \"time has proven the accuracy and perceptiveness of some of Dreyfus\\'s comments. Had he formulated them less aggressively, constructive actions they suggested might have been taken much earlier.\" [ 355 ] Searle\\'s original formulation was \"The appropriately programmed computer really is a mind, in the sense that computers given the right programs can be literally said to understand and have other cognitive states.\" [ 356 ] Strong AI is defined similarly by Russell and Norvig : \"Stong AI – the assertion that machines that do so are actually thinking (as opposed to simulating thinking).\" ^ AI set to exceed human brain power Archived 2008-02-19 at the Wayback Machine CNN.com (July 26, 2006) ^ Kaplan, Andreas; Haenlein, Michael . The Essential Turing: the ideas that gave birth to the computer age . Artificial Intelligence, Business and Civilization: Our Fate Made in Machines . A Guided Tour of Artificial Intelligence Research: Volume III: Interfaces and Applications of Artificial Intelligence . 19–21) ^ a b Funding initiatives in the early 1980s: Fifth Generation Project (Japan), Alvey (UK), Microelectronics and Computer Technology Corporation (US), Strategic Computing Initiative (US): McCorduck (2004 , pp. 301–318) ^ a b Deep learning revolution, AlexNet : Goldman Russell & Norvig (2021 , p. 26) McKinsey ^ Toews . 32–33, 1020–1021) Proposal for the modern version: Pennachin & Goertzel Warnings of overspecialization in AI from leading researchers: Nilsson McCarthy Beal & Winston ^ Russell & Norvig (2021 , §1.2). 7–12) ^ a b c Intractability and efficiency and the combinatorial explosion : Russell & Norvig (2021 , p. 21) ^ a b c Psychological evidence of the prevalence of sub-symbolic reasoning and knowledge: Kahneman Dreyfus & Dreyfus Wason & Shapiro Kahneman, Slovic & Tversky ^ Knowledge representation and knowledge engineering : Russell & Norvig (2021 , chpt. ^ Representing categories and relations: Semantic networks , description logics , inheritance (including frames , and scripts ): Russell & Norvig (2021 , §10.2 & 10.5), Poole, Mackworth & Goebel (1998 , pp. 18.3) ^ Representing events and time: Situation calculus , event calculus , fluent calculus (including solving the frame problem ): Russell & Norvig (2021 , §10.3), Poole, Mackworth & Goebel (1998 , pp. 335–337) ^ Representing knowledge about knowledge: Belief calculus, modal logics : Russell & Norvig (2021 , §10.4), Poole, Mackworth & Goebel (1998 , pp. 275–277) ^ a b Default reasoning , Frame problem , default logic , non-monotonic logics , circumscription , closed world assumption , abduction : Russell & Norvig (2021 , §10.6) Poole, Mackworth & Goebel (1998 , pp. ^ a b Breadth of commonsense knowledge: Lenat & Guha (1989 , Introduction) Crevier (1993 , pp. ^ Uncertain preferences: Russell & Norvig (2021 , Section 16.7) Inverse reinforcement learning : Russell & Norvig (2021 , Section 22.6) ^ Information value theory : Russell & Norvig (2021 , Section 16.6). 846–860) ( word embedding ) ^ a b Supervised learning : Russell & Norvig (2021 , §19.2) (Definition) Russell & Norvig (2021 , Chpt. 281) The Economist ^ \"Artificial Intelligence (AI): What Is AI and How Does It Work? ^ Modern statistical and deep learning approaches to NLP : Russell & Norvig (2021 , chpt. ^ Uninformed searches ( breadth first search , depth-first search and general state space search ): Russell & Norvig (2021 , §3.4) Poole, Mackworth & Goebel (1998 , pp. 8) ^ Heuristic or informed searches (e.g., greedy best first and A* ): Russell & Norvig (2021 , s§3.5) Poole, Mackworth & Goebel (1998 , pp. 214, 255, 459) Scientific American ^ a b Stochastic methods for uncertain reasoning: Russell & Norvig (2021 , Chpt. 381–394) ^ Information value theory : Russell & Norvig (2021 , §16.6) ^ Markov decision processes and dynamic decision networks : Russell & Norvig (2021 , chpt. 14) Hidden Markov model : Russell & Norvig (2021 , §14.3) Kalman filters : Russell & Norvig (2021 , §14.4) Dynamic Bayesian networks : Russell & Norvig (2021 , §14.5) ^ Game theory and mechanism design : Russell & Norvig (2021 , chpt. 20) Domingos (2015 , p. 210) ^ Bayesian decision theory and Bayesian decision networks : Russell & Norvig (2021 , §16.5) ^ Statistical learning methods and classifiers : Russell & Norvig (2021 , chpt. ^ Decision trees : Russell & Norvig (2021 , §19.3) Domingos (2015 , p. 88) ^ Non-parameteric learning models such as K-nearest neighbor and support vector machines : Russell & Norvig (2021 , §19.7) Domingos (2015 , p. 187) (k-nearest neighbor) Domingos (2015 , p. 88) (kernel methods) ^ Domingos , p. 152. ^ Naive Bayes classifier : Russell & Norvig (2021 , §12.6) Domingos (2015 , p. 152) ^ a b Neural networks: Russell & Norvig (2021 , Chpt. 21), Domingos (2015 , Chapter 4) ^ Gradient calculation in computational graphs, backpropagation , automatic differentiation : Russell & Norvig (2021 , §21.2), Luger & Stubblefield (2004 , pp. 3.3) ^ Universal approximation theorem : Russell & Norvig (2021 , p. 752) The theorem: Cybenko Hornik, Stinchcombe & White ^ Feedforward neural networks : Russell & Norvig (2021 , §21.1) ^ Recurrent neural networks : Russell & Norvig (2021 , §21.6) ^ Perceptrons : Russell & Norvig (2021 , pp. 21) Goodfellow, Bengio & Courville Hinton et al. Schmidhuber ^ Convolutional neural networks : Russell & Norvig (2021 , §21.3) ^ Deng & Yu , pp. \"Twenty years on from Deep Blue vs Kasparov: how a chess match started the big data revolution\" . \"Google AI learns to play open-world video games by watching them\" . \"Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap\" . Retrieved 2024-08-07 ^ Matthew Finio & Amanda Downie: IBM Think 2024 Primer, \"What is Artificial Intelligence (AI) in Finance?\" 8 Dec. 2023 ^ M. Nicolas J. Firzli: Pensions Age/European Pensions magazine, \"Artificial Intelligence: Ask the Industry\" May June 2024 https://videovoice.org/ai-in-finance-innovation-entrepreneurship-vs-over-regulation-with-the-eus-artificial-intelligence-act-wont-work-as-intended/ ^ a b c Congressional Research Service . \"The US and 30 Other Nations Agree to Set Guardrails for Military AI\" . \"ChatGPT: Most Americans Know About It, But Few Actually Use the AI Chatbot\" . \"Misinformation, mistakes and the Pope in a puffer: what rapidly evolving AI can – and can\\'t – do\" . \"How a fake image of a Pentagon explosion shared on Twitter caused a real dip on Wall Street\" . ), \"8 - AI for large-scale evacuation modeling: promises and challenges\" , Interpretable Machine Learning for the Analysis, Design, Assessment, and Informed Decision Making for Civil Infrastructure , Woodhead Publishing Series in Civil and Structural Engineering, Woodhead Publishing, pp. \"How to Stop Your Data From Being Used to Train AI\" . Full Employment, Basic Income, and Economic Democracy\\' 51 Industrial Law Journal 511–559 Archived 27 May 2023 at the Wayback Machine ^ Ford & Colvin ; McGaughey ^ IGM Chicago . ^ Leaders\\' concerns about the existential risks of AI around 2015: Rawlinson Holley Gibbs Sainato ^ \" \"Godfather of artificial intelligence\" talks impact and potential of new AI\" . \"Rise of artificial intelligence is inevitable but should not be feared, \\'father of AI\\' says\" . \"Juergen Schmidhuber, Renowned \\'Father Of Modern AI,\\' Says His Life\\'s Work Won\\'t Lead To Dystopia\" . \"Andrew Ng: \\'Do we think the world is better off with more or less intelligence?\\' ^ Arguments that AI is not an imminent risk: Brooks Geist Madrigal Lee ^ a b Christian , pp. \"Hugging Face CEO says he\\'s focused on building a \\'sustainable model\\' for the $4.5 billion open-source-AI startup\" . \"Should we make our most powerful AI models open source to all?\" \"A critical perspective on guidelines for responsible and trustworthy artificial intelligence\" . \"Ethical issues in the development of artificial intelligence: recognizing the risks\" . ^ Regulation of AI to mitigate risks: Berryhill et al. Barfield & Pagallo Iphofen & Kritikos Wirtz, Weyerer & Geyer Buiten ^ Law Library of Congress (U.S.). ^ \"Countries agree to safe and responsible development of frontier AI in landmark Bletchley Declaration\" . 8–17) Moravec (1988 , p. 3) ^ a b Turing\\'s original publication of the Turing test in \" Computing machinery and intelligence \": Turing Historical influence and philosophical implications: Haugeland (1985 , pp. ^ Developmental robotics : Weng et al. Lungarella et al. Asada et al. Oudeyer ^ Russell & Norvig , p. 25. ^ \"One of the Biggest Problems in Regulating AI Is Agreeing on a Definition\" . How to tell if a marketing tool really uses artificial intelligence\" . ^ Physical symbol system hypothesis: Newell & Simon (1976 , p. 116) Historical significance: McCorduck (2004 , p. 153) Russell & Norvig (2021 , p. 19) ^ Moravec\\'s paradox : Moravec (1988 , pp. 190–191) ^ Dreyfus\\' critique of AI : Dreyfus Dreyfus & Dreyfus Historical significance and philosophical implications: Crevier (1993 , pp. 10–11) Russell & Norvig (2021 , p. 24) A classic example of the \"scruffy\" approach to intelligence: Minsky A modern example of neat AI and its aspirations in the 21st century: Domingos ^ Pennachin & Goertzel . \"Nick Bostrom: How can we be certain a machine isn\\'t conscious?\" ^ Transhumanism : Moravec Kurzweil Russell & Norvig (2021 , p. 1005) ^ AI as evolution: Edward Fredkin is quoted in McCorduck (2004 , p. 401) Butler Dyson ^ AI in myth: McCorduck (2004 , pp. AI textbooks The two most widely used textbooks in 2023 (see the Open Syllabus ): Russell, Stuart J. ; Norvig, Peter. These were the four of the most widely used AI textbooks in 2008: Luger, George ; Stubblefield, William . Other sources AI & ML in Fusion AI & ML in Fusion, video lecture Archived 2 July 2023 at the Wayback Machine Alter, Alexandra; Harris, Elizabeth A. (20 September 2023), \"Franzen, Grisham and Other Prominent Authors Sue OpenAI\" , The New York Times Altman, Sam ; Brockman, Greg ; Sutskever, Ilya (22 May 2023). Arntz, Melanie; Gregory, Terry; Zierahn, Ulrich , \"The risk of automation for jobs in OECD countries: A comparative analysis\", OECD Social, Employment, and Migration Working Papers 189 Asada, M.; Hosoda, K.; Kuniyoshi, Y.; Ishiguro, H.; Inui, T.; Yoshikawa, Y.; Ogino, M.; Yoshida, C. . Beal, J.; Winston, Patrick , \"The New Frontier of Human-Level Artificial Intelligence\", IEEE Intelligent Systems , 24 : 21–24, doi : 10.1109/MIS.2009.75 , hdl : 1721.1/52357 , S2CID 32437713 Berdahl, Carl Thomas; Baker, Lawrence; Mann, Sean; Osoba, Osonde; Girosi, Federico (7 February 2023). Hello, World: Artificial Intelligence and its Use in the Public Sector (PDF) . Bushwick, Sophie (16 March 2023), \"What the New GPT-4 AI Can Do\" , Scientific American Butler, Samuel (13 June 1863). Dockrill, Peter (27 June 2022), \"Robots With Flawed AI Make Sexist And Racist Decisions, Experiment Shows\" , Science Alert , archived from the original on 27 June 2022 Domingos, Pedro . The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World . Mind over Machine: The Power of Human Intuition and Expertise in the Era of the Computer . \"Poll: AI poses risk to humanity, according to majority of Americans\" . The Latest Answers to the Oldest Questions: A Philosophical Adventure with the World\\'s Greatest Thinkers . \"US Leadership in Artificial Intelligence Can Shape the 21st Century Global Order\" . Instead, the United States has developed a new area of dominance that the rest of the world views with a mixture of awe, envy, and resentment: artificial intelligence... From AI models and research to cloud computing and venture capital, U.S. companies, universities, and research labs – and their affiliates in allied countries – appear to have an enormous lead in both developing cutting-edge AI and commercializing it. The value of U.S. venture capital investments in AI start-ups exceeds that of the rest of the world combined. \"Entering the Posthuman Collective in Philip K. Dick\\'s \"Do Androids Dream of Electric Sheep?\" chatbots to get their facts right — without destroying itself in the process? \"10 years later, deep learning \\'revolution\\' rages on, say AI pioneers Hinton, LeCun and Li\" . Good, I. J. , Speculations Concerning the First Ultraintelligent Machine Goodfellow, Ian; Bengio, Yoshua; Courville, Aaron , Deep Learning , MIT Press., archived from the original on 16 April 2016 , retrieved 12 November 2017 Goodman, Bryce; Flaxman, Seth . \"Deep Neural Networks for Acoustic Modeling in Speech Recognition – The shared views of four research groups\". \"Bill Gates on dangers of artificial intelligence: \\'I don\\'t understand why some people are not concerned\\' \" . \"Regulating artificial intelligence and robotics: ethics by design in a digital society\". GOP much more skeptical than Dems that government can do it right: poll\" . \"Using Commercial Knowledge Bases for Clinical Decision Support: Opportunities, Hurdles, and Recommendations\" . Lipartito, Kenneth (6 January 2011), The Narrative and the Algorithm: Genres of Credit Reporting from the Nineteenth Century to Today (PDF) (Unpublished manuscript), doi : 10.2139/ssrn.1736283 , S2CID 166742927 , archived (PDF) from the original on 9 October 2022 Lohr, Steve . \"Robots Will Take Jobs, but Not as Fast as Some Fear, New Report Says\" . \"The case against killer robots, from a guy actually working on artificial intelligence\" . \"Google\\'s Gemini: is the new AI model really better than ChatGPT?\" \"A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence\" . McCarthy, John , \"From Here to Human-Level AI\", Artificial Intelligence : 171 McCarthy, John , What is AI? Full Employment, Basic Income, and Economic Democracy , p. 51 Industrial Law Journal 511–559, doi : 10.2139/ssrn.3044448 , S2CID 219336439 , SSRN 3044448 , archived from the original on 31 January 2021 , retrieved 27 May 2023 Merkle, Daniel; Middendorf, Martin . Presidential Address to the Association for the Advancement of Artificial Intelligence . \"On the impact of robotics in behavioral and cognitive sciences: from insect navigation to human cognitive development\" (PDF) . Reisner, Alex (19 August 2023), \"Revealed: The Authors Whose Pirated Books are Powering Generative AI\" , The Atlantic Roberts, Jacob . \"Stephen Hawking, Elon Musk, and Bill Gates Warn About Artificial Intelligence\" . \"Computer says no: why making AIs fair, accountable and transparent is crucial\" . Simon, H. A. , The Shape of Automation for Men and Management , New York: Harper & Row Simonite, Tom (31 March 2016). \\'Godfather of AI\\' Geoffrey Hinton quits Google and warns over dangers of misinformation\" . Turing, Alan (October 1950), \"Computing Machinery and Intelligence\", Mind , LIX : 433–460, doi : 10.1093/mind/LIX.236.433 , ISSN 0026-4423 UNESCO Science Report: the Race Against Time for Smarter Development . Valinsky, Jordan (11 April 2019), \"Amazon reportedly employs thousands of people to listen to your Alexa conversations\" , CNN.com Verma, Yugesh (25 December 2021). \"OpenAI has published the text-generating AI it said was too dangerous to share\" . \"The scary truth about AI copyright is nobody knows what will happen next\" . Are there computers that are inherently fuzzy and do not apply the usual binary logic?\" Williams, Rhiannon (28 June 2023), \"Humans may be more likely to believe disinformation generated by AI\" , MIT Technology Review Wirtz, Bernd W.; Weyerer, Jan C.; Geyer, Carolin (24 July 2018). Wong, Matteo (19 May 2023), \"ChatGPT Is Already Obsolete\" , The Atlantic Yudkowsky, E , \"Artificial Intelligence as a Positive and Negative Factor in Global Risk\" (PDF) , Global Catastrophic Risks , Oxford University Press, 2008, Bibcode : 2008gcr..book..303Y , archived (PDF) from the original on 19 October 2013 , retrieved 24 September 2021 Further reading Scholia has a topic profile for Artificial intelligence . The History and Future of Workplace Automation\" 29 Journal of Economic Perspectives 3. How to Think about the Future of AI\", Foreign Affairs , vol. George Dyson , historian of computing, writes (in what might be called \"Dyson\\'s Law\") that \"Any system simple enough to be understandable will not be complicated enough to behave intelligently, while any system complicated enough to behave intelligently will be too complicated to understand.\" Computer scientist Alex Pentland writes: \"Current AI machine-learning algorithms are, at their core, dead simple stupid. chatbots to get their facts right — without destroying itself in the process?\" New York Times Magazine (July 18, 2023) online Gleick, James , \"The Fate of Free Will\" (review of Kevin J. Mitchell , Free Agents: How Evolution Gave Us Free Will , Princeton University Press, 2023, 333 pp. For biological creatures, reason and purpose come from acting in the world and experiencing the consequences. Hughes-Castleberry, Kenna , \"A Murder Mystery Puzzle: The literary puzzle Cain\\'s Jawbone , which has stumped humans for decades, reveals the limitations of natural-language-processing algorithms\", Scientific American , vol. \"This murder mystery competition has revealed that although NLP ( natural-language processing ) models are capable of incredible feats, their abilities are very much limited by the amount of context they receive. This [...] could cause [difficulties] for researchers who hope to use them to do things such as analyze ancient languages . In some cases, there are few historical records on long-gone civilizations to serve as training data for such a purpose.\" \"If by \\' deepfakes \\' we mean realistic videos produced using artificial intelligence that actually deceive people, then they barely exist. [...] A.I.-generated videos are not, in general, operating in our media as counterfeited evidence. Johnston, John The Allure of Machinic Life: Cybernetics, Artificial Life, and the New AI , MIT Press. Marcus, Gary , \"Artificial Confidence: Even the newest, buzziest systems of artificial general intelligence are stymmied by the same old problems\", Scientific American , vol. Roivainen, Eka , \"AI\\'s IQ: ChatGPT aced a [standard intelligence] test but showed that intelligence cannot be measured by IQ alone\", Scientific American , vol. \"Despite its high IQ, ChatGPT fails at tasks that require real humanlike reasoning or an understanding of the physical and social world.... ChatGPT seemed unable to reason logically and tried to rely on its vast database of... facts derived from online texts.\" Scharre, Paul, \"Killer Apps: The Real Dangers of an AI Arms Race\", Foreign Affairs , vol. Rules-based systems cannot deal with circumstances their programmers did not anticipate. Learning systems are limited by the data on which they were trained. Advanced autopilot features in cars, although they perform well in some circumstances, have driven cars without warning into trucks, concrete barriers, and parked cars. In the wrong situation, AI systems go from supersmart to superdumb in an instant. When an enemy is trying to manipulate and hack an AI system, the risks are even greater.\" \"Comparing the expert survey and citation impact journal ranking methods: Example from the field of Artificial Intelligence\" (PDF) . \"Mastering the game of Go with deep neural networks and tree search\" . White Paper: On Artificial Intelligence – A European approach to excellence and trust (PDF) . External links Artificial intelligence at Wikipedia\\'s sister projects Definitions from Wiktionary Media from Commons Quotations from Wikiquote Textbooks from Wikibooks Resources from Wikiversity Data from Wikidata \"Artificial Intelligence\" . Articles related to Artificial intelligence v t e John McCarthy Artificial intelligence Circumscription Dartmouth workshop Frame problem Garbage collection Lisp ALGOL 60 McCarthy evaluation McCarthy Formalism McCarthy 91 function Situation calculus Space fountain v t e Philosophy of mind Philosophers G. E. M. Anscombe Aristotle Armstrong Thomas Aquinas J. L. Austin Alexander Bain George Berkeley Henri Bergson Ned Block Franz Brentano C. D. Broad Tyler Burge David Chalmers Patricia Churchland Paul Churchland Andy Clark Dharmakirti Donald Davidson Daniel Dennett René Descartes Fred Dretske Fodor Goldman Martin Heidegger David Hume Edmund Husserl William James Frank Cameron Jackson Immanuel Kant David Lewis (philosopher) John Locke Gottfried Wilhelm Leibniz Maurice Merleau-Ponty Marvin Minsky Thomas Nagel Alva Noë Derek Parfit Plato Hilary Putnam Richard Rorty Gilbert Ryle John Searle Wilfrid Sellars Baruch Spinoza Alan Turing Michael Tye Vasubandhu Ludwig Wittgenstein Stephen Yablo Zhuangzi more... Theories Behaviorism Biological naturalism Dualism Eliminative materialism Emergent materialism Epiphenomenalism Functionalism Interactionism Naïve realism Neurophenomenology Neutral monism New mysterianism Nondualism Occasionalism Parallelism Phenomenalism Phenomenology Physicalism Type physicalism Property dualism Representational Solipsism Substance dualism Concepts Abstract object Chinese room Creativity Cognition Cognitive closure Concept Consciousness Hard problem of consciousness Hypostatic abstraction Idea Identity Intelligence Artificial Human Intentionality Introspection Intuition Language of thought Mental event Mental image Mental process Mental property Mental representation Mind Mind–body problem Pain Problem of other minds Propositional attitude Qualia Tabula rasa Understanding Zombie Related Metaphysics Philosophy of artificial intelligence / information / perception / self Category Philosophers category Project Task Force v t e Philosophy of science Concepts Analysis Analytic–synthetic distinction A priori and a posteriori Causality Commensurability Consilience Construct Creative synthesis Demarcation problem Empirical evidence Explanatory power Fact Falsifiability Feminist method Functional contextualism Ignoramus et ignorabimus Inductive reasoning Intertheoretic reduction Inquiry Nature Objectivity Observation Paradigm Problem of induction Scientific evidence Evidence-based practice Scientific law Scientific method Scientific pluralism Scientific Revolution Scientific theory Testability Theory choice Theory-ladenness Underdetermination Unity of science more... Theories Coherentism Confirmation holism Constructive empiricism Constructive realism Constructivist epistemology Contextualism Conventionalism Deductive-nomological model Epistemological anarchism Evolutionism Fallibilism Foundationalism Hypothetico-deductive model Inductionism Instrumentalism Model-dependent realism Naturalism Physicalism Positivism / Reductionism / Determinism Pragmatism Rationalism / Empiricism Received view / Semantic view of theories Scientific essentialism Scientific formalism Scientific realism / Anti-realism Scientific skepticism Scientism Structuralism Uniformitarianism Vitalism Philosophy of... Biology Chemistry Physics Space and time Social science Archaeology Economics\\u200e Geography History Linguistics Psychology Related topics Criticism of science Descriptive science Epistemology Exact sciences Faith and rationality Hard and soft science History and philosophy of science Non-science Pseudoscience Normative science Protoscience Relationship between religion and science Rhetoric of science Science studies Sociology of scientific ignorance Sociology of scientific knowledge Philosophers of science Precursors Roger Bacon Francis Bacon Galileo Galilei Isaac Newton David Hume Auguste Comte Henri Poincaré Pierre Duhem Rudolf Steiner Karl Pearson Charles Sanders Peirce Wilhelm Windelband Alfred North Whitehead Bertrand Russell Otto Neurath C. D. Broad Michael Polanyi Hans Reichenbach Rudolf Carnap Karl Popper Carl Gustav Hempel W. V. O. Quine Thomas Kuhn Imre Lakatos Paul Feyerabend Ian Hacking Bas van Fraassen Larry Laudan Category Philosophy portal Science portal v t e Evolutionary computation Main Topics Evolutionary algorithm Evolutionary data mining Evolutionary multimodal optimization Human-based evolutionary computation Interactive evolutionary computation Algorithms Cellular evolutionary algorithm Covariance Matrix Adaptation Evolution Strategy (CMA-ES) Cultural algorithm Differential evolution Evolutionary programming Genetic algorithm Genetic programming Gene expression programming Evolution strategy Natural evolution strategy Neuroevolution Learning classifier system Related techniques Swarm intelligence Ant colony optimization Bees algorithm Cuckoo search Particle swarm optimization Bacterial Colony Optimization Metaheuristic methods Firefly algorithm Harmony search Gaussian adaptation Memetic algorithm Related topics Artificial development Artificial intelligence Artificial life Digital organism Evolutionary robotics Fitness function Fitness landscape Fitness approximation Genetic operators Interactive evolutionary computation No free lunch in search and optimization Machine learning Mating pool Program synthesis Journals Evolutionary Computation (journal) v t e Differentiable computing General Differentiable programming Information geometry Statistical manifold Automatic differentiation Neuromorphic engineering Pattern recognition Tensor calculus Computational learning theory Inductive bias Concepts Gradient descent SGD Clustering Regression Overfitting Hallucination Adversary Attention Convolution Loss functions Backpropagation Batchnorm Activation Softmax Sigmoid Rectifier Regularization Datasets Augmentation Diffusion Autoregression Applications Machine learning In-context learning Artificial neural network Deep learning Scientific computing Artificial Intelligence Language model Large language model Hardware IPU TPU VPU Memristor SpiNNaker Software libraries TensorFlow PyTorch Keras Theano JAX Flux.jl MindSpore Implementations Audio–visual AlexNet WaveNet Human image synthesis HWR OCR Speech synthesis Speech recognition Facial recognition AlphaFold Text-to-image models DALL-E Midjourney Stable Diffusion Text-to-video models Sora VideoPoet Whisper Verbal Word2vec Seq2seq BERT Gemini LaMDA Bard NMT Project Debater IBM Watson IBM Watsonx Granite GPT-1 GPT-2 GPT-3 GPT-4 ChatGPT GPT-J Chinchilla AI PaLM BLOOM LLaMA PanGu-Σ Decisional AlphaGo AlphaZero Q-learning SARSA OpenAI Five Self-driving car MuZero Action selection Auto-GPT Robot control People Yoshua Bengio Alex Graves Ian Goodfellow Stephen Grossberg Demis Hassabis Geoffrey Hinton Yann LeCun Fei-Fei Li Andrew Ng Jürgen Schmidhuber David Silver Ilya Sutskever Organizations Anthropic EleutherAI Google DeepMind Hugging Face OpenAI Meta AI Mila MIT CSAIL Huawei Architectures Neural Turing machine Differentiable neural computer Transformer Recurrent neural network (RNN) Long short-term memory (LSTM) Gated recurrent unit (GRU) Echo state network Multilayer perceptron (MLP) Convolutional neural network Residual neural network Mamba Autoencoder Variational autoencoder (VAE) Generative adversarial network (GAN) Graph neural network Portals Computer programming Technology Categories Artificial neural networks Machine learning v t e Computer science Note: This template roughly follows the 2012 ACM Computing Classification System . Hardware Printed circuit board Peripheral Integrated circuit Very Large Scale Integration Systems on Chip (SoCs) Energy consumption (Green computing) Electronic design automation Hardware acceleration Processor Size / Form Computer systems organization Computer architecture Computational complexity Dependability Embedded system Real-time computing Networks Network architecture Network protocol Network components Network scheduler Network performance evaluation Network service Software organization Interpreter Middleware Virtual machine Operating system Software quality Software notations and tools Programming paradigm Programming language Compiler Domain-specific language Modeling language Software framework Integrated development environment Software configuration management Software library Software repository Software development Control variable Software development process Requirements analysis Software design Software construction Software deployment Software engineering Software maintenance Programming team Open-source model Theory of computation Model of computation Formal language Automata theory Computability theory Computational complexity theory Logic Semantics Algorithms Algorithm design Analysis of algorithms Algorithmic efficiency Randomized algorithm Computational geometry Mathematics of computing Discrete mathematics Probability Statistics Mathematical software Information theory Mathematical analysis Numerical analysis Theoretical computer science Information systems Database management system Information storage systems Enterprise information system Social information systems Geographic information system Decision support system Process control system Multimedia information system Data mining Digital library Computing platform Digital marketing World Wide Web Information retrieval Security Cryptography Formal methods Security hacker Security services Intrusion detection system Hardware security Network security Information security Application security Human–computer interaction Interaction design Social computing Ubiquitous computing Visualization Accessibility Concurrency Concurrent computing Parallel computing Distributed computing Multithreading Multiprocessing Artificial intelligence Natural language processing Knowledge representation and reasoning Computer vision Automated planning and scheduling Search methodology Control method Philosophy of artificial intelligence Distributed artificial intelligence Machine learning Supervised learning Unsupervised learning Reinforcement learning Multi-task learning Cross-validation Graphics Animation Rendering Photograph manipulation Graphics processing unit Mixed reality Virtual reality Image compression Solid modeling Applied computing Quantum Computing E-commerce Enterprise software Computational mathematics Computational physics Computational chemistry Computational biology Computational social science Computational engineering Differentiable computing Computational healthcare Digital art Electronic publishing Cyberwarfare Electronic voting Video games Word processing Operations research Educational technology Document management Category Outline Glossaries v t e Emerging technologies Fields Information and communications Ambient intelligence Internet of things Artificial intelligence Applications of artificial intelligence Machine translation Machine vision Mobile translation Progress in artificial intelligence Semantic Web Speech recognition Atomtronics Carbon nanotube field-effect transistor Cybermethodology Fourth-generation optical discs 3D optical data storage Holographic data storage GPGPU Memory CBRAM ECRAM FRAM Millipede MRAM NRAM PRAM Racetrack memory RRAM SONOS UltraRAM Optical computing RFID Chipless RFID Software-defined radio Three-dimensional integrated circuit Topics Automation Collingridge dilemma Differential technological development Disruptive innovation Ephemeralization Ethics Bioethics Cyberethics Neuroethics Robot ethics Exploratory engineering Proactionary principle Technological change Technological unemployment Technological convergence Technological evolution Technological paradigm Technology forecasting Accelerating change Future-oriented technology analysis Horizon scanning Moore\\'s law Technological singularity Technology scouting Technology in science fiction Technology readiness level Technology roadmap Transhumanism List v t e Robotics Main articles Outline Glossary Index History Geography Hall of Fame Ethics Laws Competitions AI competitions Types Aerobot Anthropomorphic Humanoid Android Cyborg Gynoid Claytronics Companion Automaton Animatronic Audio-Animatronics Industrial Articulated arm Domestic Educational Entertainment Juggling Military Medical Service Disability Agricultural Food service Retail BEAM robotics Soft robotics Classifications Biorobotics Cloud robotics Continuum robot Unmanned vehicle aerial ground Mobile robot Microbotics Nanorobotics Necrobotics Robotic spacecraft Space probe Swarm Telerobotics Underwater remotely-operated Robotic fish Locomotion Tracks Walking Hexapod Climbing Electric unicycle Robotic fins Navigation and mapping Motion planning Simultaneous localization and mapping Visual odometry Vision-guided robot systems Research Evolutionary Kits Simulator Suite Open-source Software Adaptable Developmental Human–robot interaction Paradigms Perceptual Situated Ubiquitous Companies Amazon Robotics Anybots Barrett Technology Boston Dynamics Energid Technologies FarmWise FANUC Figure AI Foster-Miller Harvest Automation Honeybee Robotics Intuitive Surgical IRobot KUKA Starship Technologies Symbotic Universal Robotics Wolf Robotics Yaskawa Related Critique of work Powered exoskeleton Workplace robotics safety Robotic tech vest Technological unemployment Terrainability Fictional robots Category Outline v t e Existential risk from artificial intelligence Concepts AGI AI alignment AI capability control AI safety AI takeover Consequentialism Effective accelerationism Ethics of artificial intelligence Existential risk from artificial general intelligence Friendly artificial intelligence Instrumental convergence Intelligence explosion Longtermism Machine ethics Suffering risks Superintelligence Technological singularity Organizations Alignment Research Center Center for AI Safety Center for Applied Rationality Center for Human-Compatible Artificial Intelligence Centre for the Study of Existential Risk EleutherAI Future of Humanity Institute Future of Life Institute Google DeepMind Humanity+ Institute for Ethics and Emerging Technologies Leverhulme Centre for the Future of Intelligence Machine Intelligence Research Institute OpenAI People Scott Alexander Sam Altman Yoshua Bengio Nick Bostrom Paul Christiano Eric Drexler Sam Harris Stephen Hawking Dan Hendrycks Geoffrey Hinton Bill Joy Shane Legg Elon Musk Steve Omohundro Huw Price Martin Rees Stuart J. Russell Jaan Tallinn Max Tegmark Frank Wilczek Roman Yampolskiy Eliezer Yudkowsky Other Statement on AI risk of extinction Human Compatible Open letter on artificial intelligence Our Final Invention The Precipice Superintelligence: Paths, Dangers, Strategies Do You Trust This Computer? Artificial Intelligence Act Category v t e Subfields of and cyberneticians involved in cybernetics Subfields Artificial intelligence Biological cybernetics Biomedical cybernetics Biorobotics Biosemiotics Neurocybernetics Catastrophe theory Computational neuroscience Connectionism Control theory Conversation theory Cybernetics in the Soviet Union Decision theory Emergence Engineering cybernetics Homeostasis Information theory Management cybernetics Medical cybernetics Second-order cybernetics Semiotics Sociocybernetics Synergetics Cyberneticians Alexander Lerner Alexey Lyapunov Alfred Radcliffe-Brown Allenna Leonard Anthony Wilden Buckminster Fuller Charles François Genevieve Bell Margaret Boden Claude Bernard Cliff Joslyn Erich von Holst Ernst von Glasersfeld Francis Heylighen Francisco Varela Frederic Vester Charles Geoffrey Vickers Gordon Pask Gordon S. Brown Gregory Bateson Heinz von Foerster Humberto Maturana I. A. Richards Igor Aleksander Jacque Fresco Jakob von Uexküll Jason Jixuan Hu Jay Wright Forrester Jennifer Wilby John N. Warfield Kevin Warwick Ludwig von Bertalanffy Maleyka Abbaszadeh Manfred Clynes Margaret Mead Marian Mazur N. Katherine Hayles Natalia Bekhtereva Niklas Luhmann Norbert Wiener Pyotr Grigorenko Qian Xuesen Ranulph Glanville Robert Trappl Sergei P. Kurdyumov Anthony Stafford Beer Stuart Kauffman Stuart Umpleby Talcott Parsons Ulla Mitzdorf Valentin Turchin Valentin Braitenberg William Ross Ashby Walter Bradford Cannon Walter Pitts Warren McCulloch William Grey Walter v t e Glossaries of science and engineering Aerospace engineering Agriculture Archaeology Architecture Artificial intelligence Astronomy Biology Botany Calculus Cell biology Chemistry Civil engineering Clinical research Computer hardware Computer science Developmental and reproductive biology Ecology Economics Electrical and electronics engineering Engineering A–L M–Z Entomology Environmental science Genetics and evolutionary biology Cellular and molecular biology 0–L M–Z Geography A–M N–Z Arabic toponyms Hebrew toponyms Western and South Asia Geology Ichthyology Machine vision Mathematics Mechanical engineering Medicine Meteorology Mycology Nanotechnology Ornithology Physics Probability and statistics Psychiatry Quantum computing Robotics Scientific naming Structural engineering Virology Authority control databases : National Spain France BnF data Germany Israel United States Latvia Japan Czech Republic Retrieved from \" https://en.wikipedia.org/w/index.php?title=Artificial_intelligence&oldid=1241046606 \" Categories : Articles with BNFdata identifiers Artificial intelligence Computational fields of study Computational neuroscience Cybernetics Data science Formal sciences Intelligence by type Hidden categories: Webarchive template wayback links CS1: long volume value Harv and Sfn no-target errors Articles with short description Short description is different from Wikidata Use dmy dates from July 2023 Wikipedia indefinitely semi-protected pages All articles with unsourced statements Articles with unsourced statements from June 2024 All accuracy disputes Articles with disputed statements from July 2024 Pages displaying short descriptions of redirect targets via Module:Annotated link Pages using Sister project links with hidden wikidata Articles with Internet Encyclopedia of Philosophy links Articles with BNE identifiers Articles with BNF identifiers Articles with GND identifiers Articles with J9U identifiers Articles with LCCN identifiers Articles with LNB identifiers Articles with NDL identifiers Articles with NKC identifiers'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}